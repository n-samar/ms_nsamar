\section{F1 Implementation}
\label{sec:f1_implementation}

We have implemented F1's components in RTL, and synthesize them in a commercial
14/12nm process using state-of-the-art tools. These include a commercial SRAM
compiler that we use for scratchpad and register file banks.

We use a dual-frequency design: most components run at 1\,GHz, but memories
(register files and scratchpads) run double-pumped at 2\,GHz. Memories meet
this frequency easily and this enables using single-ported SRAMs while serving
up to two accesses per cycle. By keeping most of the logic at 1\,GHz, we
achieve higher energy efficiency. We explored several non-blocking on-chip
networks (Clos, Benes, and crossbars). We use 3 16$\times$16 bit-sliced
crossbars~\cite{passas:tocaid12:crossbar} (scratch\-pad$\rightarrow$cluster,
cluster$\rightarrow$scratchpad, and cluster$\rightarrow$cluster).

\autoref{tbl:f1GF12} shows a breakdown of area by component, as well as the
area of our F1 configuration, 151.4\,mm$^2$. FUs take 42\% of the area, with
31.7\% going to memory, 6.6\% to the on-chip network, and 19.7\% to the two
HBM2 PHYs. We assume 512\,GB/s bandwidth per PHY; this is similar to the NVIDIA
A100 GPU~\cite{choquette2021nvidia}, which has 2.4\,TB/s with 6 HBM2E
PHYs~\cite{nvidiadgx}. We use prior work to estimate HBM2 PHY
area~\cite{rambuswhite, dasgupta20208} and power~\cite{rambuswhite,
ge2011design}.

This design is constrained by memory bandwidth: though it has 1\,TB/s of
bandwidth, the on-chip network's bandwidth is 24\,TB/s, and the aggregate
bandwidth between RFs and FUs is 128\,TB/s. This is why maximizing reuse is
crucial.

\section{Experimental Methodology}
\label{sec:f1_methodology}

\paragraph{Modeled system:}
We evaluate our F1 implementation from \autoref{sec:implementation}. We use a
cycle-accurate simulator to execute F1 programs. Because the architecture is
static, this is very different from conventional simulators, and acts more as a
checker: it runs the instruction stream at each component and verifies that
latencies are as expected and there are no missed dependences or structural
hazards. We use activity-level energies from RTL synthesis to produce energy
breakdowns.

\paragraph{Benchmarks:}
We use several FHE programs to evaluate F1. All programs come from
state-of-the-art software implementations, which we port to F1:

\subparagraph{Logistic regression}
uses the HELR algorithm~\cite{han:aaai19:logistic}, which is based on CKKS. We
compute a single batch of logistic regression training with up to $256$
features, and $256$ samples per batch, starting at computational depth $L =
16$; this is equivalent to the first batch of HELR's MNIST workload. This
computation features ciphertexts with large $\log Q$ ($L = 14,15,16$), so it
needs careful data orchestration to run efficiently.

\subparagraph{Neural network} benchmarks come from Low Latency CryptoNets
(LoLa)~\cite{brutzkus:icml19:low}. This work uses B/FV, an FHE scheme that F1
does not support, so we use CKKS instead. We run two neural networks:
LoLa-MNIST is a simple, LeNet-style network used on the MNIST
dataset~\cite{lecunn:ieee98:gradient-document}, while LoLa-CIFAR is a much
larger 6-layer network (similar in computation to MobileNet
v3~\cite{howard2019searching}) used on the CIFAR-10 dataset~\cite{cifar10}.
LoLa-MNIST includes two variants with unencrypted and encrypted weights;
LoLa-CIFAR is available only with unencrypted weights. These three benchmarks
use relatively low $L$ values (their starting $L$ values are 4, 6, and 8,
respectively), so they are less memory-bound. They also feature frequent
automorphisms, showing the need for a fast automorphism~unit.

\tblFOneGF

\subparagraph{DB Lookup} is adapted from HELib's
\texttt{BGV\_country\_db\_lookup}~\cite{helib:db-lookup}. A BGV-encrypted query
string is used to traverse an encrypted key-value store and return the
corresponding value. The original implementation uses a low security level for
speed of demonstration, but in our version, we implement it at $L=$17, $N=$16K
for realism. We also parallelize the CPU version so it can effectively use all
available cores. DB Lookup is both deep and wide, so running it on F1 incurs
substantial off-chip data movement.

\addtocounter{table}{1}
\tblFOneMicrobenchmark

\subparagraph{Bootstrapping:} We evaluate bootstrapping benchmarks for BGV and
CKKS. Bootstrapping takes an $L=1$ ciphertext with an exhausted noise budget
and refreshes it by bringing it up to a chosen top value of $L=L_{max}$, then
performing the bootstrapping computation to eventually obtain a usable
ciphertext at a lower depth (e.g., $L_{max} - 15$ for BGV).

For BGV, we use Sheriff and Peikert's
algorithm~\cite{alperin:crypto13:practical} for non-packed BGV
boot\-strap\-ping, with $L_{max} = 24$. This is a particularly challenging
benchmark because it features computations at large values of $L$. This
exercises the scheduler's algorithmic choice component, which selects the right
keyswitch method to balance computation and data movement.

For CKKS, we use non-packed CKKS bootstrapping from
HEA\-AN~\cite{cheon:eurocrypt2018:bootstrapping}, also with $L_{max} = 24$.
CKKS bootstrapping has many fewer ciphertext multiplications than BGV, greatly
reducing reuse opportunities for keyswitch hints.

\paragraph{Baseline systems:}
We compare F1 with a CPU system running the baseline programs (a 4-core,
8-thread, 3.5\,GHz Xeon E3-1240v5). Since prior accelerators do not support
full programs, we also include microbenchmarks of single operations and compare
against HEAX~\cite{riazi:asplos20:heax}, the fastest prior accelerator.

\section{Evaluation}\label{sec:f1_evaluation}

\subsection{Performance}\label{sec:perf}

\tblFOneBenchmark


\paragraph{Benchmarks:}
\autoref{tbl:f1benchmark} compares the performance of F1 and the CPU on full
benchmarks. It reports execution time in milliseconds for each program (lower
is better), and F1's speedup over the CPU (higher is better). F1 achieves
dramatic speedups, from 1,195$\times$ to 17,412$\times$ (5,432$\times$ gmean).
CKKS bootstrapping has the lowest speedups as it's highly memory-bound; other
speedups are within a relatively narrow band, as compute and memory traffic are
more balanced.

These speedups greatly expand the applicability of FHE. Consider deep learning:
in software, even the simple LoLa-MNIST network takes seconds per inference,
and a single inference on the more realistic LoLa-CIFAR network takes \emph{20
minutes}. F1 brings this down to 241 \emph{milliseconds}, making real-time deep
learning inference practical: when offloading inferences to a server, this time
is comparable to the roundtrip latency between server and client.

\paragraph{Microbenchmarks:}
\autoref{tbl:f1microbenchmark} compares the performance of F1, the CPU, and
HEAX$_\sigma$ on four microbenchmarks: the basic NTT and automorphism
operations on a single ciphertext, and homomorphic multiplication and
permutation (which uses automorphisms). We report three typical sets of
parameters. We use microbenchmarks to compare against prior accelerators, in
particular HEAX. But prior accelerators do not implement automorphisms, so we
extend each HEAX keyswitching pipeline with an SRAM-based, scalar automorphism
unit. We call this extension HEAX$_\sigma$.

\autoref{tbl:f1microbenchmark} shows that
F1 achieves large speedups over HEAX$_\sigma$, ranging from 172\x to 1,866\x.
Moreover, F1's speedups over the CPU are even larger than in full benchmarks.
This is because microbenchmarks are pure compute, and thus miss the data
movement bottlenecks of FHE programs.

\subsection{Architectural Analysis}

To gain more insights into these results, we now analyze F1's data movement,
power consumption, and compute.

\paragraph{Data movement:}
\autoref{fig:f1dataMovement} shows a breakdown of off-chip memory traffic
across data types: keyswitch hints (KSH), inputs/outputs, and intermediate
values. KSH and input/output traffic is broken into compulsory and
non-compulsory (i.e., caused by limited scratchpad capacity). Intermediates,
which are always non-compulsory, are classified as loads or stores.

\autoref{fig:f1dataMovement} shows that keyswitch hints dominate in high-depth
workloads (LogReg, DB Lookup, and bootstrapping), taking up to 94\% of traffic.
Key-switch hints are also significant in the LoLa-MNIST variants. This shows
why scheduling should prioritize them. Second, due our scheduler design, F1
approaches compulsory traffic for most benchmarks, with non\hyp{}compulsory
accesses adding only 5-18\% of traffic. The exception is LoLa-CIFAR, where
intermediates consume 75\% of traffic. LoLa-CIFAR has very high reuse of
keyswitch hints, and exploiting it requires spilling intermediate ciphertexts.

\figFOneDataMovement
\figFOneOpBreakdown

\paragraph{Power consumption:}
\autoref{fig:f1power} reports average power for each benchmark, broken down by
component. This breakdown also includes off-chip memory power
(\autoref{tbl:f1GF12} only included the on-chip component). Results show
reasonable power consumption for an accelerator card. Overall, computation
consumes 20-30\% of power, and data movement dominates.

\paragraph{Utilization over time:}
F1's average FU utilization is about 30\%. However, this doesn't mean that
fewer FUs could achieve the same performance: benchmarks have memory\hyp{}bound
phases that weigh down average FU utilization. To see this,
\autoref{fig:f1opBreakdown} shows a breakdown of FU utilization over time for
LoLa-MNIST Plaintext Weights. \autoref{fig:f1opBreakdown} also shows off-chip
bandwidth utilization over time (black line). The program is initially
memory-bound, and few FUs are active. As the memory-bound phase ends, compute
intensity grows, utilizing a balanced mix of the available FUs. Finally, due to
decoupled execution, when memory bandwidth utilization peaks again, F1 can
maintain high compute intensity. The highest FU utilization happens at the end
of the benchmark and is caused by processing the final (fully connected) layer,
which is highly parallel and already has all inputs available on-chip.

\subsection{Sensitivity Studies}
\label{sec:sensitivity}

\tblFOneSensitivity

To understand the impact of our FUs and scheduling algorithms, we evaluate F1
variants without them. \autoref{tbl:f1sensitivity} reports the \emph{slowdown
(higher is worse)} of F1 with: \emph{(1)} low\hyp{}throughput NTT FUs that
follow the same design as HEAX (processing one stage of NTT butterflies per
cycle); \emph{(2)} low\hyp{}throughput automorphism FUs using a serial SRAM
memory, and \emph{(3)} Goodman's register-pressure-aware
scheduler~\cite{goodman:ics1988:code}.

For the FU experiments, our goal is to show the importance of having
high-throughput units. Therefore, the low-throughput variants use many more
(NTT or automorphism) FUs, so that aggregate throughput across all FUs in the
system is the same. Also, the scheduler accounts for the characteristics of
these FUs. In both cases, performance drops substantially, by gmean 2.6$\times$
and 3.3$\times$. This is because achieving high throughput requires excessive
parallelism, which hinders data movement, forcing the scheduler to balance
both.

Finally, the scheduler experiment uses register-pressure-aware
scheduling~\cite{goodman:ics1988:code} as the off-chip data movement scheduler
instead, operating on the full dataflow graph. This algorithm was proposed for
VLIW processors and register files; we apply it to the larger scratchpad. The
large slowdowns show that prior capacity-aware schedulers are ineffective on
F1.

\figFOneConfigs

\subsection{Scalability}
\label{sec:scalability}

Finally, we study how F1's performance changes with its area budget: we sweep
the number of compute clusters, scratchpad banks, HBM controllers, and network
topology to find the most efficient design at each area. \autoref{fig:f1pareto}
shows this Pareto frontier, with area in the $x$-axis and performance in the
$y$-axis. This curve shows that, as F1 scales, it uses resources efficiently:
performance grows about linearly through a large range of areas.
