\section{F1 Implementation}
\label{sec:f1implementation}

We have implemented F1's components in RTL,
and synthesize them in a commercial 14/12nm process using state-of-the-art tools.
These include a commercial SRAM compiler that we use for scratchpad and register file banks.

We use a dual-frequency design: most components run at 1\,GHz,
but memories (register files and scratchpads)
run double-pumped at 2\,GHz.
Memories meet this frequency easily and this enables using single-ported SRAMs while serving up to two accesses per cycle.
%
%It's easy to meet this higher frequency on these components, and this approach brings two advantages:
%it allows the use of single-ported SRAMs while still serving up to two accesses per cycle,
%and reduces the wiring in the on-chip network.
By keeping most of the logic at 1\,GHz, we achieve higher energy efficiency.
%
We explored several non-blocking on-chip networks (Clos, Benes, and crossbars).
We use 3 16$\times$16 bit-sliced crossbars~\cite{passas:tocaid12:crossbar} (scratch\-pad$\rightarrow$cluster, cluster$\rightarrow$scratchpad, and cluster$\rightarrow$cluster). %that provide an aggregate 24\,TB/s.

\autoref{tbl:f1GF12} shows a breakdown of area by component, as well as the area of our F1 configuration,
151.4\,mm$^2$.
FUs take 42\% of the area, with 31.7\% going to memory,
6.6\% to the on-chip network, and 19.7\% to the two HBM2 PHYs.
% dsm: This is important because HBM2 is nominally 256GB/s
We assume 512\,GB/s bandwidth per PHY;
this is similar to the NVIDIA A100 GPU~\cite{choquette2021nvidia}, which has 2.4\,TB/s with 6 HBM2E PHYs~\cite{nvidiadgx}.
%, we be\-lieve this is rea\-sona\-ble
We use prior work to estimate HBM2 PHY area~\cite{rambuswhite, dasgupta20208} and power~\cite{rambuswhite, ge2011design}.

% Bandwidths:
% At FUs - 16 operands/cycle from/to RFs (4 3-operand FUs, 2 2-operand FUs), 512 bytes/operand, 10 clusters, 1 GHz -> 80 TB/s
% At on-chip network, 26 ports, 2 GHz, 512 bytes/cycle/port -> 26 TB/s (assuming 256 bytes, we can do 13 TB/s?). But note this isn't really symmetric, so this isn't right: there's 10 compute clusters, and we get 4 operands (2 in + 2 out) per cycle. So, 20 TB/s at 2GHz, and 10 TB/s with 256B links instead of 512B.
% Off-chip -> 1 TB/s
This design is constrained by memory bandwidth: though it has 1\,TB/s of bandwidth,
the on-chip network's bandwidth is 24\,TB/s, and the aggregate bandwidth between RFs and FUs is 128\,TB/s.
This is why maximizing reuse is crucial.

%We use Synopsys Design Compiler~\cite{} to synthesize all functional units (FUs) and generate area and power estimates (\autoref{tab:GF12}). We use a 14/12nm technology node library for synthesis. We use a commercial SRAM compiler to generate register file (RF) and scratchpad area and power estimates. The FUs are designed to run at 1GHz, while RFs are double-pumped at 2GHz. The functional units are implemented in Minispec~\cite{}, a simplified version of the BlueSpec RTL language~\cite{}. The scratchpad is \tmp{?}-way banked. The functional units and register files are distributed across 10 clusters, each containing 1 NTT, 1 automorphism, 2 adder, and 2 multiplier units. The clusters are connected via a \tmp{26}$\times$\tmp{26} crossbar running at \tmp{?}TB/s peak bandwidth. We connect our accelerator to a \tmp{?}TB/s high-bandwidth memory.



\section{Experimental Methodology}
\label{sec:f1methodology}

\paragraph{Modeled system:}
We evaluate our F1 implementation from \autoref{sec:implementation}.
We use a cycle-accurate simulator to execute F1 programs.
Because the architecture is static, this is very different from conventional simulators,
and acts more as a checker: it runs the instruction stream at each component and verifies
that latencies are as expected and there are no missed dependences or structural hazards.
% dsm: A 0-information sentence, since we will show them those breakdowns :)
%The simulator also produces traffic and energy breakdowns.
We use activity-level energies from RTL synthesis to produce energy breakdowns.

\paragraph{Benchmarks:}
We use several FHE programs to evaluate F1. %\autoref{tab:benchmarks} summarizes their characteristics.
All programs come from state-of-the-art software implementations, which we port to F1:

\subparagraph{Logistic regression}
uses the HELR algorithm~\cite{han:aaai19:logistic}, which is based on CKKS.
We compute a single batch of logistic regression training with up to $256$ features, and $256$ samples per batch,
starting at computational depth $L = 16$; this is equivalent to the first batch of HELR's MNIST workload. 
This computation features %ciphertext-ciphertext multiplications as well as automorphisms on
ciphertexts with large $\log Q$ ($L = 14,15,16$), so it needs careful data orchestration to run efficiently.

\subparagraph{Neural network} benchmarks come from Low Latency CryptoNets (LoLa)~\cite{brutzkus:icml19:low}.
This work uses B/FV, an FHE scheme that F1 does not support, so we use CKKS instead.
% dsm: Axel, please check. We talked about switching from CKKS to be more balanced, and I think this avoids the "amount of work" issues.
We run two neural networks:
LoLa-MNIST is a simple, LeNet-style network used on the MNIST dataset~\cite{lecunn:ieee98:gradient-document},
while LoLa-CIFAR is a much larger 6-layer network (similar in computation to MobileNet v3~\cite{howard2019searching})
used on the CIFAR-10 dataset~\cite{cifar10}.
LoLa-MNIST includes two variants with unencrypted and encrypted weights;
LoLa-CIFAR is available only with unencrypted weights.
These three benchmarks use relatively low $L$ values (their starting $L$ values are 4, 6, and 8, respectively),
so they are less memory-bound.
%This allows storing many key-switch hints on chip.
They also feature frequent automorphisms,
showing the need for a fast automorphism~unit.


\tblFOneGF % dsm: Babysitted

\subparagraph{DB Lookup} is adapted from HELib's \texttt{BGV\_country\_db\_lookup}~\cite{helib:db-lookup}. A BGV-encrypted query string is used to traverse an encrypted key-value store and return the corresponding value. The original implementation uses a low security level for speed of demonstration, but in our version, we implement it at $L=$17, $N=$16K for realism. We also parallelize the CPU version so it can effectively use all available cores. DB Lookup is both deep and wide, so running it on F1 incurs substantial off-chip data movement.

% dsm: Babysitted
\addtocounter{table}{1}
\tblFOneMicrobenchmark

\subparagraph{Bootstrapping:} We evaluate bootstrapping benchmarks for BGV and CKKS.
Bootstrapping takes an $L=1$ ciphertext with an exhausted noise budget and refreshes it
by bringing it up to a chosen top value of $L=L_{max}$, then performing the bootstrapping computation
to eventually obtain a usable ciphertext at a lower depth (e.g., $L_{max} - 15$ for BGV).

For BGV, we use Sheriff and Peikert's algorithm~\cite{alperin:crypto13:practical} for non-packed BGV boot\-strap\-ping, with $L_{max} = 24$.
This is a particularly challenging benchmark because it features computations at large values of $L$.
This exercises the scheduler's 
algorithmic choice component, which selects
the right key-switch method to balance computation and data movement.

%At $L = 24$, key-switch hints using the standard method are 72\,MB, whereas the 
%We can get this down to 12.125MB using an alternative relinearization method, however, this comes at a huge computational cost.

For CKKS, we use non-packed CKKS bootstrapping from HEA\-AN~\cite{cheon:eurocrypt2018:bootstrapping}, also with $L_{max} = 24$.
CKKS bootstrapping has many fewer ciphertext multiplications than BGV, greatly reducing
reuse opportunities for key-switch hints.

%This benchmark is extremely bandwidth bound by its many large KSH matrices.
%Like BGV Boostrapping, we implement this benchmark at $L = 24$.

% HACK(dsm): To show this table where it should be, we must place it before tblBenchmark... so twek counters to show the right order

\paragraph{Baseline systems:}
We compare F1 with a CPU system running the baseline programs (a 4-core, 8-thread, 3.5\,GHz Xeon E3-1240v5).
Since prior accelerators do not support full programs, we also include microbenchmarks of single operations
and compare against HEAX~\cite{riazi:asplos20:heax}, the fastest prior accelerator.

\section{Evaluation}\label{sec:evaluation}
%\vspace{-0.1in}
\subsection{Performance}\label{sec:perf}

% HACK(dsm): See above
\addtocounter{table}{-2}
\tblFOneBenchmark
\addtocounter{table}{1}


\paragraph{Benchmarks:}
\autoref{tbl:f1benchmark} compares the performance of F1 and the CPU on full benchmarks.
It reports execution time in milliseconds for each program (lower is better), and F1's speedup over the CPU (higher is better).
F1 achieves dramatic speedups, from 1,195$\times$ to 17,412$\times$ (5,432$\times$ gmean).
CKKS bootstrapping has the lowest speedups as it's highly memory-bound;
other speedups are within a relatively narrow band, as compute and memory traffic are more balanced.

These speedups greatly expand the applicability of FHE. Consider deep learning:
in software, even the simple LoLa-MNIST network takes seconds per inference,
and a single inference on the more realistic LoLa-CIFAR network takes \emph{20 minutes}.
F1 brings this down to 241 \emph{milliseconds},
making real-time deep learning inference practical:
when offloading inferences to a server, this time is comparable
to the roundtrip latency between server and client.

\paragraph{Microbenchmarks:}
\autoref{tbl:f1microbenchmark} compares the performance of F1, the CPU, and HEAX$_\sigma$ on four microbenchmarks:
the basic NTT and automorphism operations on a single ciphertext,
and homomorphic multiplication and permutation (which uses automorphisms).
We report three typical sets of parameters.
We use microbenchmarks to compare against prior accelerators,
in particular HEAX.
But prior accelerators do not implement automorphisms,
so we extend each HEAX key-switching pipeline with an SRAM-based, scalar automorphism unit.
We call this extension HEAX$_\sigma$.

\autoref{tbl:f1microbenchmark} shows that
F1 achieves large speedups over HEAX$_\sigma$,
ranging from 172\x to 1,866\x.
%and those become more significant for larger parameters.
Moreover, F1's speedups over the CPU are even larger than in full benchmarks.
This is because microbenchmarks are pure compute,
and thus miss the data movement bottlenecks of FHE programs.

%\vspace{-2pt}
\subsection{Architectural analysis}

To gain more insights into these results, we now analyze F1's data movement, power consumption, and compute.

\paragraph{Data movement:}
\autoref{fig:f1dataMovement} shows a breakdown of off-chip memory traffic across data types:
key-switch hints (KSH), inputs/outputs, and intermediate values.
KSH and input/output traffic is broken into compulsory and
non-compulsory (i.e., caused by limited scratchpad capacity).
Intermediates, which are always non-compulsory, are classified as loads or stores.

\autoref{fig:f1dataMovement} shows that key-switch hints dominate in high-depth workloads
(LogReg, DB Lookup, and bootstrapping), taking up to 94\% of traffic.
Key-switch hints are also significant in the LoLa-MNIST variants.
This shows why scheduling should prioritize them.
Second, due our scheduler design, F1 approaches compulsory
traffic for most benchmarks, with non\hyp{}compulsory accesses
adding only 5-18\% of traffic.
The exception is LoLa-CIFAR, where intermediates consume 75\% of traffic.
LoLa-CIFAR has very high reuse of key-switch hints,
%which F1 exploits.
and exploiting it requires spilling intermediate ciphertexts.

\figFOneDataMovement
\figFOneOpBreakdown

\paragraph{Power consumption:}
\autoref{fig:f1power} reports average power for each benchmark, broken down by component.
This breakdown also includes off-chip memory power (\autoref{tbl:f1GF12} only included the on-chip component).
Results show reasonable power consumption for an accelerator card.
Overall, computation consumes 20-30\% of power, and data movement dominates.

\paragraph{Utilization over time:}
F1's average FU utilization is about 30\%.
However, this doesn't mean that fewer FUs could achieve the same performance:
benchmarks have memory\hyp{}bound phases 
that weigh down average FU utilization.
%and phases with high FU utilization.
To see this, \autoref{fig:f1opBreakdown} shows a breakdown of FU utilization over 
time for LoLa-MNIST Plaintext Weights.
\autoref{fig:f1opBreakdown} also shows off-chip bandwidth utilization over time (black line).
The program is initially memory-bound, and few FUs are active.
As the memory-bound phase ends, compute intensity grows, 
utilizing a balanced mix of the available FUs.
Finally, due to decoupled execution,
when memory bandwidth utilization peaks again,
F1 can maintain high compute intensity.
The highest FU utilization happens at the end of the benchmark and is caused by processing
the final (fully connected) layer, which is highly parallel and already has all inputs available on-chip.

% TODO(dsm): Maybe mention that 30% is not really low, and reducing compute clusters doesn't help due to memory-bound phases.
% TODO(dsm): With infinite space and time, a histogram or time trace of FU utilization would be nice.

%Figure \cite{fig:dataMovement} details the overall off-chip data movement costs incurred by our scheduled benchmarks.
%By fully exploiting all available key switch hints (KSH) reuse, we are able to only very limited excess data movement in our schedules for LoLa, logistic regression, and bootstrapping at $L = 20$.
%Bootstrapping at larger values of $L$ requires KSH matrices too large to fit entirely on-chip, necessitating additional data movement.
%We reduce this excess data movement by using the low-footprint relinearization method for low-reuse relinearizations at high values of $L$, however, our most performant schedules do not eliminate it entirely.

\subsection{Sensitivity studies}
\label{sec:sensitivity}

\tblFOneSensitivity

To understand the impact of our FUs and scheduling algorithms, we evaluate F1 variants without them.
\autoref{tbl:f1sensitivity} reports the \emph{slowdown (higher is worse)} of F1 with:
\emph{(1)} low\hyp{}throughput NTT FUs that follow the same design as HEAX
(processing one stage of NTT butterflies per cycle); %rather than full pipelining);
\emph{(2)} low\hyp{}throughput automorphism FUs using a serial SRAM memory,
and \emph{(3)} Goodman's register-pressure-aware scheduler~\cite{goodman:ics1988:code}.

For the FU experiments, our goal is to show the importance of having high-throughput units.
Therefore, the low-throughput variants use many more (NTT or automorphism) FUs,
so that aggregate throughput across all FUs in the system is the same.
Also, the scheduler accounts for the characteristics of these FUs.
In both cases, performance drops substantially, by gmean 2.6$\times$ and 3.3$\times$.
This is because achieving high throughput requires excessive parallelism,
which hinders data movement, forcing the scheduler to balance both.



Finally, the scheduler experiment uses register-pressure-aware scheduling~\cite{goodman:ics1988:code}
as the off-chip data movement scheduler instead, operating on the full dataflow graph.
This algorithm was proposed for VLIW processors and register files; we apply it to the larger scratchpad.
The large slowdowns show that prior capacity-aware schedulers are ineffective on F1.


\figFOneConfigs

\subsection{Scalability}
\label{sec:scalability}

Finally, we study how F1's performance changes with its area budget: 
we sweep the number of compute clusters, scratchpad banks, HBM controllers,
and network topology to find the most efficient design at each area.
\autoref{fig:f1pareto} shows this
Pareto frontier, with
area in the $x$-axis and performance in the $y$-axis.
This curve shows that, as F1 scales, it uses resources efficiently:
performance grows about linearly through a large range of areas.

%\vspace{2em} % dsm: Leave enough space for the wrapfig

%\paragraph{LRU vs. Min:} We quantified the performance gains from the fact that we are able to use Belady's Min as a replacement policy instead of a least-recently used (LRU) policy in \ref{tbl:f1sensitivity}.
%As expected, we see no difference for computations that fit or almost-fit on-chip.
%However, for bootstrapping at higher values of $L$, we start to see increasing benefits to our Min replacement policy.

%\paragraph{High Throughput NTT Units:} Compared to existing FPGA FHE accelerators such as HEAX \cite{riazi:asplos20:heax}, F1 uses a smaller number of higher throughput NTT units;
%this allows us to reduce the number of intermediate values stored in our scarce on-chip storage, and make better use of its capacity.

%\paragraph{High Throughput Automorphism Units:} Existing FHE accelerators do not implement high throughput automorphism units, and instead use memories to perform automorphisms \nikola{it is unclear what this means}.
%The importance of this unit is best illustrated by ciphertext rotations:
%each ciphertext rotation requires $2L$ automorphisms and $L^2$ NTTs; therefore, at higher values of $L$, automorphisms are not a bottleneck.
%However, for computations of low values of $L$ (e.g. shallow neural network inference), high automorphism throughput is critical to end-to-end latency.

\begin{comment}
\subsubsection{Relinearization Strategy}

To evaluate the effectiveness of using different relinearization algorithms on different kernels, we choose our bootstrapping benchmarks as they operate at high values of $L$.
We then choose a value $t$ \nikola{what is $t$?} such that all relinearizations with $L > t$ use the low-footprint high-compute relinearization method and all relinearizations with $L \le t$ use the standard approach.
Note that at $t = L$, all relinearizations will use the standard approach. 

In our bootstrapping algorithm, we perform many distinct automorphisms on a single ciphertext at our maximum $L$ value, leading to a lot of KHS matrices with no reuse.
While the computation blow-up from the non-standard relinearization method is substantial, we notice significant speedups when we use the non-standard method for all operations at the highest value of $L$.
Performance figures are presented in figure \ref{fig:f1sweepRelinThresh}.

\end{comment}
