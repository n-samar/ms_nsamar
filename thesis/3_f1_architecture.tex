\section{F1 Architecture}\label{sec:f1_arch}

\autoref{fig:f1arch} shows an overview of F1, which we derive from the insights
in \autoref{sec:fhe_analysis}.

\paragraph{Vector processing with specialized functional units:}
F1 features wide-vector execution with functional units (FUs) tailored to
primitive FHE operations. Specifically, F1 implements vector FUs for modular
addition, modular multiplication, NTTs (forward and inverse in the same unit),
and automorphisms. Because we leverage RNS representation, these FUs use a
fixed, small arithmetic word size (32 bits in our implementation), avoiding
wide arithmetic.

FUs process vectors of configurable \emph{length} $N$ using a fixed number of
\emph{vector lanes} $E$. Our implementation uses $E=$128 lanes and supports
power-of-two lengths $N$ from 1,024 to 16,384. This covers the common range of
FHE polynomial sizes, so an RNS polynomial maps to a single vector. Larger
polynomials (e.g., of 32K elements) can use multiple vectors.

All FUs are \emph{fully pipelined}, so they achieve the same throughput of
$E=128$ elements/cycle. FUs consume their inputs in contiguous chunks of $E$
elements in consecutive cycles. This is easy for element-wise operations, but
hard for NTTs and automorphisms. \autoref{sec:f1_fus} details our novel FU
implementations, including the first vector implementation of automorphisms.
F1's evaluation shows that these FUs achieve much higher performance than those
of prior work. This is important because, as we saw in
\autoref{sec:fhe_analysis}, \emph{having fewer high-throughput FUs reduces
parallelism and thus memory footprint}.

\paragraph{Compute clusters:}
Functional units are grouped in \emph{compute clusters}, as
\autoref{fig:f1arch} shows. Each cluster features several FUs (1 NTT, 1
automorphism, 2 multipliers, and 2 adders in our implementation) and a banked
register file that can (cheaply) supply enough operands each cycle to keep all
FUs busy. The chip has multiple clusters (16 in our implementation).

\figFOneArch

\paragraph{Memory system:}
F1 features an explicitly managed memory hierarchy. As \autoref{fig:f1arch}
shows, F1 features a large, heavily banked scratchpad (64\,MB across {16} banks
in our implementation). The scratchpad interfaces with both high-bandwidth
off-chip memory (HBM2 in our implementation) and with compute clusters through
an on-chip network.

F1 uses decoupled data orchestration~\cite{pellauer:asplos19:buffets} to hide
main memory latency. Scratchpad banks work autonomously, fetching data from
main memory far ahead of its use. Since memory has relatively low bandwidth,
off-chip data is always staged in scratchpads, and compute clusters do not
access main memory directly.

The on-chip network connecting scratchpad banks and compute clusters provides
very high bandwidth, which is necessary because register files are small and
achieve limited reuse. We implement a single-stage bit-sliced crossbar
network~\cite{passas:tocaid12:crossbar} that provides full bisection bandwidth.
Banks and the network have wide ports (512 bytes), so that a single scratchpad
bank can send a vector to a compute unit at the rate it is consumed (and
receive it at the rate it is produced). This avoids long staging of vectors at
the register files.

\paragraph{Static scheduling:}
Because FHE programs are completely regular, F1 adopts a \emph{static, exposed
microarchitecture}: all components have fixed latencies, which are exposed to
the compiler. The compiler is responsible for scheduling operations and data
transfers in the appropriate cycles to prevent structural or data hazards. This
is in the style of VLIW processors~\cite{fisher:isca83:very}.

Static scheduling simplifies logic throughout the chip. For example, FUs need
no stalling logic; register files and scratchpad banks need no dynamic
arbitration to handle conflicts; and the on-chip network uses simple switches
that change their configuration independently over time, without the buffers
and arbiters of packet-switched networks.

Because memory accesses do have a variable latency, we assume the worst-case
latency, and buffer data that arrives earlier (note that, because we access
large chunks of data, e.g., 64\,KB, this worst-case latency is not far from the
average).

\paragraph{Distributed control:}
Though static scheduling is the hallmark of VLIW, F1's implementation is quite
different: rather than having a single stream of instructions with many
operations each, in F1 each component has an \emph{independent instruction
stream}. This is possible because F1 does not have any control flow: though FHE
programs may have loops, we unroll them to avoid all branches, and compile
programs into linear sequences of instructions.

This approach may appear costly. But vectors are very long, so each instruction
encodes a lot of work and this overhead is minimal. Moreover, this enables a
compact instruction format, which encodes a single operation followed by the
number of cycles to wait until running the next instruction. This encoding
avoids the low utilization of VLIW instructions, which leave many operation
slots empty. Each FU, register file, network switch, scratchpad bank, and
memory controller has its own instruction stream, which a control~unit~fetches
in small blocks and distributes to components. Overall, instruction fetches
consume less than 0.1\% of memory traffic.

\paragraph{Register file (RF) design:}
Each cluster in F1 requires 10 read ports and 6 write ports to keep all FUs
busy. To enable this cheaply, we use an 8-banked \emph{element-partitioned}
register file design~\cite{asanovic:ucb98:vector} that leverages long vectors:
each vector is striped across banks, and each FU cycles through all banks over
time, using a single bank each cycle. By staggering the start of each vector
operation, FUs access different banks each cycle. This avoids multiporting,
requires a simple RF-FU interconnect, and performs within 5\% of an ideal
infinite-ported RF.
