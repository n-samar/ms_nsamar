\chapter{Introduction}\label{sec:intro}

Despite massive efforts to improve the security of computer systems, security
breaches are only becoming more frequent and damaging, as more sensitive data
is processed in the
cloud~\cite{malekos-smith:csis20:hidden-costs-cybercrime,ibm20:breach-cost-report}.
Current encryption technology is of limited help, because servers must decrypt
data before processing it. Once data is decrypted, it is vulnerable to
breaches.

\figWorkflow

Fully Homomorphic Encryption (FHE) is a class of encryption schemes that
address this problem by enabling \emph{generic computation on encrypted data}.
\autoref{fig:workflow} shows how FHE enables secure offloading of computation.
The client wants to compute an expensive function $f$ (e.g., a deep learning
inference) on some private data $x$. To do this, the client encrypts $x$ and
sends it to an untrusted server, which computes $f$ on this encrypted data
\emph{directly} using FHE, and returns the encrypted result to the client. FHE
provides ideal security properties: even if the server is compromised,
attackers cannot learn anything about the data, as it remains encrypted
throughout.


FHE is a young but quickly developing technology. First realized in
2009~\cite{gentry09}, early FHE schemes were about 10$^9$ times slower than
performing computations on unencrypted data. Since then, improved FHE schemes
have greatly reduced these overheads and broadened its
applicability~\cite{albrecht:hesg18:standard,peikert2016decade}. FHE has
inherent limitations---for example, data-dependent branching is impossible,
since data is encrypted---so it won't subsume all computations. Nonetheless,
important classes of computations, like deep learning
inference~\cite{cheon:ictaci17:homomorphic,dathathri:pldi19:chet,dathathri:pldi20:eva},
linear algebra, secure genome analysis, private set intersection, information
retrieval, and other inference and learning
tasks~\cite{kim2020semi,gilad:icml16:cryptonets,han:aaai19:logistic,han:iacr18:efficient,juvekar2018gazelle,DBLP:conf/ccs/ChenLR17,DBLP:conf/tcc/GentryH19}
are a good fit for FHE. This has sparked significant industry and government
investments~\cite{ibm,intel,dprive} to widely deploy FHE.

Unfortunately, FHE still carries substantial performance overheads: despite
recent advances~\cite{dathathri:pldi19:chet, dathathri:pldi20:eva,
roy:hpca19:fpga-he, brutzkus:icml19:low, polyakov:17:palisade}, FHE is still
10,000$\times$ to 100,000$\times$ slower than unencrypted computation when
executed in carefully optimized software. Though this slowdown is large, it can
be addressed with hardware acceleration: \emph{if a specialized FHE accelerator
provides large speedups over software execution, it can bridge most of this
performance gap and enable new use cases.}

\section{Challenges and Opportunities of FHE Acceleration}
\label{sec:general}

For an FHE accelerator to be broadly useful, it should be programmable, i.e.,
capable of executing arbitrary FHE computations. While prior work has proposed
several FHE accelerators, they do not meet this goal. Prior FHE
accelerators~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,riazi:asplos20:heax,turan:tc20:heaws}
target some FHE operations, but miss important ones that they leave to
software. These designs are FPGA-based, so they are small and miss the data
movement issues facing an FHE ASIC accelerator. These designs also
overspecialize their functional units to specific parameters and algorithms,
and cannot efficiently handle the range of parameters needed within a program
or across programs.

We identify four key characteristics of FHE that drive our architectures:

\noindent \textbf{\emph{(1) Complex operations on long vectors:}}
FHE encodes information using very large vectors, several thousand elements
long, and processes them using modular arithmetic. Therefore, an FHE
accelerator can employ \emph{vector processing} with \emph{wide functional
units} tailored to FHE operations to achieve large speedups. The challenge is
that two key operations on these vectors, the Number-Theoretic Transform (NTT)
and automorphisms, are not element-wise and require complex dataflows that are
hard to implement as vector operations. This is one of the key reasons why
existing vector processors and GPUs achieve limited speedups for FHE.

\noindent \textbf{\emph{(2) Regular computation:}}
FHE programs are dataflow graphs of arithmetic operations on vectors. All
operations and their dependences are known ahead of time (since data is
encrypted, branches or dependences determined by runtime values are
impossible). An accelerator can exploit this by adopting \emph{static
scheduling}: in the style of Very Long Instruction Word (VLIW) processors, all
components have fixed latencies and the compiler is in charge of scheduling
operations and data movement across components, with no hardware mechanisms to
handle hazards (i.e., no stall logic). With this approach, an accelerator can
issue many operations per cycle with minimal control overheads; combined with
vector processing, an effective FHE accelerator can issue tens of thousands of
scalar operations per cycle.

\noindent \textbf{\emph{(3) Challenging data movement:}}
In FHE, encrypting data increases its size (typically by at least 50$\times$);
data is grouped in long vectors; and some operations require large amounts
(tens of MBs) of auxiliary data. Thus, we find that data movement is \emph{the
key challenge} for FHE acceleration: despite requiring complex functional
units, in current technology, limited on-chip storage and memory bandwidth are
the bottleneck for most FHE programs. Therefore, the primary goal of an FHE
accelerator should be to minimize data movement. To that end, we design our
accelerators to:
\begin{compactenum}
\item Use a large and explicitly managed on-chip storage.
\item Decouple data movement. This allows hiding access latencies by
    loading data far ahead of its use.
\item Focus on maximizing operand reuse when scheduling operations. This reduces
    memory bandwidth pressure.
\item Build relatively \emph{few functional units with extremely high
    throughput}, rather than many lower-throughput functional units as in prior
    work. This \emph{reduces the amount of data that must reside on-chip
    simultaneously}, allowing higher reuse.
\end{compactenum}

\noindent \textbf{\emph{(4) Algorithmic diversity:}}
FHE is a young and quickly developing field. As a result, there is abundant
diversity in proposed schemes and algorithms. For example, keyswitching
(\autoref{sec:fhe_analysis}, \autoref{sec:boostedKeyswitching}), a key FHE
kernel, has many variants with diverse performance-security tradeoffs
(\autoref{sec:boostedSecurity}). An effective FHE accelerator needs to support
all these algorithms well, and be flexible enough to adapt to future advances
in the field.

\section{Contributions}
\label{sec:contributions}

This thesis presens two FHE accelerators: F1 and CraterLake. We first introduce
F1, the first \emph{programmable} FHE accelerator. Then we explain why deep FHE
computations require operating in a different regime from what F1 can
effectively support. Finally, we present how CraterLake overcomes these issues,
making it the first accelerator to support FHE computations of \emph{unbounded
depth}.

\paragraph{F1} is a wide-vector processor with novel functional units deeply
specialized to FHE primitives, such as modular arithmetic, number-theoretic
transforms, and automorphisms. F1 is a multicore architecture, with compute
logic organized into 16 identical clusters connected by an on-chip network.
This organization provides so much compute throughput that data movement
becomes the key bottleneck. Thus, F1 is primarily designed to minimize data
movement. Hardware provides an explicitly managed memory hierarchy and
mechanisms to decouple data movement from execution. A novel compiler leverages
these mechanisms to maximize reuse and schedule off-chip and on-chip data
movement.

F1 is the first FHE accelerator that is programmable, i.e., capable of executing
full FHE programs. In contrast to prior accelerators, which build fixed pipelines
tailored to specific FHE schemes and parameters, F1 introduces a more effective
design approach: it accelerates the \emph{primitive} computations shared by
higher-level operations unisng novel high-throughpuh functional units.

We implement the main components of F1 in RTL and synthesize them in a
commercial 14nm/12nm process. With a modest area budget of 151\,mm$^2$, our F1
implementation provides 36 tera-ops/second of 32-bit modular arithmetic, 64\,MB
of on-chip storage, and a 1\,TB/s high-bandwidth memory. We evaluate F1 using
cycle-accurate simulation running complete FHE applications, and demonstrate
speedups of 1,200$\times$--17,000$\times$ on shallow benchmarks over
state-of-the-art software implementations.

In short, F1's key contribution is to show that, for FHE, we can achieve
ASIC-level performance without sacrificing programmability.


\paragraph{CraterLake}, our second FHE accelerator, leverages our insights from
designing F1 while addressing its major drawback: namely, F1 is efficient only
on a limited subset of simple FHE computations---those of \emph{shallow
multiplicative depth}. For example, F1 can run neural network inference
efficiently only for networks with few layers (3-6), but it cannot accelerate
state-of-the-art deep neural networks (DNNs) with tens to hundreds of layers.

This is because deep FHE programs require large ciphertexts and scalable
algorithms that F1 is not designed for (\autoref{sec:deepChallenges}).

To tackle this challenge, we introduce the \emph{CraterLake} architecture
(\autoref{ch:craterlake}), the first FHE accelerator
that achieves high performance on unbounded FHE programs. CraterLake builds on
many of F1's key insights: specilized vector functional units, static
scheduling, and a focus on data movement. However, CraterLake succesfully
addresses F1's shortcomings by contributing:
\begin{compactitem}
\item A new extremely wide (2,048 lanes) vector \emph{uniprocessor}
    architecture that spreads each vector operation across the chip, departing
    from F1's multicore architecture. The uniprocessor approach reduces the
    number of concurrent operations, which minimizes footprint, reducing
    off-chip traffic, and simplifies the compiler.
\item An efficient implementation of this uniprocessor, which is
    challenging for non-SIMD FHE operations, NTTs and automorphisms, by
    decomposing these operations in a novel way that allows the use of a
    \emph{fixed transpose network} among physically distributed groups of
    lanes. This reduces on-chip data movement and interconnect cost over F1's
    approach.
\item The change RNS base (CRB) unit, a new functional unit that encapsulates
    the bulk of operations in keyswitching, improving efficiency and
    enabling high utilization across ciphertexts of all sizes. CraterLake also
    contributes the keyswith hint generator (KSHGen), a new functional unit
    that generates half of the required auxiliary data on the fly (reducing
    overheads from 52\,MB to 26\,MB), saving on-chip storage and memory
    bandwidth.
\item A vector chaining technique that builds long FU pipelines to enable many
    concurrent operations with few register ports.
\end{compactitem}

We evaluate CraterLake through a combination of simulation and RTL synthesis
(to find its area and power). F1 area budget is modest (151\,mm$^2$) and
delivers good performance, but it is insufficient for deep FHE benchmarks.
CraterLake, a 472\,mm$^2$ chip, is sufficient to effectively accelerate
unbounded depth FHE programs.

To evaluate CraterLake, we use a broad range of FHE benchmarks, including
programs with high multiplicative depth that require packed bootstrapping, an
FHE algorithm critical to deep computation which the original F1 accelerator
cannot support without losing orders of magnitude of performance
(\autoref{sec:deepChallenges}). CraterLke outperforms a scaled-up and improved
version of F1, by gmean 11.2$\times$ on these deep computations, and is
4,600$\times$ faster than a 32-core CPU. These speedups enable new use cases
for FHE. For example, deep neural networks like ResNet take 23 minutes per
inference on the CPU, whereas CraterLake achieves 250 \emph{milliseconds} per
inference, enabling real-time private deep learning.
