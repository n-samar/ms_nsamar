\chapter{Introduction}
\label{sec:intro}

% TODO(group): Spend more time on motivation, concrete example. Link to Figure 1. Point to recent efforts by companies, government, academia as high-level motivation.

Despite massive efforts to improve the security of computer systems,
security breaches are only becoming more frequent and damaging,
as more sensitive data is processed in the cloud~\cite{malekos-smith:csis20:hidden-costs-cybercrime,ibm20:breach-cost-report}.
Current encryption technology is of limited help,
%they protect data in transit or at rest,
because servers must decrypt data before processing it.
Once data is decrypted, it is vulnerable to breaches.

Fully Homomorphic Encryption (FHE) is a class of encryption schemes that
address this problem by enabling \emph{generic computation on encrypted data}.
\autoref{fig:overview} shows how FHE enables secure offloading of computation.
The client wants to compute an expensive function $f$
(e.g., a deep learning inference) on some private data $x$.
To do this, the client encrypts $x$ and sends it to an untrusted server,
which computes $f$ on this encrypted data \emph{directly} using FHE,
and returns the encrypted result to the client.
%
FHE provides ideal security properties: even if the server is compromised,
attackers cannot learn anything about the data,
as it remains encrypted throughout.

% dsm: Split 
FHE is a young but quickly developing technology.
First realized in 2009~\cite{gentry09}, early FHE schemes were
about 10$^9$ times slower than performing computations on unencrypted data.
Since then, improved FHE schemes have greatly reduced these overheads
and broadened its applicability~\cite{albrecht:hesg18:standard,peikert2016decade}.
FHE has inherent limitations---for example, data-dependent branching is impossible,
since data is encrypted---so it will not subsume all computations.
Nonetheless, important classes of computations, like deep learning inference~\cite{cheon:ictaci17:homomorphic,dathathri:pldi19:chet,dathathri:pldi20:eva},
linear algebra~\cite{halevi:crypto14:algorithms}, genomics~\cite{blatt:nas20:secure}, and other inference and learning tasks~\cite{han:aaai19:logistic} are a good fit for FHE.
This has sparked significant industry and government investments~\cite{ibm,intel,dprive}
to widely deploy FHE.

\figOverview

Unfortunately, FHE still carries substantial performance overheads:
despite recent advances~\cite{dathathri:pldi19:chet, dathathri:pldi20:eva, roy:hpca19:fpga-he, brutzkus:icml19:low, polyakov:17:palisade},
FHE is still 10,000$\times$ to 100,000$\times$ slower than unencrypted computation when executed in carefully optimized software.
Though this slowdown is large, it can be addressed with hardware acceleration:
\emph{if a specialized FHE accelerator provides large speedups over software execution,
it can bridge most of this performance gap and enable new use cases.}

For an FHE accelerator to be broadly useful, it should be programmable, i.e., capable of executing arbitrary FHE computations.
%and flexible, i.e., capable of achieving high performance over a wide range of parameters and, ideally, be able to accelerate multiple FHE schemes (such as BGV for integer operations and CKKS for fixed-point and approximate computations).
% TODO(group): FHE schemes were not defined, and caused general confusion. Try to avoid introducing them till later. Focus on programmability, and make it clear what a program is (examples).
While prior work has proposed several FHE accelerators, they do not meet this goal.
Prior FHE accelerators~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,riazi:asplos20:heax,turan:tc20:heaws} target individual FHE operations,
and miss important ones that they leave to software.
These designs are designed for a Field Programmable Gate Arrays (FPGA), so they are small and 
miss the data movement issues facing an FHE Application specific Integrated Circuit (ASIC) accelerator.
These designs also overspecialize their functional units to specific parameters,
and cannot efficiently handle the range of parameters needed within a program or across programs.
% dsm: Note that HEAX etc don't vary Q, right? It's not even that they don't allow for variable N.
%
%Specifically, prior FHE accelerators~\cite{roy:hpca19:fpga-he, riazi:asplos20:heax} are small and target FPGAs, 
%missing the opportunities and challenges of an ASIC design.
%First, existing accelerators target individual FHE operations, and miss important operations that they delegate to software.
%Second, FPGA designs miss the large data movement issues that an FHE ASIC accelerator incurs,
%which requires new techniques to decouple and minimize data movement.
%Third, because they are FPGA-based, existing designs are tailored to specific parameters,
%whereas an FHE ASIC must balance specialization and flexibility \nikola{this seems like a disadvantage for ASICs?}.
% dsm: Now discussed only in related work, since it's a bit of a detour here
%Recent work has also proposed ASIC accelerators for some homomorphic encryption primitives
%in the context of oblivious neural networks~\cite{juvekar:usenixsecurity18:gazelle,reagen:hpca21:cheetah}, but these approaches target a fixed algorithm
%and combine homomorphic encryption with multi-party computation,
%so their techniques are quite different from FHE.
% TODO(group): Trim a bit; align with later description of challenges.

In this thesis we present \name, the first programmable FHE accelerator.
\name builds on an in-depth architectural analysis of the characteristics of FHE computations, which exposes the main challenges and reveals the design principles a programmable FHE architecture should exploit.

%Based on this analysis, we design \name as a \emph{wide vector processor} with \emph{specialized functional units},
%\emph{static VLIW scheduling}, and a \emph{scratchpad-based memory hierarchy} with techniques to \emph{decouple data movement from computation}.

% TODO(group): This part should include the minimum infor on FHE schemes that conveys the challenges:
%  - Operations on long vectors (~10K elems); complex transforms that don't map to SIMD/vector execution.
%  - Ciphertext much larger than plaintext + some operations use large auxiliary data -> data movement challenges
%  - Regular computation: No data-dependent branching, all operations and dependences known ahead -> static scheduling.
\paragraph{Harnessing opportunities and challenges in FHE:}
\name is tailored to the three defining characteristics of FHE:

\noindent \textbf{\emph{(1) Complex operations on long vectors:}}
FHE encodes information using very large vectors, several thousand elements long,
and processes them using modular arithmetic.
%
\name employs \emph{vector processing} with \emph{wide functional units} tailored to FHE operations
to achieve large speedups.
The challenge is that two key operations on these vectors, the Number-Theoretic Transform (NTT) and automorphisms, are not element-wise and require complex dataflows that are hard to implement as vector operations. 
To tackle these challenges, \name features specialized NTT units and the first vector implementation of an automorphism functional unit.

\noindent \textbf{\emph{(2) Regular computation:}}
FHE programs are dataflow graphs of arithmetic operations on vectors.
All operations and their dependences are known ahead of time (since data is encrypted, branches
or dependences determined by runtime values are impossible).
%
\name exploits this by adopting \emph{static scheduling}:
in the style of Very Long Instruction Word (VLIW) processors,
all components have fixed latencies and the compiler is in charge of scheduling
operations and data movement across components, with no hardware mechanisms to handle hazards (i.e., no stall logic).
%A scheduler exploits knowledge of the computation to be performed  to orchestrate operations across several clusters of functional units.
Thanks to this design, \name can issue many operations per cycle with minimal control overheads;
%out the complexities of superscalar processors;
combined with vector processing, \name can issue tens of thousands of scalar operations per cycle. %in a programmable way
%with minimal control overheads.

\noindent \textbf{\emph{(3) Challenging data movement:}}
In FHE, encrypting data increases its size (typically by at least 50$\times$);
data is grouped in long vectors; and some operations require large amounts (tens of megabytes) of auxiliary data.
Thus, we find that data movement is \emph{the key challenge} for FHE acceleration:
despite requiring complex functional units, in current technology, limited on-chip storage and memory bandwidth are the bottlenecks for most FHE programs.
%
Therefore, \name is primarily designed to minimize data movement.
First, \name features an explicitly managed on-chip memory hierarchy,
with a heavily banked scratchpad and distributed register files.
Second, \name uses mechanisms to decouple data movement and hide access latencies by loading data far ahead of its use.
Third, \name uses new, FHE-tailored scheduling algorithms that maximize reuse and make the best out of limited memory bandwidth.
Fourth, \name uses relatively \emph{few functional units with extremely high throughput}, rather than lower-throughput functional units as in prior work.
This \emph{reduces the amount of data that must reside on-chip simultaneously}, allowing higher reuse.

In summary, \name brings decades of research in architecture to bear, including vector processing and static scheduling, and combines them with new techniques and scheduling algorithms (\autoref{sec:scheduler}), to design a programmable FHE accelerator.
The main components of \name were implemented in RTL and synthesized in a commercial 14nm/12nm process.
With a modest area budget of 151\,mm$^2$, our \name implementation 
%features functional units that provide a modular arithmetic throughput exceeding 
% dsm: 128*14+128*4 = 2304 GOps/cluster, *16 -> 36.864 TOps
provides 36 tera-ops/second of 32-bit modular arithmetic, 64\,MB of on-chip storage, and a 1\,TB/s high-bandwidth memory.
% dsm: Not very relevant, and definitely less relevant than making the point on apps.
%This system is the first to accelerate all FHE operations, and supports multiple FHE schemes (BGV, CKKS, and GSW).

We evaluate \name using cycle-accurate simulation running complete FHE applications,
and demonstrate speedups of 1,200$\times$--17,000$\times$ over state-of-the-art software implementations.
These dramatic speedups counter most of FHE's overheads and enable new applications.
For example, \name executes a deep learning inference that used to take 20 minutes in 240 milliseconds,
enabling secure real-time deep learning in the cloud.

\section{Contributions}

\name is a collaborative project. This thesis describes the full design and implementation of \name, while emphasizing the author's key contributions:

\begin{itemize}
\item \textbf{Design and implementation of the \name software stack:} We implement a complete software stack that takes high level descriptions of FHE computations and compiles them down to \name instructions issued at precise cycles. Because our computations are statically scheduled, the scheduled programs also provide the basis for our performance evaluation (\autoref{sec:scheduler}, \autoref{sec:implementation}).
\item \textbf{Design space exploration:} Using the \name scheduler, we perform a design-space exploration to select efficient configurations of our accelerator (\autoref{sec:scalability}).
\item \textbf{FHE benchmark programs:} We implement FHE programs from the literature in the \name DSL to evaluate our accelerator's performance on relevant workloads (\autoref{sec:benchmarks}).
\end{itemize}

%\paragraph{Contributions:} In summary, in this paper, we
%(1) articulate the architectural design principles needed for programmable FHE acceleration (e.g., vector processing, static scheduling, VLIW, specialized hardware functional units, and decoupled data movement); (2) design and present an architecture that leverages these principles; and (3) evaluate the performance, area, and power of this architecture with a combination of timing simulation and logic synthesis. 
%We demonstrate speedups of \tmp{XXX-YYY}  on important computations.



