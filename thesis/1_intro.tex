\section{Introduction}\label{sec:intro}

A large and increasing fraction of the world's compute runs on the cloud,
which is vulnerable to data breaches.
Conventional techniques to mitigate attacks offer limited 
security, as cloud servers must decrypt data in order to process it.

\emph{Fully homomorphic encryption (FHE)} is a special type of encryption scheme
that enables \emph{computing on encrypted data directly}, without decrypting it.
FHE allows a client to offload a computation
to an untrusted server \emph{without} revealing any data (\autoref{fig:workflow}).
This enables the client to harness the compute power of the cloud while maintaining cryptographic privacy.
Though FHE has some limitations (e.g., data-dependent branching is not possible), 
it is general enough to support many compelling use cases,
such as privacy-preserving machine learning, secure genome analysis, private set intersection,
private information retrieval, and many more~\cite{kim2020semi,gilad:icml16:cryptonets,han:aaai19:logistic,han:iacr18:efficient,juvekar2018gazelle,DBLP:conf/ccs/ChenLR17,DBLP:conf/tcc/GentryH19}. 
%\nnote{add citations and more examples}

% axelf: purging unfortunately/fortunately from the whole paper
Despite its ideal privacy, FHE is rarely used today because it incurs prohibitive overheads:
in CPUs, FHE computations are 10,000$\times$ to 100,000$\times$
slower than equivalent unencrypted computations, even when using highly optimized FHE libraries.
% dsm: I don't think this adds much here, other than distance to get to the actual point
%Some of these overhead costs are due
%to the size of FHE ciphertexts while
%others are due to the additional complexity
%of representing arbitrary functions in a form suitable for
% computing using FHE.

Fortunately, state-of-the-art FHE schemes are well-suited to hardware acceleration.
First, they are regular and structured:
FHE programs operate on very long vectors, and all operations are known ahead of time.
Second, FHE requires several non-SIMD operations,
such as \emph{number-theoretic transforms} (NTTs),
that are inefficient on CPUs and GPUs.
But these operations can be accelerated by specialized functional units,
avoiding these inefficiencies.
As a result, prior work has proposed FPGA and ASIC-based
accelerators~\cite{riazi:asplos20:heax,cousins:hpec12:sipher-fpga,cousins:tetc17:fpga-he,turan:tc20:heaws,cousins:hpec14:fpga-he, roy:hpca19:fpga-he,feldmann:micro21:f1}.
While most prior accelerators achieve limited speedups, a recent design,
F1~\cite{feldmann:micro21:f1}, achieves speedups of
%2,000-15,000$\times$ 
% dsm: Let's avoid the 15000x speedup, we don't have anything that large but we're using a different baseline, diff configs, etc.
about 5,000$\times$
on FHE programs.

% dsm: Unfortunately here is important. We were just describing potential, we want to signpost that we're entering negative territory.
Unfortunately, prior accelerators are efficient only on a limited subset of simple FHE
computations---those of \emph{shallow multiplicative depth}.
% FIXME(dsm): I think this may be from edits, but right now several terms are left undefined: DEPTH and MULTIPLICATIVE DEPTH. I think this is because the text below does not talk about multiplications.
For example, prior FHE accelerators can run neural network inference efficiently only for networks with few layers (3-6),
but they cannot accelerate state-of-the-art deep neural networks (DNNs) with tens to hundreds of layers.

This limitation stems from the characteristics of FHE schemes:
each ciphertext has some associated noise, which grows with each homomorphic operation, and especially with multiplications.
If noise becomes too large, it garbles the message, making decryption impossible. 
%To support computations with higher multiplicative depth, the size of ciphertexts must increase to counteract the additional noise growth, resulting in each FHE operation being more computationally expensive.
Larger ciphertexts tolerate more noise before becoming undecryptable. 
However, operations on larger ciphertexts are also more expensive.
To enable computations of unbounded depth, ciphertexts can be ``refreshed'' using a procedure called \emph{bootstrapping}
that reduces noise. But bootstrapping is expensive, 
% dsm: multiplicative depth is NOT YET DEFINED
%and itself consumes multiplicative depth, 
so ciphertexts must be very large (10s of MBs) for bootstrapping to be
efficient.

Prior FHE accelerators do not efficiently handle unbounded-depth computations because
they natively support vectors of a limited size and they use algorithms that scale poorly
to the large ciphertexts in high-depth programs.
As a result, they can only run small FHE computations, and they do not support sufficient depth
to run the full bootstrapping procedure.

\figWorkflow

In this paper we tackle this challenge through \name, the first FHE accelerator
to support \emph{FHE computations of unbounded depth}.
To achieve this, we contribute new algorithms, specialized functional units,
hardware architecture, and compiler techniques that overcome the key challenge
of deep FHE computations---its extreme data movement demands.

\paragraph{Deep FHE is limited by data movement:}
FHE schemes encode information over very long vectors of wide elements.
Concretely, supporting unbounded-depth computations requires vectors of
64K elements with 1,600 bits per element.
This takes 25\,MB per ciphertext, 12$\times$ larger than what prior FHE accelerators target.
% nikola; 25MB = 2 * 1600 bits * 64*1024 elements * (1/8) bytes / bit * 1/2^20 bytes / MB

% (alex): I changed the comparisson from to 32 MB to 23 MB ciphertexts;
%         Being charittable, this is L = 1500/32 = 46 for F1 and KSH is 23*46 = 1058MB
% nikola: 2MB ciphertext in F1 => 1MB per ciphertext polynomial => 512 bits per coeff at N=16K
% => L = 16 (32-bit moduli) => KSH = 2*L^2*N * 32 bits / elem = 32 MB
% nikola: 26MB ciphertext in F1 => 13MB per ciphertext polynomial + assume N=64K
% => log Q = 1,664 => L = 52 => KSH = 2*L^2*N * 32 bits / elem * (1/8) B/bit * 1/2^30 GB/B
% = 1.32 GB
% At the same log Q = 1,664, CL needs L = 1664/28 = 60: KSH = 2*2*L*N = 
% = 2*2 * 60 (L) * 64*1024 (N) * 28 bits / elem * (1/8) B/bit * 1/2^20 MB/B = 52.5 MB
% (note that L is actually 1668/26.7, but we divide by 28 to be consistent with the F1
% methodology, where we multiply by 32)
Moreover, prior work has employed FHE algorithms that require even larger amounts of auxiliary data.
For example, multiplying 2\,MB ciphertexts in F1~\cite{feldmann:micro21:f1} requires 32\,MB of auxiliary data,
and scaling their algorithm to 26\,MB ciphertexts would require over
1.3\,GB 
of auxiliary data---far too large to fit on-chip.
To tackle this challenge, our \emph{key insight} is to adopt an FHE
algorithm called \emph{boosted keyswitching} (\autoref{sec:keyswitching}) that eliminates most of the
auxiliary data,
% (alex): KSH it's 2 ciphertexts without PRG
reducing this overhead from 1.3\,GB to 52.5\,MB.
%We further reduce this overhead to 23\,MB by introducing a new functional unit that
%generates half of this auxiliary data on the fly, rather than fetching it from memory.
Boosted keyswitching also reduces computation costs.
However, this new algorithm is a poor match for prior accelerators:
it is dominated by simple operations where these designs have limited efficiency,
and makes poor use of the specialized functional units that prior designs leverage.

Beyond being inefficient, prior accelerators suffer from a hard-to-scale vector multicore architecture:
to support the needed non-SIMD operations with reasonable cost,
they implement multiple independent cores with narrower vector datapaths~\cite{feldmann:micro21:f1}.
However, this causes excessive inter-core communication,
and the high-bandwidth interconnect
needed grows superlinearly with the number of cores.


\paragraph{Deep FHE demands new hardware techniques:}
To tackle these challenges, we introduce the \emph{\name} architecture (\autoref{sec:overview}, \autoref{sec:architecture}),
the first FHE accelerator that achieves high performance on unbounded FHE programs.
\name is a wide-vector uniprocessor with specialized functional units.
The design is statically scheduled to leverage the regularity of FHE computations.
We contribute several new techniques that make this possible, including:
\begin{compactitem}
\item A new extremely wide (2,048 lanes) vector uniprocessor architecture that
    spreads each vector operation across the chip, departing from prior vector multicore architectures.
The uniprocessor approach reduces the number of concurrent operations, which minimizes footprint,
  reducing off-chip traffic, and simplifies the compiler.
\item An efficient implementation of the above architecture, which is challenging for non-SIMD FHE operations, NTTs and automorphisms,
  by decomposing these operations in a novel way that allows the use of a \emph{fixed transpose network} among physically distributed groups of lanes.
  This reduces on-chip data movement and interconnect cost over prior approaches.
\item A new functional unit that encapsulates the bulk of operations in boosted keyswitching, improving efficiency and enabling high utilization across ciphertexts of all sizes.
\item A new functional unit that generates half of the required auxiliary data on the fly (reducing overheads from 52\,MB to 26\,MB), saving on-chip storage and memory bandwidth.
\item A vector chaining technique that builds long FU pipelines to enable many concurrent operations with few register ports.
\end{compactitem}
To program \name, we develop a novel compiler (\autoref{sec:compiler}) that produces efficient code from high-level FHE programs.
The compiler schedules operations to maximize reuse, decouples data movement from computation,
and adapts the state-of-the-art bootstrapping algorithm to achieve high
utilization~\cite{bossuat:crypto21:efficient}.


We evaluate \name through a combination of simulation and RTL synthesis (to find its area and power).
We use a broad range of FHE benchmarks, including programs with high multiplicative depth
that require bootstrapping.
\name outperforms a scaled-up and improved version of the state-of-the-art FHE accelerator, F1, by gmean 
11.2$\times$ on these deep computations,
and is 4,600$\times$ faster than a 32-core CPU.
These speedups enable new use cases for FHE.
For example, deep neural networks like ResNet
% which takes tens of \emph{minutes} per inference on CPUs,
% nikola: again, it really takes tens of minutes, but that's cuz of hours algo improvements
% dsm: Let's not confuse the reader, we can mention that this used to be hours in the eval but saying hours to ms is comparing apples to oranges
take 23 minutes per inference on the CPU,
whereas \name achieves 250 \emph{milliseconds} per inference,
enabling real-time private deep learning.
% 23 minutes = 1.37694e+12 ns / (10^9*60) = 22.9 min
% 265 ms = 2.64245e+08 / (10^6) = 264 ms

