\chapter{Introduction}\label{sec:intro}

A large and increasing fraction of the world's compute runs on the cloud,
which is vulnerable to data breaches.
Conventional techniques to mitigate attacks offer limited 
security, as cloud servers must decrypt data in order to process it.

\emph{Fully homomorphic encryption (FHE)} is a special type of encryption scheme
that enables \emph{computing on encrypted data directly}, without decrypting it.
FHE allows a client to offload a computation
to an untrusted server \emph{without} revealing any data (\autoref{fig:workflow}).
This enables the client to harness the compute power of the cloud while maintaining cryptographic privacy.
Though FHE has some limitations (e.g., data-dependent branching is not possible), 
it is general enough to support many compelling use cases,
such as privacy-preserving machine learning, secure genome analysis, private set intersection,
private information retrieval, and many more~\cite{kim2020semi,gilad:icml16:cryptonets,han:aaai19:logistic,han:iacr18:efficient,juvekar2018gazelle,DBLP:conf/ccs/ChenLR17,DBLP:conf/tcc/GentryH19}. 
%\nnote{add citations and more examples}

% axelf: purging unfortunately/fortunately from the whole paper
Despite its ideal privacy, FHE is rarely used today because it incurs prohibitive overheads:
in CPUs, FHE computations are 10,000$\times$ to 100,000$\times$
slower than equivalent unencrypted computations, even when using highly optimized FHE libraries.

For an FHE accelerator to be broadly useful, it should be programmable, i.e., capable of executing arbitrary FHE computations.
%and flexible, i.e., capable of achieving high performance over a wide range of parameters and, ideally, be able to accelerate multiple FHE schemes (such as BGV for integer operations and CKKS for fixed-point and approximate computations).
% TODO(group): FHE schemes were not defined, and caused general confusion. Try to avoid introducing them till later. Focus on programmability, and make it clear what a program is (examples).
While prior work has proposed several FHE accelerators, they do not meet this goal.
Prior FHE accelerators~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,riazi:asplos20:heax,turan:tc20:heaws} target individual FHE operations,
and miss important ones that they leave to software.
These designs are FPGA-based, so they are small and 
miss the data movement issues facing an FHE ASIC accelerator.
These designs also overspecialize their functional units to specific parameters,
and cannot efficiently handle the range of parameters needed within a program or across programs.
% dsm: Note that HEAX etc don't vary Q, right? It's not even that they don't allow for variable N.
%
%Specifically, prior FHE accelerators~\cite{roy:hpca19:fpga-he, riazi:asplos20:heax} are small and target FPGAs, 
%missing the opportunities and challenges of an ASIC design.
%First, existing accelerators target individual FHE operations, and miss important operations that they delegate to software.
%Second, FPGA designs miss the large data movement issues that an FHE ASIC accelerator incurs,
%which requires new techniques to decouple and minimize data movement.
%Third, because they are FPGA-based, existing designs are tailored to specific parameters,
%whereas an FHE ASIC must balance specialization and flexibility \nikola{this seems like a disadvantage for ASICs?}.
% dsm: Now discussed only in related work, since it's a bit of a detour here
%Recent work has also proposed ASIC accelerators for some homomorphic encryption primitives
%in the context of oblivious neural networks~\cite{juvekar:usenixsecurity18:gazelle,reagen:hpca21:cheetah}, but these approaches target a fixed algorithm
%and combine homomorphic encryption with multi-party computation,
%so their techniques are quite different from FHE.
% TODO(group): Trim a bit; align with later description of challenges.

In this paper we present F1, the first programmable FHE accelerator.
F1 builds on an in-depth architectural analysis of the characteristics of FHE computations, which exposes the main challenges and reveals the design principles a programmable FHE architecture should exploit.

\paragraph{Harnessing opportunities and challenges in FHE:}
F1 is tailored to the three defining characteristics of FHE:

\noindent \textbf{\emph{(1) Complex operations on long vectors:}}
FHE encodes information using very large vectors, several thousand elements long,
and processes them using modular arithmetic.
%
F1 employs \emph{vector processing} with \emph{wide functional units} tailored to FHE operations
to achieve large speedups.
The challenge is that two key operations on these vectors, the Number-Theoretic Transform (NTT) and automorphisms, are not element-wise and require complex dataflows that are hard to implement as vector operations. 
To tackle these challenges, F1 features specialized NTT units and the first vector implementation of an automorphism functional unit.

\noindent \textbf{\emph{(2) Regular computation:}}
FHE programs are dataflow graphs of arithmetic operations on vectors.
All operations and their dependences are known ahead of time (since data is encrypted, branches
or dependences determined by runtime values are impossible).
%
F1 exploits this by adopting \emph{static scheduling}:
in the style of Very Long Instruction Word (VLIW) processors,
all components have fixed latencies and the compiler is in charge of scheduling
operations and data movement across components, with no hardware mechanisms to handle hazards (i.e., no stall logic).
%A scheduler exploits knowledge of the computation to be performed  to orchestrate operations across several clusters of functional units.
Thanks to this design, F1 can issue many operations per cycle with minimal control overheads;
%out the complexities of superscalar processors;
combined with vector processing, F1 can issue tens of thousands of scalar operations per cycle. %in a programmable way
%with minimal control overheads.

\noindent \textbf{\emph{(3) Challenging data movement:}}
In FHE, encrypting data increases its size (typically by at least 50$\times$);
data is grouped in long vectors; and some operations require large amounts (tens of MBs) of auxiliary data.
Thus, we find that data movement is \emph{the key challenge} for FHE acceleration:
despite requiring complex functional units, in current technology, limited on-chip storage and memory bandwidth are the bottleneck for most FHE programs.
%
Therefore, F1 is primarily designed to minimize data movement.
First, F1 features an explicitly managed on-chip memory hierarchy,
with a heavily banked scratchpad and distributed register files.
Second, F1 uses mechanisms to decouple data movement and hide access latencies by loading data far ahead of its use.
Third, F1 uses new, FHE-tailored scheduling algorithms that maximize reuse and make the best out of limited memory bandwidth.
Fourth, F1 uses relatively \emph{few functional units with extremely high throughput}, rather than lower-throughput functional units as in prior work.
This \emph{reduces the amount of data that must reside on-chip simultaneously}, allowing higher reuse.

In summary, F1 brings decades of research in architecture to bear, including vector processing and static scheduling, and combines them with new specialized functional units (\autoref{sec:FUs}) and scheduling algorithms (\autoref{sec:scheduler}) to design a programmable FHE accelerator.
We implement the main components of F1 in RTL and synthesize them in a commercial 14nm/12nm process.
% dsm: Comment for revision 
With a modest area budget of 151\,mm$^2$, our F1 implementation 
% dsm: 128*14+128*4 = 2304 GOps/cluster, *16 -> 36.864 TOps
provides 36 tera-ops/second of 32-bit modular arithmetic, 64\,MB of on-chip storage, and a 1\,TB/s high-bandwidth memory.
% dsm: Not very relevant, and definitely less relevant than making the point on apps.
%This system is the first to accelerate all FHE operations, and supports multiple FHE schemes (BGV, CKKS, and GSW).
We evaluate F1 using cycle-accurate simulation running complete FHE applications,
and demonstrate speedups of 1,200$\times$--17,000$\times$ over state-of-the-art software implementations.
These dramatic speedups counter most of FHE's overheads and enable new applications.
For example, F1 executes a deep learning inference that used to take 20 minutes in 240 milliseconds,
enabling secure real-time deep learning in the cloud.

\subsection{F1 Drawbacks}

Unfortunately, F1 is efficient only on a limited subset of simple FHE
computations---those of \emph{shallow multiplicative depth}.
For example, F1 can run neural network inference efficiently only for networks with few layers (3-6),
but they cannot accelerate state-of-the-art deep neural networks (DNNs) with tens to hundreds of layers.

This limitation stems from the characteristics of FHE schemes:
each ciphertext has some associated noise, which grows with each homomorphic operation, and especially with multiplications.
If noise becomes too large, it garbles the message, making decryption impossible. 
%To support computations with higher multiplicative depth, the size of ciphertexts must increase to counteract the additional noise growth, resulting in each FHE operation being more computationally expensive.
Larger ciphertexts tolerate more noise before becoming undecryptable. 
However, operations on larger ciphertexts are also more expensive.
To enable computations of unbounded depth, ciphertexts can be ``refreshed'' using a procedure called \emph{bootstrapping}
that reduces noise. But bootstrapping is expensive, 
so ciphertexts must be very large (10s of MBs) for bootstrapping to be
efficient.

Prior FHE accelerators do not efficiently handle unbounded-depth computations because
they natively support vectors of a limited size and they use algorithms that scale poorly
to the large ciphertexts in high-depth programs.
As a result, they can only run small FHE computations, and they do not support sufficient depth
to run the full bootstrapping procedure.

\figWorkflow

In this paper we tackle this challenge through \name, the first FHE accelerator
to support \emph{FHE computations of unbounded depth}.
To achieve this, we contribute new algorithms, specialized functional units,
hardware architecture, and compiler techniques that overcome the key challenge
of deep FHE computations---its extreme data movement demands.

\paragraph{Deep FHE is limited by data movement:}
FHE schemes encode information over very long vectors of wide elements.
Concretely, supporting unbounded-depth computations requires vectors of
64K elements with 1,600 bits per element.
This takes 25\,MB per ciphertext, 12$\times$ larger than what prior FHE accelerators target.
% nikola; 25MB = 2 * 1600 bits * 64*1024 elements * (1/8) bytes / bit * 1/2^20 bytes / MB

Moreover, prior work has employed FHE algorithms that require even larger amounts of auxiliary data.
For example, multiplying 2\,MB ciphertexts in F1~\cite{feldmann:micro21:f1} requires 32\,MB of auxiliary data,
and scaling their algorithm to 26\,MB ciphertexts would require over
1.3\,GB 
of auxiliary data---far too large to fit on-chip.
To tackle this challenge, our \emph{key insight} is to adopt an FHE
algorithm called \emph{boosted keyswitching} (\autoref{sec:keyswitching}) that eliminates most of the
auxiliary data,
% (alex): KSH it's 2 ciphertexts without PRG
reducing this overhead from 1.3\,GB to 52.5\,MB.
%We further reduce this overhead to 23\,MB by introducing a new functional unit that
%generates half of this auxiliary data on the fly, rather than fetching it from memory.
Boosted keyswitching also reduces computation costs.
However, this new algorithm is a poor match for prior accelerators:
it is dominated by simple operations where these designs have limited efficiency,
and makes poor use of the specialized functional units that prior designs leverage.

Beyond being inefficient, prior accelerators suffer from a hard-to-scale vector multicore architecture:
to support the needed non-SIMD operations with reasonable cost,
they implement multiple independent cores with narrower vector datapaths~\cite{feldmann:micro21:f1}.
However, this causes excessive inter-core communication,
and the high-bandwidth interconnect
needed grows superlinearly with the number of cores.


\paragraph{Deep FHE demands new hardware techniques:}
To tackle these challenges, we introduce the \emph{\name} architecture (\autoref{sec:overview}, \autoref{sec:architecture}),
the first FHE accelerator that achieves high performance on unbounded FHE programs.
\name is a wide-vector uniprocessor with specialized functional units.
The design is statically scheduled to leverage the regularity of FHE computations.
We contribute several new techniques that make this possible, including:
\begin{compactitem}
\item A new extremely wide (2,048 lanes) vector uniprocessor architecture that
    spreads each vector operation across the chip, departing from prior vector multicore architectures.
The uniprocessor approach reduces the number of concurrent operations, which minimizes footprint,
  reducing off-chip traffic, and simplifies the compiler.
\item An efficient implementation of the above architecture, which is challenging for non-SIMD FHE operations, NTTs and automorphisms,
  by decomposing these operations in a novel way that allows the use of a \emph{fixed transpose network} among physically distributed groups of lanes.
  This reduces on-chip data movement and interconnect cost over prior approaches.
\item A new functional unit that encapsulates the bulk of operations in boosted keyswitching, improving efficiency and enabling high utilization across ciphertexts of all sizes.
\item A new functional unit that generates half of the required auxiliary data on the fly (reducing overheads from 52\,MB to 26\,MB), saving on-chip storage and memory bandwidth.
\item A vector chaining technique that builds long FU pipelines to enable many concurrent operations with few register ports.
\end{compactitem}
To program \name, we develop a novel compiler (\autoref{sec:compiler}) that produces efficient code from high-level FHE programs.
The compiler schedules operations to maximize reuse, decouples data movement from computation,
and adapts the state-of-the-art bootstrapping algorithm to achieve high
utilization~\cite{bossuat:crypto21:efficient}.


We evaluate \name through a combination of simulation and RTL synthesis (to find its area and power).
We use a broad range of FHE benchmarks, including programs with high multiplicative depth
that require bootstrapping.
\name outperforms a scaled-up and improved version of the state-of-the-art FHE accelerator, F1, by gmean 
11.2$\times$ on these deep computations,
and is 4,600$\times$ faster than a 32-core CPU.
These speedups enable new use cases for FHE.
For example, deep neural networks like ResNet
% which takes tens of \emph{minutes} per inference on CPUs,
% nikola: again, it really takes tens of minutes, but that's cuz of hours algo improvements
% dsm: Let's not confuse the reader, we can mention that this used to be hours in the eval but saying hours to ms is comparing apples to oranges
take 23 minutes per inference on the CPU,
whereas \name achieves 250 \emph{milliseconds} per inference,
enabling real-time private deep learning.
% 23 minutes = 1.37694e+12 ns / (10^9*60) = 22.9 min
% 265 ms = 2.64245e+08 / (10^6) = 264 ms

