\chapter{Introduction}\label{sec:intro}

A large and increasing fraction of the world's compute runs on the cloud, which
is vulnerable to data breaches. Conventional techniques to mitigate attacks
offer limited security, as cloud servers must decrypt data in order to process
it.

\figWorkflow

\emph{Fully homomorphic encryption (FHE)} is a special type of encryption
scheme that enables \emph{computing on encrypted data directly}, without
decrypting it. FHE allows a client to offload a computation to an untrusted
server \emph{without} revealing any data (\autoref{fig:workflow}). This enables
the client to harness the compute power of the cloud while maintaining
cryptographic privacy. Though FHE has some limitations (e.g., data-dependent
branching is not possible), it is general enough to support many compelling use
cases, such as privacy-preserving machine learning, secure genome analysis,
private set intersection, private information retrieval, and many
more~\cite{kim2020semi,gilad:icml16:cryptonets,han:aaai19:logistic,han:iacr18:efficient,juvekar2018gazelle,DBLP:conf/ccs/ChenLR17,DBLP:conf/tcc/GentryH19}.
%\nnote{add citations and more examples}

% axelf: purging unfortunately/fortunately from the whole paper
Despite its ideal privacy, FHE is rarely used today because it incurs
prohibitive overheads: in CPUs, FHE computations are 10,000$\times$ to
100,000$\times$ slower than equivalent unencrypted computations, even when
using highly optimized FHE libraries.

For an FHE accelerator to be broadly useful, it should be programmable, i.e.,
capable of executing arbitrary FHE computations. While prior work has proposed
several FHE accelerators, they do not meet this goal. Prior FHE
accelerators~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,riazi:asplos20:heax,turan:tc20:heaws}
target individual FHE operations, and miss important ones that they leave to
software. These designs are FPGA-based, so they are small and miss the data
movement issues facing an FHE ASIC accelerator. These designs also
overspecialize their functional units to specific parameters, and cannot
efficiently handle the range of parameters needed within a program or across
programs.

In this paper we present F1, the first programmable FHE accelerator. F1 builds
on an in-depth architectural analysis of the characteristics of FHE
computations, which exposes the main challenges and reveals the design
principles a programmable FHE architecture should exploit.

\paragraph{Harnessing opportunities and challenges in FHE:}
F1 is tailored to the three defining characteristics of FHE:

\noindent \textbf{\emph{(1) Complex operations on long vectors:}}
FHE encodes information using very large vectors, several thousand elements
long, and processes them using modular arithmetic. F1 employs \emph{vector
processing} with \emph{wide functional units} tailored to FHE operations to
achieve large speedups. The challenge is that two key operations on these
vectors, the Number-Theoretic Transform (NTT) and automorphisms, are not
element-wise and require complex dataflows that are hard to implement as vector
operations. To tackle these challenges, F1 features specialized NTT units and
the first vector implementation of an automorphism functional unit.

\noindent \textbf{\emph{(2) Regular computation:}}
FHE programs are dataflow graphs of arithmetic operations on vectors. All
operations and their dependences are known ahead of time (since data is
encrypted, branches or dependences determined by runtime values are
impossible). F1 exploits this by adopting \emph{static scheduling}: in the
style of Very Long Instruction Word (VLIW) processors, all components have
fixed latencies and the compiler is in charge of scheduling operations and data
movement across components, with no hardware mechanisms to handle hazards
(i.e., no stall logic). Thanks to this design, F1 can issue many operations per
cycle with minimal control overheads; combined with vector processing, F1 can
issue tens of thousands of scalar operations per cycle.

\noindent \textbf{\emph{(3) Challenging data movement:}}
In FHE, encrypting data increases its size (typically by at least 50$\times$);
data is grouped in long vectors; and some operations require large amounts
(tens of MBs) of auxiliary data. Thus, we find that data movement is \emph{the
key challenge} for FHE acceleration: despite requiring complex functional
units, in current technology, limited on-chip storage and memory bandwidth are
the bottleneck for most FHE programs. Therefore, F1 is primarily designed to
minimize data movement. First, F1 features an explicitly managed on-chip memory
hierarchy, with a heavily banked scratchpad and distributed register files.
Second, F1 uses mechanisms to decouple data movement and hide access latencies
by loading data far ahead of its use. Third, F1 uses new, FHE-tailored
scheduling algorithms that maximize reuse and make the best out of limited
memory bandwidth. Fourth, F1 uses relatively \emph{few functional units with
extremely high throughput}, rather than lower-throughput functional units as in
prior work. This \emph{reduces the amount of data that must reside on-chip
simultaneously}, allowing higher reuse.

In summary, F1 brings decades of research in architecture to bear, including
vector processing and static scheduling, and combines them with new specialized
functional units (\autoref{sec:FUs}) and scheduling algorithms
(\autoref{sec:scheduler}) to design a programmable FHE accelerator. We
implement the main components of F1 in RTL and synthesize them in a commercial
14nm/12nm process. With a modest area budget of 151\,mm$^2$, our F1
implementation provides 36 tera-ops/second of 32-bit modular arithmetic, 64\,MB
of on-chip storage, and a 1\,TB/s high-bandwidth memory. We evaluate F1 using
cycle-accurate simulation running complete FHE applications, and demonstrate
speedups of 1,200$\times$--17,000$\times$ over state-of-the-art software
implementations. These dramatic speedups counter most of FHE's overheads and
enable new applications. For example, F1 executes a deep learning inference
that used to take 20 minutes in 240 milliseconds, enabling secure real-time
deep learning in the cloud.

\section{Motivating CraterLake}

Unfortunately, F1 is efficient only on a limited subset of simple FHE
computations---those of \emph{shallow multiplicative depth}. For example, F1
can run neural network inference efficiently only for networks with few layers
(3-6), but they cannot accelerate state-of-the-art deep neural networks (DNNs)
with tens to hundreds of layers.

This limitation stems from the characteristics of FHE schemes: each ciphertext
has some associated noise, which grows with each homomorphic operation, and
especially with multiplications. If noise becomes too large, it garbles the
message, making decryption impossible. Larger ciphertexts tolerate more noise
before becoming undecryptable. However, operations on larger ciphertexts are
also more expensive. To enable computations of unbounded depth, ciphertexts can
be ``refreshed'' using a procedure called \emph{bootstrapping} that reduces
noise. But bootstrapping is expensive, so ciphertexts must be very large (10s
of MBs) for bootstrapping to be efficient.

Prior FHE accelerators do not efficiently handle unbounded-depth computations
because they natively support vectors of a limited size and they use algorithms
that scale poorly to the large ciphertexts in high-depth programs. As a result,
they can only run small FHE computations, and they do not support sufficient
depth to run the full bootstrapping procedure.


In this paper we tackle this challenge through \name, the first FHE accelerator
to support \emph{FHE computations of unbounded depth}. To achieve this, we
contribute new algorithms, specialized functional units, hardware architecture,
and compiler techniques that overcome the key challenge of deep FHE
computations---its extreme data movement demands.

\paragraph{Deep FHE is limited by data movement:}
FHE schemes encode information over very long vectors of wide elements.
Concretely, supporting unbounded-depth computations requires vectors of 64K
elements with 1,600 bits per element. This takes 25\,MB per ciphertext,
12$\times$ larger than what prior FHE accelerators target.

Moreover, prior work has employed FHE algorithms that require even larger
amounts of auxiliary data. For example, multiplying 2\,MB ciphertexts in
F1~\cite{feldmann:micro21:f1} requires 32\,MB of auxiliary data, and scaling
their algorithm to 26\,MB ciphertexts would require over 1.3\,GB of auxiliary
data---far too large to fit on-chip. To tackle this challenge, our \emph{key
insight} is to adopt an FHE algorithm called \emph{boosted keyswitching}
(\autoref{sec:keyswitching}) that eliminates most of the auxiliary data,
reducing this overhead from 1.3\,GB to 52.5\,MB. Boosted keyswitching also
reduces computation costs. However, this new algorithm is a poor match for
prior accelerators: it is dominated by simple operations where these designs
have limited efficiency, and makes poor use of the specialized functional units
that prior designs leverage.

Beyond being inefficient, prior accelerators suffer from a hard-to-scale vector
multicore architecture: to support the needed non-SIMD operations with
reasonable cost, they implement multiple independent cores with narrower vector
datapaths~\cite{feldmann:micro21:f1}. However, this causes excessive inter-core
communication, and the high-bandwidth interconnect needed grows superlinearly
with the number of cores.


\paragraph{Deep FHE demands new hardware techniques:}
To tackle these challenges, we introduce the \emph{\name} architecture
(\autoref{sec:overview}, \autoref{sec:architecture}), the first FHE accelerator
that achieves high performance on unbounded FHE programs. \name is a
wide-vector uniprocessor with specialized functional units. The design is
statically scheduled to leverage the regularity of FHE computations. We
contribute several new techniques that make this possible, including:
\begin{compactitem}
\item A new extremely wide (2,048 lanes) vector uniprocessor architecture that
    spreads each vector operation across the chip, departing from prior vector
    multicore architectures. The uniprocessor approach reduces the number of
    concurrent operations, which minimizes footprint, reducing off-chip
    traffic, and simplifies the compiler.
\item An efficient implementation of the above architecture, which is
    challenging for non-SIMD FHE operations, NTTs and automorphisms, by
    decomposing these operations in a novel way that allows the use of a
    \emph{fixed transpose network} among physically distributed groups of
    lanes. This reduces on-chip data movement and interconnect cost over prior
    approaches.
\item A new functional unit that encapsulates the bulk of operations in boosted
    keyswitching, improving efficiency and enabling high utilization across
    ciphertexts of all sizes.
\item A new functional unit that generates half of the required auxiliary data
    on the fly (reducing overheads from 52\,MB to 26\,MB), saving on-chip
    storage and memory bandwidth.
\item A vector chaining technique that builds long FU pipelines to enable many
    concurrent operations with few register ports.
\end{compactitem}
To program \name, we develop a novel compiler (\autoref{sec:compiler}) that
produces efficient code from high-level FHE programs. The compiler schedules
operations to maximize reuse, decouples data movement from computation, and
adapts the state-of-the-art bootstrapping algorithm to achieve high
utilization~\cite{bossuat:crypto21:efficient}.


We evaluate \name through a combination of simulation and RTL synthesis (to
find its area and power). We use a broad range of FHE benchmarks, including
programs with high multiplicative depth that require bootstrapping. \name
outperforms a scaled-up and improved version of the state-of-the-art FHE
accelerator, F1, by gmean 11.2$\times$ on these deep computations, and is
4,600$\times$ faster than a 32-core CPU. These speedups enable new use cases
for FHE. For example, deep neural networks like ResNet take 23 minutes per
inference on the CPU, whereas \name achieves 250 \emph{milliseconds} per
inference, enabling real-time private deep learning.
