\chapter{Introduction}\label{sec:intro}

Despite massive efforts to improve the security of computer systems, security
breaches are only becoming more frequent and damaging, as more sensitive data
is processed in the
cloud~\cite{malekos-smith:csis20:hidden-costs-cybercrime,ibm20:breach-cost-report}.
Current encryption technology is of limited help, because servers must decrypt
data before processing it. Once data is decrypted, it is vulnerable to
breaches.

\figWorkflow

Fully Homomorphic Encryption (FHE) is a class of encryption schemes that
address this problem by enabling \emph{generic computation on encrypted data}.
\autoref{fig:workflow} shows how FHE enables secure offloading of computation.
The client wants to compute an expensive function $f$ (e.g., a deep learning
inference) on some private data $x$. To do this, the client encrypts $x$ and
sends it to an untrusted server, which computes $f$ on this encrypted data
\emph{directly} using FHE, and returns the encrypted result to the client. FHE
provides ideal security properties: even if the server is compromised,
attackers cannot learn anything about the data, as it remains encrypted
throughout.


FHE is a young but quickly developing technology. First realized in
2009~\cite{gentry09}, early FHE schemes were about 10$^9$ times slower than
performing computations on unencrypted data. Since then, improved FHE schemes
have greatly reduced these overheads and broadened its
applicability~\cite{albrecht:hesg18:standard,peikert2016decade}. FHE has
inherent limitations---for example, data-dependent branching is impossible,
since data is encrypted---so it won't subsume all computations. Nonetheless,
important classes of computations, like deep learning
inference~\cite{cheon:ictaci17:homomorphic,dathathri:pldi19:chet,dathathri:pldi20:eva},
linear algebra, secure genome analysis, private set intersection, information
retrieval, and other inference and learning
tasks~\cite{kim2020semi,gilad:icml16:cryptonets,han:aaai19:logistic,han:iacr18:efficient,juvekar2018gazelle,DBLP:conf/ccs/ChenLR17,DBLP:conf/tcc/GentryH19}
are a good fit for FHE. This has sparked significant industry and government
investments~\cite{ibm,intel,dprive} to widely deploy FHE.

Unfortunately, FHE still carries substantial performance overheads: despite
recent advances~\cite{dathathri:pldi19:chet, dathathri:pldi20:eva,
roy:hpca19:fpga-he, brutzkus:icml19:low, polyakov:17:palisade}, FHE is still
10,000$\times$ to 100,000$\times$ slower than unencrypted computation when
executed in carefully optimized software. Though this slowdown is large, it can
be addressed with hardware acceleration: \emph{if a specialized FHE accelerator
provides large speedups over software execution, it can bridge most of this
performance gap and enable new use cases.}

\section{Challenges of FHE Acceleration}
\label{sec:general}

For an FHE accelerator to be broadly useful, it should be programmable, i.e.,
capable of executing arbitrary FHE computations. While prior work has proposed
several FHE accelerators, they do not meet this goal. Prior FHE
accelerators~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,riazi:asplos20:heax,turan:tc20:heaws}
target individual FHE operations, and miss important ones that they leave to
software. These designs are FPGA-based, so they are small and miss the data
movement issues facing an FHE ASIC accelerator. These designs also
overspecialize their functional units to specific parameters, and cannot
efficiently handle the range of parameters needed within a program or across
programs.

We identify five key characteristics of FHE that drive our architectures:

\noindent \textbf{\emph{(1) Complex operations on long vectors:}}
FHE encodes information using very large vectors, several thousand elements
long, and processes them using modular arithmetic. Therefore, an FHE
accelerator can employ \emph{vector processing} with \emph{wide functional
units} tailored to FHE operations to achieve large speedups. The challenge is
that two key operations on these vectors, the Number-Theoretic Transform (NTT)
and automorphisms, are not element-wise and require complex dataflows that are
hard to implement as vector operations.

\noindent \textbf{\emph{(2) Regular computation:}}
FHE programs are dataflow graphs of arithmetic operations on vectors. All
operations and their dependences are known ahead of time (since data is
encrypted, branches or dependences determined by runtime values are
impossible).

\noindent \textbf{\emph{(3) Challenging data movement:}}
In FHE, encrypting data increases its size (typically by at least 50$\times$);
data is grouped in long vectors; and some operations require large amounts
(tens of MBs) of auxiliary data. Thus, we find that data movement is \emph{the
key challenge} for FHE acceleration: despite requiring complex functional
units, in current technology, limited on-chip storage and memory bandwidth are
the bottleneck for most FHE programs. Therefore, the primary goal of an FHE
accelerator should be to minimize data movement.

\noindent \textbf{\emph{(4) Algorithmic diversity:}}
FHE is a young and quickly-developing field. As a result, there is abundant
diversity in proposed schemes and algorithms. For example, keyswitching, a key
FHE kernel, has many variants with diverse performance-security tradeoffs. An
effective FHE accelerator needs to support all these algorithms well, and be
flexible enough to adapt to future advances in the field.

\noindent \textbf{\emph{(5) Deep FHE computation:}}
Challenges that come with deep FHE programs (i.e., those of high multiplicative
depth) are inherent to state-of-the-art FHE schemes. Namely, each FHE
ciphertext has some associated noise, which grows with each homomorphic
operation, and especially with multiplications. If noise becomes too large, it
garbles the message, making decryption impossible. Larger ciphertexts tolerate
more noise before becoming undecryptable. However, operations on larger
ciphertexts are also more expensive. To enable computations of unbounded depth,
ciphertexts can be ``refreshed'' using a procedure called \emph{bootstrapping}
that reduces noise. But bootstrapping is expensive, so ciphertexts must be very
large (10s of MBs) for bootstrapping to be efficient. Concretely, supporting
unbounded-depth computations requires vectors of 64K elements with 1,600 bits
per element. This takes 25\,MB per ciphertext. Thus, data movement issues are
even more pronounced with deep FHE computation.

\section{Contributions}

The first four of the above challenges have driven F1's architecture:
\begin{compactitem}
\item It features specialized NTT units and a first vector implementation of an
    automorphism functional unit.
\item F1 adopts \emph{static scheduling}: in the style of Very Long Instruction
    Word (VLIW) processors, all components have fixed latencies and the
    compiler is in charge of scheduling operations and data movement across
    components, with no hardware mechanism to handle hazards (i.e., no stall
    logic). With this approach, F1 can issue many operations per
    cycle with minimal control overheads; combined with vector processing,
    F1 can issue tens of thousands of scalar operations
    per cycle.
\item To minimize data movement, F1 is designed to: \emph{(1)} Use a large and
    explicitly managed on-chip scratchpad; \emph{(2)} Focus on maximizing operand
    reuse when scheduling operations. This reduces memory bandwidth pressure;
    \emph{(3)} Decouple data movement. This allows us to hide access latencies
    by loading data far ahead of its use; \emph{(4)} Build relatively \emph{few
    functional units with extremely high throughput}, rather than
    lower-throughput functional units as in prior work. This \emph{reduces the
        amount of data that must reside on-chip simultaneously}, allowing
        higher reuse.
\item F1 allows for plentiful algorithmic diversity because its functional
    units support low-level operations on large integer polynomials: primitives
    that all FHE schemes and algorithms depend on.
\end{compactitem}

In summary, F1 brings decades of research in architecture to bear, including
vector processing and static scheduling, and combines them with new specialized
functional units (\autoref{sec:FUs}) and scheduling algorithms
(\autoref{sec:scheduler}) to design a programmable FHE accelerator. We
implement the main components of F1 in RTL and synthesize them in a commercial
14nm/12nm process. With a modest area budget of 151\,mm$^2$, our F1
implementation provides 36 tera-ops/second of 32-bit modular arithmetic, 64\,MB
of on-chip storage, and a 1\,TB/s high-bandwidth memory. We evaluate F1 using
cycle-accurate simulation running complete FHE applications, and demonstrate
speedups of 1,200$\times$--17,000$\times$ on shallow benchmarks over
state-of-the-art software implementations.

\paragraph{Deep FHE demands new hardware techniques:}
Unfortunately, F1 is efficient only on a limited subset of simple FHE
computations---those of \emph{shallow multiplicative depth}. For example, F1
can run neural network inference efficiently only for networks with few layers
(3-6), but it cannot accelerate state-of-the-art deep neural networks (DNNs)
with tens to hundreds of layers. That is, F1 does not appropriatelly address
our fifth challenge: deep FHE computation.

F1 and prior accelerators do not efficiently handle unbounded-depth
computations because they natively support vectors of a limited size and they
use algorithms that scale poorly to the large ciphertexts in high-depth
programs. As a result, they can only run small FHE computations, and they do
not support sufficient depth to run the full bootstrapping procedure.

Specifically, F1 and prior work focus on FHE algorithms that do not scale well
to ciphertext sizes necessary for unbounded-depth FHE. For example, multiplying
2\,MB ciphertexts in F1 requires 32\,MB of auxiliary data, and scaling F1's
algorithm to 26\,MB ciphertexts would require over 1.3\,GB of auxiliary
data---far too large to fit on-chip. To tackle this challenge, CraterLake's
\emph{key insight} is to adopt an FHE algorithm called \emph{boosted
keyswitching} (\autoref{sec:boostedKeyswitching}) that eliminates most of the
auxiliary data, reducing this overhead from 1.3\,GB to 52.5\,MB. Boosted
keyswitching also reduces computation costs. However, this new algorithm is a
poor match for F1 and prior accelerators: it is dominated by simple operations
where these designs have limited efficiency, and makes poor use of the
specialized functional units that prior designs leverage.

Beyond being inefficient, F1 suffers from a hard-to-scale vector multicore
architecture: to support the needed non-SIMD operations with reasonable cost,
it implements multiple independent cores with narrower vector datapaths.
However, this causes excessive inter-core communication, and the high-bandwidth
interconnect needed grows superlinearly with the number of cores.

To tackle these challenges, we introduce the \emph{CraterLake} architecture
(\autoref{ch:craterlake}), the first FHE accelerator
that achieves high performance on unbounded FHE programs. CraterLake builds on
many of F1's key insights: specilized vector functional units, static
scheduling, and a focus on data movement. However, CraterLake succesfully
addresses F1's shortcomings by contributing:
\begin{compactitem}
\item A new extremely wide (2,048 lanes) vector \emph{uniprocessor}
    architecture that spreads each vector operation across the chip, departing
    from F1's multicore architectures. The uniprocessor approach reduces the
    number of concurrent operations, which minimizes footprint, reducing
    off-chip traffic, and simplifies the compiler.
\item An efficient implementation of the above architecture, which is
    challenging for non-SIMD FHE operations, NTTs and automorphisms, by
    decomposing these operations in a novel way that allows the use of a
    \emph{fixed transpose network} among physically distributed groups of
    lanes. This reduces on-chip data movement and interconnect cost over F1's
    approach.
\item The change RNS base (CRB) unit, a new functional unit that encapsulates
    the bulk of operations in boosted keyswitching, improving efficiency and
    enabling high utilization across ciphertexts of all sizes.
\item A new functional unit that generates half of the required auxiliary data
    on the fly (reducing overheads from 52\,MB to 26\,MB), saving on-chip
    storage and memory bandwidth.
\item A vector chaining technique that builds long FU pipelines to enable many
    concurrent operations with few register ports.
\end{compactitem}

We evaluate CraterLake through a combination of simulation and RTL synthesis
(to find its area and power).
F1 area budget is modest (151\,mm$^2$) and delivers good performance, but it is
insufficient for deep FHE benchmarks. CraterLake, a 472\,mm$^2$ chip, is
sufficient to effectively accelerate unbounded depth FHE programs.

To evaluate CraterLake, we use a broad range of FHE benchmarks, including
programs with high multiplicative depth that require bootstrapping. CraterLke
outperforms a scaled-up and improved version of the state-of-the-art FHE
accelerator, F1, by gmean 11.2$\times$ on these deep computations, and is
4,600$\times$ faster than a 32-core CPU. These speedups enable new use cases
for FHE. For example, deep neural networks like ResNet take 23 minutes per
inference on the CPU, whereas CraterLake achieves 250 \emph{milliseconds} per
inference, enabling real-time private deep learning.
