\section{Implementation}\label{sec:implementation}

\tblGF

We implement CraterLake's components in RTL, and synthesized them in a
commercial 14/12nm process using state-of-the-art tools. These include a
commercial SRAM compiler that we use for register file banks.

We target a configuration with area and power budgets similar to a
modern GPU or server CPU. This design requires careful optimizations to limit
power. We target 1\,GHz frequency for most components, and pipeline them to
their energy-optimal points, using high-Vt cells and clock gating. Register
file banks run double-pumped at 2\,GHz. This enables using single-ported
SRAMs while serving up to two accesses per cycle and bank.

\autoref{tbl:GF12} shows CraterLake's area and its breakdown by component.
Overall, our CraterLake configuration requires 472\,mm$^2$. FUs occupy 51\% of
the area, with 41\% going to the register file, 6\% to the two HBM2 PHYs, and
2\% to the on-chip interconnect. As we will see in the evaluation, this design
stays within a power budget of 320\,W, in line with GPUs and server CPUs.

Finally, while these figures could be considered high, note that we are not
using a leading-edge fabrication node: based on published logic and SRAM
scaling factors~\cite{yeap:iedm19:tsmc-n5}, on current TSMC 5\,nm technology,
CraterLake would consume a very modest 157\,mm$^\textrm{2}$ area and 146\,W
peak power.

\paragraph{Host communication:}
We assume that CraterLake is implemented as an accelerator. The needed
CPU-accelerator bandwidth required to stream inputs and outputs in our
benchmarks is 50\,GB/s on average and 130\,GB/s at most, so a commodity PCIe\,5
interface suffices to achieve full throughput. CPU-accelerator latency is not
an issue, as these are bulk transfers and can be overlapped with computation.

