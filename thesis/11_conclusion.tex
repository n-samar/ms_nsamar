\chapter{Conclusions}\label{sec:conclusion}

FHE has the potential to enable computation offloading with guaranteed
security. But FHE's high computation overheads currently limit its
applicability to narrow cases (simple computations where privacy is paramount).
F1 tackles this challenge, accelerating full FHE computations by over 3-4
orders of magnitude. This enables new use cases for FHE, like secure real-time
deep learning inference.

F1 is the first FHE accelerator that is programmable, i.e., capable of
executing full FHE programs. In contrast to prior accelerators, which build
fixed pipelines tailored to specific FHE schemes and parameters, F1 introduces
a more effective design approach: it accelerates the \emph{primitive}
computations shared by higher-level operations using novel high\hyp{}throughput
functional units, and hardware and compiler are co-designed to minimize data
movement, the key bottleneck. This flexibility makes F1 broadly useful: the
same hardware can accelerate all operations within a program, arbitrary FHE
programs, and even multiple FHE schemes. In short, our key contribution is to
show that, for FHE, we can achieve ASIC-level performance without sacrificing
programmability.

While F1 shows programmable FHE is possible, accelerators must efficiently
support deep computations for FHE to be widely adopted. \name is the first
accelerator to achieve this. By adopting state-of-the-art algorithms and using
them to design \name, we target a new regime of FHE not explored by prior
approaches. Through new architectural and compiler techniques, \name addresses
the overheads of deep computations and provides order-of-magnitude speedups
over prior accelerators (even when that prior work is scaled up and allowed
several idealizations). As a result, \name enables new applications for FHE,
such as real-time inference using deep neural networks like ResNet or LSTMs.

