\chapter{Background}\label{sec:background}

State-of-the-art FHE schemes implement operations on~\emph{encrypted vectors}.
The FHE ciphertexts in these schemes support several \emph{homomorphic
operations}: element-wise addition, element-wise multiplication, and cyclic
rotations of vector elements. Each homomorphic operation produces a ciphertext
that, when decrypted, produces the same result as if the operation had been
performed on the unencrypted inputs.

Importantly, homomorphic operations have a different implementation from their
unencrypted counterparts---for example, a homomorphic multiplication is not
implemented using element-wise multiplication of the input ciphertexts, but a
more complex sequence of operations. Therefore, it is useful to differentiate
between FHE's \emph{interface}, i.e., its supported plaintext datatypes and
operations, and its \emph{implementation}, i.e., the structure of ciphertexts
and the implementation of homomorphic operations.

There are several FHE schemes, which mainly differ in their plaintext datatypes
and the operations they support. For example,
BGV~\cite{brakerski:toct14:leveled} encodes vectors of integers modulo a
constant, whereas CKKS~\cite{cheon:ictaci17:homomorphic} encodes vectors of
fixed-point numbers. Despite the differences between these schemes, % all
state-of-the-art FHE schemes the commonalities in their underlying
implementation~\cite{lyubashevsky:tact10:ideal} make it possible for the same
hardware to accelerate many schemes efficiently---\name supports CKKS, BGV, and
GSW~\cite{gentry:crypto13:homomorphic}. For concreteness, the rest of this
section will focus only on CKKS as it is the scheme best suited for machine
learning tasks and has been the focus of recent work in FHE
algorithms~\cite{han:iacr18:efficient,lee:2021:privacy,gilad:icml16:cryptonets,podschwadt:2020:classification,dathathri:pldi19:chet,dathathri:pldi20:eva}.

\section{FHE Interface}
\label{sec:fhe_mapping}

FHE schemes implement operations on vectors of values. In CKKS, each vector
element is a \emph{fixed-point} complex number with a configurable number of
bits. (Programs that do not use complex arithmetic can zero out the imaginary
part.)

Since values are encrypted, FHE does not permit data-dependent branching or
indirection. Thus, all operations and dependencies are known ahead of time, and
FHE programs can be represented using \emph{static dataflow graphs}.

Homomorphic operations in CKKS include element-wise addition, element-wise
multiplication, and cyclic rotations. While these operations are
\emph{approximate} in CKKS, inducing a small and controllable amount of error,
this error can be made arbitrarily small at the cost of reduced performance.
This error is acceptable in practice as machine learning applications are
insensitive to it.

FHE exposes a vector programming model with a restricted set of operations; in
particular, FHE does not provide access to individual vector elements. This
makes it challenging to implement some operations that are trivial in
plaintext: For example, implementing a convolutional layer of a neural network
requires the careful replication of filter weights. The lack of non-linear
functions introduces other difficulties. For example, the ReLU activation
function must be approximated using a high-degree
polynomial~\cite{lee:2021:precise}. As a result, faithfully replicating deep
neural networks in FHE, as done by a recent ResNet
implementation~\cite{lee:2021:privacy}, comes at a high compute cost. However,
recent work has proposed neural network structures that are optimized for FHE
and achieve higher performance while maintaining similar
accuracy~\cite{brutzkus:icml19:low}. We evaluate \name on both styles of neural
networks.

Finally, not all data needs to be encrypted: additions and multiplications are
much cheaper in FHE if one of the operands is unencrypted. This allows
algorithms to trade privacy for performance. For example, running a neural
network using unencrypted weights is faster; it still ensures the privacy of
inputs and results, but does not protect weights~\cite{brutzkus:icml19:low}.

\section{FHE Implementation}
\label{sec:fhe_operation}

We now describe how CKKS represents and operates on encrypted data (i.e.,
ciphertexts); other schemes (e.g., BGV) have a similar structure.

\paragraph{Encryption:}
A ciphertext holds an encrypted vector of plaintext values. To create a
ciphertext, the vector of plaintext values is first encoded, or \emph{packed},
in a polynomial; this polynomial is then \emph{encrypted}. CKKS packs a
plaintext vector of $ n = N/2$ complex fixed-point numbers into a
degree-$(N-1)$ polynomial:
\begin{equation*}
    (c_0, c_1, ..., c_n) \xmapsto{pack} \mathfrak{m} = k_0 + k_1x + ... + k_{N-1}x^{N-1} %\in R_q
\end{equation*}
$\mathfrak{m}$ is then encrypted into a ciphertext. Each ciphertext consists of
$\mathfrak{ct}_0, \mathfrak{ct}_1$---two \emph{ciphertext polynomials} with
coefficients modulo a \emph{ciphertext modulus} $Q$. Specifically, we encrypt
$\mathfrak{m}$ under a \emph{secret key} $\mathfrak{s}$ by sampling a uniformly
random $\mathfrak{a}$ and a small \emph{error} $\mathfrak{e}$ ($\mathfrak{s}$,
$\mathfrak{a}$, and $\mathfrak{e}$ are also polynomials):
\begin{equation*}
    \mathfrak{m} \xmapsto{encrypt} \mathfrak{ct} = (\mathfrak{ct}_0, \mathfrak{ct}_1) = (\mathfrak{a}, \mathfrak{a}\cdot\mathfrak{s}+\mathfrak{e}+\mathfrak{m})
\end{equation*}
The above process produces a \emph{fully-packed} ciphertext, i.e., one that
encodes as many plaintext values as possible. It is possible (though almost
always less efficient) to pack~fewer~values, producing a partially packed or
unpacked (one-element) ciphertext.

\paragraph{Homomorphic operations} are implemented through several
modular-arithmetic operations on ciphertext polynomials, i.e., vectors of
coefficients. Specifically:
\begin{compactitem}
\item \emph{Homomorphic addition} of two ciphertexts simply requires modular
    addition of their ciphertext polynomials: $\mathfrak{ct}_{\textrm{add}} =
    \mathfrak{a} + \mathfrak{b} = (\mathfrak{a}_0+\mathfrak{b}_0,
    \mathfrak{a}_1+\mathfrak{b}_1)$.
\item \emph{Homomorphic multiplication} is implemented using polynomial
    multiplications and additions; multiplying two polynomials requires
    convolving their coefficients.
\item \emph{Homomorphic rotation} rotates the vector encrypted~in~a~ciphertext.
    Implementing it requires performing an \emph{automorphism} on the
    ciphertext polynomials, a structured permutation where, for automorphism
    $k$, each input index $i$ is mapped to output index $ik \bmod N$. There are
    $N$ possible automorphisms; each automorphism induces a simpler, cyclic
    rotation in the plaintext.
\end{compactitem}

On top of this, homomorphic multiplications and rotations also require a
procedure called \emph{keyswitching}, which is needed so that the final
ciphertext stays encrypted by the same secret key as the input. Keyswitching is
expensive, and, in practice, \emph{takes over 90\% of all operations}.
Keyswitching is central to \name, so we discuss it in detail in
\autoref{sec:keyswitching}.

\section{Algorithmic insights and optimizations}\label{sec:algoInsights}
\label{sec:fhe_optimizations}

F1 leverages two optimizations developed in prior work:

\paragraph{Fast polynomial multiplication via NTTs:} Multiplying two
polynomials requires convolving their coefficients, an expensive (naively
$O(N^2)$) operation. Just like convolutions can be made faster with the Fast
Fourier Transform, polynomial multiplication can be made faster with the
Number-Theoretic Transform (NTT)~\cite{moenck1976practical},  % victor asked
for a  ``reassuring read-more-about-NTT citation'' a variant of the discrete
Fourier transform for modular arithmetic. The NTT takes an $N$\hyp{}coefficient
polynomial as input and returns an $N$\hyp{}element vector representing the
input in the \textit{NTT domain}. Polynomial multiplication can be performed as
element-wise multiplication in the NTT domain. Specifically,
\begin{equation*}
    NTT(\mathfrak{a}\mathfrak{b}) = NTT(\mathfrak{a}) \odot NTT(\mathfrak{b}),
\end{equation*}
where $\odot$ denotes component-wise multiplication. (For this relation to hold
with $N$\hyp{}point NTTs, a \emph{negacyclic}
NTT~\cite{lyubashevsky:tact10:ideal} must be used (\autoref{sec:fourStepNTT}).)

Because an NTT requires only $O(N \log N)$ modular operations, multiplication
can be performed in $O(N \log N)$ operations by using two forward NTTs,
element-wise multiplication, and an inverse NTT. And in fact, optimized FHE
implementations often store polynomials in the NTT domain rather than in their
coefficient form \emph{across operations}, further reducing the number of NTTs.
This is possible because the NTT is a linear transformation, so additions and
automorphisms can also be performed in the NTT domain:

\begin{align*}
    NTT(\sigma_k(\mathfrak{a})) &= \sigma_k(NTT(\mathfrak{a})) \\
    NTT(\mathfrak{a} + \mathfrak{b}) &= NTT(\mathfrak{a}) + NTT(\mathfrak{b})
\end{align*}

\paragraph{Avoiding wide arithmetic via Residue Number System (RNS) representation:}
FHE requires wide ciphertext coefficients (e.g., 512 bits), but wide arithmetic
is expensive: the cost of a modular multiplier (which takes most of the
compute) grows quadratically with bit width in our range of interest. Moreover,
\mbox{we need to efficiently} support a broad range of widths (e.g., 64 to 512
bits in 32-bit increments), both because programs need different widths, and
because modulus switching progressively reduces coefficient widths.

RNS representation \cite{garner:1959:residue} enables representing a single
polynomial with wide coefficients as multiple polynomials with narrower
coefficients, called \emph{residue polynomials}. To achieve this, the
modulus~$Q$  is chosen to be the product of $L$ smaller distinct primes, $Q =
q_1q_2\cdots\ q_L$. Then, a polynomial in $R_Q$ can be represented as $L$
polynomials in $R_{q_1}, \ldots, R_{q_L}$, where the coefficients in the $i$-th
polynomial are simply the wide coefficients modulo $q_i$. For example, with $W
= 32$-bit words, a ciphertext polynomial with $512$-bit modulus~$Q$ is
represented as $L = \log Q/W = 16$ polynomials with $32$-bit coefficients.

All FHE operations can be carried out under RNS representation, and have either
better or equivalent bit-complexity than operating on one wide-coefficient
polynomial.

\section{Architectural analysis of FHE}
\label{sec:fhe_analysis}

We now analyze a key FHE kernel in depth to understand how we can (and cannot)
accelerate it. Specifically, we consider the the standard keyswitching
operation, which is expensive and takes the majority of work in all of F1's
benchmarks. CraterLake implements \emph{boosted} keyswitching, a variant of
keyswitching more amenable to acceleration (\autoref{sec:keyswitching});
however, F1 does not target boosted keyswitching. Nonetheless, many of the
conclusions from this section carry over to the boosted keyswitching setting.

\autoref{listing:keyswitch} shows an implementation of standard keyswitching.
Standard keyswitching takes three inputs: a polynomial \texttt{x}, and two
\emph{keyswitch hint matrices} \texttt{ksh0} and \texttt{ksh1}. \texttt{x} is
stored in RNS form as $L$ residue polynomials (\texttt{RVec}). Each residue
polynomial \texttt{x[i]} is a vector of $N$ 32-bit integers modulo $q_i$.
Inputs and outputs are in the NTT domain; only the \texttt{y[i]} polynomials
(line 3) are in coefficient form.


\begin{figure}
\begin{center}
  \begin{lstlisting}[caption={Standard keyswitch implementation. \texttt{RVec} is an $N$-element vector of 32-bit values, storing a single RNS polynomial in either the coefficient or the NTT domain.
    }, mathescape=true, style=custompython, label=listing:keyswitch]
  def keySwitch(x: RVec[L],
        ksh0: RVec[L][L], ksh1: RVec[L][L]):
    y = [INTT(x[i],$q_i$) for i in range(L)]
    u0: RVec[L] = [0, ...]
    u1: RVec[L] = [0, ...]
    for i in range(L):
      for j in range(L):
        xqj = (i == j) ? x[i] : NTT(y[i], $q_j$)
        u0[j] += xqj * ksh0[i,j] mod $q_j$
        u1[j] += xqj * ksh1[i,j] mod $q_j$
    return (u0, u1)
  \end{lstlisting}
\end{center}
\end{figure}

\paragraph{Computation vs.\ data movement:}
A single keyswitch requires $L^2$ NTTs, $2L^2$ multiplications, and $2L^2$
additions of $N$-element \mbox{vectors}. In RNS form, the rest of a homomorphic
multiplication (excluding keyswitching) is $4L$ multiplications and $3L$
additions (\autoref{sec:fhe_operation}), so keyswitching is dominant.

However, the main cost at high values of $L$ and $N$ is data movement. For
example, at $L = 16$, $N = 16K$, each RNS polynomial (\texttt{RVec}) is 64\,KB;
each ciphertext polynomial is 1\,MB; each ciphertext is 2\,MB; and the
keyswitch hints dominate, taking up 32\,MB. With F1's compute throughput,
fetching the inputs of each keyswitching from off-chip memory would demand
about 10\,TB/s of memory bandwidth. Thus, it is crucial to reuse these values
as much as possible.

Fortunately, keyswitch hints can be reused: all homomorphic multiplications use
the same keyswitch hint matrices, and each automorphism has its own pair of
matrices. But values are so large that few of them fit on-chip.

Finally, note that there is no effective way to decompose or tile this
operation to reduce storage needs while achieving good reuse: tiling the
keyswitch hint matrices on either dimension produces many long-lived
intermediate values; and tiling across \texttt{RVec} elements is even worse
because in NTTs every input element affects every output element.

\paragraph{Performance requirements:} We conclude that, to accommodate these
large operands, an FHE accelerator requires a memory system that \emph{(1)}
decouples data movement from computation, as demand misses during frequent
keyswitches would tank performance; and \emph{(2)} implements a large amount of
on-chip storage (over 32\,MB in our example) to allow reuse across entire
homomorphic operations (e.g., reusing the same keyswitch hints across many
homomorphic multiplications).

Moreover, the FHE accelerator must be designed to use the memory system well.
First, scheduling data movement and computation is crucial: data must be
fetched far ahead of its use to provide decoupling, and operations must be
ordered carefully to maximize reuse. Second, since values are large, excessive
parallelism can increase footprint and hinder reuse. Thus, the system should
use relatively few high-throughput functional units rather than many
low-throughput ones.

\paragraph{Functionality requirements:}
Programmable FHE accelerators must support a wide range of parameters, both $N$
(polynomial/vector sizes) and $L$ (number of RNS polynomials, i.e., number of
32-bit prime factors of $Q$). While $N$ is generally fixed for a single
program, $L$ changes as modulus switching sheds off polynomials.

Moreover, FHE accelerators must avoid overspecializing in order to support
algorithmic diversity. For instance, we have described \emph{an} implementation
of keyswitching, but there are
others~\cite{kim:jmir18:helr,gentry:crypto2012:homomorphic} with different
tradeoffs.

F1 accelerates \emph{primitive operations on large vectors}: modular
arithmetic, NTTs, and automorphisms. It exploits wide vector processing to
achieve very high throughput, even though this makes NTTs and automorphisms
costlier. F1 avoids building functional units for coarser primitives, like
keyswitching, which would hinder algorithmic diversity.

\section{Challenges of Deep FHE Computation}\label{sec:deepChallenges}

FHE ciphertexts include some \emph{noise} or \emph{error} term above) to ensure
cryptographic privacy~\cite{lyubashevsky:tact10:ideal}. Noise compounds during
homomorphic operations, which adds overheads. Noise increases primarily during
ciphertext multiplications; each ciphertext can tolerate only a fixed amount of
noise before decryption becomes impossible. Therefore, we say that the
\emph{multiplicative depth} a ciphertext can tolerate is the ciphertext's
\emph{multiplicative budget}.

Supporting a high multiplicative budget requires using ciphertexts with wide
coefficients and a large ciphertext modulus $Q$. For example, ciphertexts with
512-bit coefficients have a multiplicative budget of about 16. After each
multiplication, the ciphertext is \emph{rescaled} to use a smaller modulus
(e.g., dropping 32 bits). This trims the noise and makes computation more
efficient over time, as narrower coefficients are cheaper to operate on.
Ciphertexts run out of multiplicative depth when their coefficients become too
narrow to support further operations (e.g., 32 bits). In CKKS, the specific
number of bits to drop per operation is not fixed, but depends on the precision
that the application requires.

A high multiplicative depth computation can be supported by simply using
ciphertexts with sufficiently high multiplicative budgets, but this adds major
overheads. First, it requires using very wide coefficients, which take more
storage per plaintext element and make computations more complex. Moreover,
wide coefficients induce a second hurdle: they force the use of larger vectors.
This is because, for security, $N/\log Q$ must be above a certain threshold.
For instance, a multiplicative budget of 16 requires $Q$ of about 512 bits and
$N$=16K (i.e., 2\,MB per ciphertext), and a multiplicative budget of 32
requires $Q$ of about 1,024 bits and $N$=32K (i.e., 8\,MB per ciphertext).
Though larger vectors can pack more plaintext elements, this quickly results in
vectors so large that they cannot fit on-chip. Overall, ciphertext size grows
quadratically with multiplicative budget, and compute cost cubically (inducing
linear and quadratic overheads per plaintext element, respectively).

\figBootstrapping

\paragraph{Bootstrapping:} FHE schemes limit the overheads of deep computation
through a procedure called bootstrapping that refreshes the multiplicative
budget of a ciphertext. Bootstrapping enables computations of arbitrary depth
by separating them into regions of limited depth. But
bootstrapping~is~an~expensive and deep computation, so it should happen
infrequently.

\autoref{fig:bootstrapping} illustrates a typical evolution of a ciphertext's
multiplicative budget during execution of a program: computation proceeds until
the ciphertext runs out of budget, then bootstrapping is applied to refresh the
ciphertext. For example, in our LSTM benchmark, computation starts with a
multiplicative budget of 57 and bootstrapping consumes the highest 35 levels
(in red in \autoref{fig:bootstrapping}), leaving 22 levels for application
computation (in blue in \autoref{fig:bootstrapping}).

\paragraph{Ciphertext sizes needed for deep FHE:}
We now show that \name supports the ciphertext sizes required for deep FHE, and
why prior work falls short. \autoref{fig:bootstrappingFrequency} reports the
cost per homomorphic operation (in scalar multiplies per homomorphic multiply,
$y$-axis) of two deep programs, as a function of the maximum ciphertext size
used ($x$-axis). This determines bootstrapping frequency: using larger
ciphertexts requires less frequent bootstrapping.
\autoref{fig:bootstrappingFrequency} also breaks down cost by~that used for
application computation (blue) and bootstrapping (red).

The left plot is for a serial chain of multiplies, the worst case for
bootstrapping cost, as the amount of computation between bootstrappings is
minimal. Consequently, bootstrapping cost dominates. By contrast, the right
plot is for a very wide graph with 100 multiplies per depth, which converge to
a single output after each level. This allows boostrapping to be amortized
across many operations, a best-case scenario.

Crucially, the optimal choice of maximum ciphertext size (shown with a black
dot) is in a narrow range for both extremes, between 20\,MB (right) and 26\,MB
(left). This is because \emph{both} application computation and bootstrapping
become more expensive with ciphertext size, so regardless of which dominates,
once bootstrapping is infrequent enough, moving to larger ciphertexts only
hurts performance.

Thus, 20--26\,MB max ciphertexts are the sweet spot for most deep programs,
which fall between these extremes. In practice, bootstrapping placement is
NP-hard~\cite{benhamouda2017optimization}, because real FHE programs are not as
regular. But all our benchmarks show a similar tradeoff curve to these
synthetic programs.

\figBootstrappingFrequency

\emph{Prior FHE accelerators cannot efficiently support ciphertexts this
large}. For example, F1~\cite{feldmann:micro21:f1} \emph{becomes inefficient
past 2\,MB}. Prior accelerators~\cite{riazi:asplos20:heax} are limited to even
smaller values. This is insufficient to run even bootstrapping itself.
(Although F1 supports \emph{unpacked} bootstrapping of ciphertexts that encode
only a single element, this is $>$1,000$\times$ slower per element and thus
impractical for full applications, as \autoref{sec:results} shows.)

As we will see, scaling to large ciphertexts is not merely a matter of scaling
up hardware; it requires new algorithms and a new hardware organization to
support these algorithms and to cope with the huge footprint of ciphertexts.

\section{CraterLake vs. F1}

Though F1 is programmable and can accelerate full computations, it targets
shallow computations. Specifically, F1 is tailored to a keyswitching algorithm
that does not scale to high multiplicative budgets $L$
(\autoref{sec:keyswitching}). As a result, F1 is inefficient when using a more
scalable keyswitching algorithm: It has an inappropriate mix of functional
units, and even with the right mix, it would be dominated by simple operations
that would require over 100 register file ports for the FUs to be fully
utilized. Moreover, F1's organization incurs excessive communication and is
hard to scale to larger systems.

As a result, \name introduces a fundamentally different design, needed for deep
computations: it adopts a new, simpler hardware organization and data tiling
approach that reduces communication and scales to the large ciphertexts
required, and it is tailored to use an efficient keyswitching algorithm, which
requires new functional units and optimizations.

\section{Prior FHE Accelerators and Their Limitations}\label{sec:drawbacks}


Prior work has proposed several FHE accelerators for
FPGAs~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,migliore:tecs17:he-karatsuba,riazi:asplos20:heax,turan:tc20:heaws,mert:tvlsi20:bfv-accel}.
These systems have three important limitations. First, they work by
accelerating some primitives but defer others to a general-purpose host
processor, and rely on the host processor to sequence operations. This causes
excessive data movement that limits speedups. Second, these accelerators build
functional units for \emph{fixed parameters} $N$ and $L$ (or $\log Q$ for those
not using RNS). Third, many of these systems build overspecialized primitives
that limit algorithmic diversity.

Most of these systems achieve limited speedups, about 10$\times$ over software
baselines. HEAX~\cite{riazi:asplos20:heax} achieves larger speedups
(200$\times$ vs.\ a single core). But it does so by overspecializing: it uses
relatively low-throughput functional units for primitive operations, so to
achieve high performance, it builds a fixed-function pipeline for keyswitching.
