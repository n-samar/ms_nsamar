\section{Background}\label{sec:background}
% \nikola{need to introduce the NTT somewhere in this section; probably after data representation}
% \nikola{need to define an automorphism in this section; probably when defining homomorphic rotation}
% \nnote{referring to CKKS as an FHE scheme even though it's only for approximate arithmetic... people sometimes complain about this}


% CKKS\footnote{named after the authors}~\cite{ckks}
% is an FHE scheme that supports encrypted computations on vectors of real numbers.
% CKKS ciphertexts are a pair of degree-$N$ polynomials with coefficients
% being large integers modulo some $Q$. Common parameter
% regimes include $N$ from $1K$ to $64K$ and $\log_2(Q)$
% from $1K$ to $2K$.
% Choosing $N$ and $Q$ for proper performance
% and security depends on the program you want to run, and is an active area of
% research~\cite{DBLP:conf/crypto/May21}.
% For this reason, we focus exclusively on optimizing
% the performance of CKKS, although our accelerator can also be
% used for other FHE schemes.


State-of-the-art % dsm: Unless this is TFHE jab, why? nikola: The point is that in theory there may exist yet-undiscovered FHE schemes that work really well but dont operate on encrypted vectors :P
FHE schemes implement operations on~\emph{encrypted vectors}.
The FHE ciphertexts in these schemes support several \emph{homomorphic operations}:
element-wise addition, element-wise multiplication, and cyclic rotations of vector elements.
Each homomorphic operation produces a ciphertext that, when decrypted,
produces the same result as if the operation had been performed on the unencrypted inputs.

Importantly, homomorphic operations have a different implementation from their unencrypted counterparts---for example,
a homomorphic multiplication is not implemented using element-wise multiplication
of the input ciphertexts, but a more complex
sequence of operations. Therefore, it is useful to differentiate between FHE's \emph{interface},
i.e., its supported plaintext datatypes and operations,
and its \emph{implementation},
i.e., the structure of ciphertexts and the implementation of homomorphic operations.

There are several FHE schemes, which mainly differ in their plaintext datatypes and the operations they support.
For example, BGV~\cite{brakerski:toct14:leveled} encodes vectors of integers modulo a constant,
whereas CKKS~\cite{cheon:ictaci17:homomorphic} encodes vectors of fixed-point numbers.
Despite the differences between these schemes, % all state-of-the-art FHE schemes
%use a similar format for ciphertexts, as they rely on the hardness of the same problem
%(LWE or ring-LWE~\cite{lyubashevsky:tact10:ideal}) to attain security.
%have similarities since they all rely on the hardness of learning with errors (LWE) or its ring variant (ring-LWE)~\cite{lyubashevsky:tact10:ideal} to attain security.
the commonalities in their underlying implementation~\cite{lyubashevsky:tact10:ideal}
make it possible for the same hardware to
accelerate many schemes
efficiently---\name supports CKKS, BGV, and GSW~\cite{gentry:crypto13:homomorphic}.
For concreteness, the rest of this section will focus only on CKKS as it
is the scheme best suited for machine learning tasks and has been the focus of
recent work in FHE algorithms~\cite{han:iacr18:efficient,lee:2021:privacy,gilad:icml16:cryptonets,podschwadt:2020:classification,dathathri:pldi19:chet,dathathri:pldi20:eva}.

\subsection{FHE Interface}

%State-of-the-art % dsm: Whatever, once was enough
FHE schemes implement operations on vectors of values.
In CKKS, each vector element is a \emph{fixed-point} complex number with a configurable number of bits.
(Programs that do not use complex arithmetic can zero out the imaginary part.)
% with a set number of quotient and mantissa bits. % nikola: confusing % dsm: why?
% nikola: this is confusing because it is the definition of fixed point.
% so why put it there? As a reminder? That's alright, but I would add "i.e." or
% something
% dsm: No, it is important because architects are used to thinking about FIXED FORMATS, like fp32, bfloat16, etc. We must say that the number of bits is configurable.

Since values are encrypted, FHE does not permit data-dependent branching or indirection.
Thus, all operations and dependencies are known ahead of time, and FHE programs can
be represented using \emph{static dataflow graphs}.

% nikola: regarding Daniel's note on arbitrarily vs relatively. It _is_ true
% that we can make computation arbitrarily precise (just add bits of precision!).
Homomorphic operations in CKKS include element-wise addition, element-wise multiplication,
and cyclic rotations.
While these operations are \emph{approximate} in CKKS, inducing a small and
controllable amount of error, this error can be made arbitrarily small at the
cost of reduced performance. % \nnote{this is only true for a priori bounded-depth computation, but i think it's fine to not mention this}
% dsm: Someone changed arbitrarily to relatively. Unless this error absolutely cannot be reduced beyond a non-zero threshold, please don't fudge it. I don't care that it's expensive and it's not done in practice. Digital logic is also expensive vs analog, and wasn't considered practical for a long time, yet here we are. I'm trying to distance this from the minefield thsat is approximate computing.
This error is acceptable in practice as machine learning applications
are insensitive to it.

FHE exposes a vector programming model with a restricted set of operations; in particular,
FHE does not provide access to individual vector elements.
This makes it challenging to implement some operations
that are trivial in plaintext:
For example,
implementing a convolutional layer of a neural network requires the careful
replication of filter weights.
The lack of non-linear functions introduces other difficulties.
For example, the ReLU activation function must be approximated
using a high-degree polynomial~\cite{lee:2021:precise}.
%
As a result, faithfully replicating deep neural networks in FHE,
as done by a recent ResNet implementation~\cite{lee:2021:privacy}, comes at a high compute cost.
However, recent work has proposed neural network structures that are optimized
for FHE and achieve higher performance while maintaining similar accuracy~\cite{brutzkus:icml19:low}.
We evaluate \name on both styles of neural networks.

Finally, not all data needs to be encrypted:
additions and multiplications are much cheaper in FHE if one of the operands is unencrypted.
This allows algorithms to trade privacy for performance.
For example, running a neural network using unencrypted weights is faster; it
still ensures the privacy of inputs and results, but does not
protect weights~\cite{brutzkus:icml19:low}.

\subsection{FHE Implementation}

% \nnote{some FHE schemes are actually quite different, so i changed the wording here a bit. also, GSW is kind of different from BGV and CKKS}
We now describe how CKKS represents and operates on encrypted data (i.e., ciphertexts);
other schemes (e.g., BGV) have a similar structure.

\paragraph{Encryption:} A ciphertext holds an encrypted vector of plaintext values.
To create a ciphertext, the vector of plaintext values is first encoded, or \emph{packed},
in a polynomial; this polynomial is then \emph{encrypted}.
CKKS packs a plaintext vector of $ n = N/2$ complex fixed-point numbers
into a degree-$(N-1)$ polynomial:
% dsm: Sinbce this modulus doesn;t show up anywhere else, and it's unclear how you would pick t to support fixed-point numbers, I'm killing it.
%with integer coefficients modulo a plaintext
%modulus $t$:
\begin{equation*}
    (c_0, c_1, ..., c_n) \xmapsto{pack} \mathfrak{m} = k_0 + k_1x + ... + k_{N-1}x^{N-1} %\in R_q
\end{equation*}
% NOTE(dsm): Do not call m a plaintext.
$\mathfrak{m}$ is then encrypted into a ciphertext. Each ciphertext consists of
% nikola: better to call these ct_0, ct_1 instead of p_0, p_1 cuz then the homomorphic add example makes more sense (i.e., ct. polys are named after the ciphertext + an index)
$\mathfrak{ct}_0, \mathfrak{ct}_1$---two \emph{ciphertext polynomials}
with coefficients modulo a \emph{ciphertext modulus} $Q$.
Specifically, we encrypt $\mathfrak{m}$
under a \emph{secret key} $\mathfrak{s}$
by sampling a uniformly random $\mathfrak{a}$
and a small \emph{error} $\mathfrak{e}$ ($\mathfrak{s}$, $\mathfrak{a}$, and $\mathfrak{e}$ are also polynomials):
% nikola: change the above remark to a footnote instead of writing \in R_Q because s and e are *just* polynomials, and a is actually in R_Q. This distinction doesnt matter for ISCA, so I just say they are all polynomials, which is correct. But saying they are all in R_Q would actually be incorrect, so I forgoe it
\begin{equation*}
    \mathfrak{m} \xmapsto{encrypt} \mathfrak{ct} = (\mathfrak{ct}_0, \mathfrak{ct}_1) = (\mathfrak{a}, \mathfrak{a}\cdot\mathfrak{s}+\mathfrak{e}+\mathfrak{m})
\end{equation*}
% dsm: Comment out if low on space... but we do use partially packed later on.
The above process produces a \emph{fully-packed} ciphertext, i.e., one
that encodes as many plaintext values as possible. It is possible
(though almost always less efficient) to pack~fewer~values,
producing a partially packed or unpacked (one-element) ciphertext.

% dsm: We need to be careful with how this is described, it seems like this is horribly approximate and the interface is already talking about the error. Positionaing this as approximate computing would be a strategic mistake.
%The noise term is necessary for the message $m$ to be cryptographically secure.
%Also, note how the message's low-order bits are corrupted by the error.
%This often does not interfere with the final computation
%since computations done with CKKS are convergent and CKKS's
%encoding mechanism accounts for the error-message interaction,
%i.e.\ the significant bits are encoded well above the error.
%The latter is because the noise growth in each FHE computation is predictable.


\paragraph{Homomorphic operations} are implemented through several modular-arithmetic operations on ciphertext polynomials, i.e., vectors of coefficients. Specifically:
\begin{compactitem}
\item \emph{Homomorphic addition} of two ciphertexts simply requires modular addition of their ciphertext polynomials:
    $\mathfrak{ct}_{\textrm{add}} = \mathfrak{a} + \mathfrak{b} = (\mathfrak{a}_0+\mathfrak{b}_0, \mathfrak{a}_1+\mathfrak{b}_1)$.
\item \emph{Homomorphic multiplication} is implemented using polynomial multiplications and additions;
  multiplying two polynomials requires convolving their coefficients.
% nikola: there was a convolving elements thing here... this is confusing. I would just say it this way.
% dsm: Nikola, the whole point is to make it crystal clear thea POLYNOMAIL MULTIPLICATION != MULTIPLICATION. Otherwise the NTT will ome as a complete surprise.
\item \emph{Homomorphic rotation} rotates the vector encrypted~in~a~ciphertext.
  Implementing it requires performing an \emph{automorphism} on the ciphertext polynomials,
  % nikola: there is no benefit to us in explaining what automorphisms are (this is different from F1, where this was critical as it was one of our contributions).
  % we can just leave it like above (i.e., automorphism is some fairy dust you sprinkle and you get homomorphic rotates).
  % Actually explainin what an automorphism is causes unnecessary confusion. It's hard enough to remember that homomorphic rotations cyclically rotate the vector.
  % dsm: I disagree. This paper needs to be self-contained! We ARE using F1's approach to implement automorphisms, but if you don't explain them even superficially, epople will jump to barrel shifters, etc.
  a structured permutation where, for automorphism $k$, each input index $i$ is mapped to output index $ik \bmod N$.
  There are $N$ possible automorphisms; each automorphism induces a simpler, cyclic rotation in the plaintext.

\end{compactitem}

On top of this, homomorphic multiplications and rotations also require a procedure called \emph{keyswitching},
which is needed so that the final ciphertext
stays encrypted by the same secret key as the input.
Keyswitching is expensive, and, in practice, \emph{takes over 90\% of all operations}.
Keyswitching is central to \name, so we discuss it in detail in \autoref{sec:keyswitching}.

%Homomorphic addition of ciphertexts
%$\mathfrak{a} = (\mathfrak{a}_0, \mathfrak{a}_1)$ and
%$\mathfrak{b} = (\mathfrak{b}_0, \mathfrak{b}_1)$
%is done simply by adding their corresponding polynomials:
%$ct_{\textrm{add}} = \mathfrak{a} + \mathfrak{b} = (\mathfrak{a}_0+\mathfrak{b}_0, \mathfrak{a}_1+\mathfrak{b}_1)$.
%Homomorphic rotations, on a ciphertext encrypting plaintext $(c_0, c_1, ..., c_n)$,
%are done by ciphertext automorphisms. These are signed permutations
%on the ciphertext coefficients which allow a user to homomorphically
%rotate the plaintext values. There are $n$ such automorphisms,
%but they are generated by one automorphism representing cyclic
%plaintext shift. One can combine these rotations with plaintext
%multiplications (with a 0-1 plaintext vector) to perform any
%permutation homomorphically.

%Some other operations are also straightforward: specifically,
%ciphertext-plaintext additions / multiplications,
%plaintext rotations.
%However, ciphertext-ciphertext multiplications and ciphertext rotations are computationally
%expensive; they require 10-100 polynomial operations. This is because these operations
%require \emph{keyswitching}, a crucial homomorphic suboperation that is responsible
%for over $90$\% of polynomial operations on all of our benchmarks. We discuss
%keyswitching in detail in \autoref{sec:keyswitching}.

%The implementation of homomorphic multiplication is shown in
%\autoref{listing:homomorphicMult}.

\subsection{Challenges of Deep FHE Computation}\label{sec:deepChallenges}

%add overheads to FHE programs beyond those
%inherent in computation on encrypted data, as mentioned in \autoref{sec:intro}.

FHE ciphertexts include some \emph{noise} or \emph{error} %(the $\mathfrak{e}$ term above)
to ensure cryptographic privacy~\cite{lyubashevsky:tact10:ideal}.
Noise compounds during homomorphic operations, which adds overheads.
Noise increases primarily during ciphertext multiplications;
each ciphertext can tolerate
only a fixed amount of noise before decryption becomes impossible.
Therefore, we say that the \emph{multiplicative depth} a ciphertext can tolerate
is the ciphertext's \emph{multiplicative budget}.
% nikola: I try to use multiplicative budget instead of noise budget cuz I want it to
% simplify things: it's a little confusing that "spending noise budget gives you multiplicative levels"
% it is more intuitive to "spend multiplicative budget to get multiplicative levels"

Supporting a high multiplicative budget requires using ciphertexts with wide
coefficients and a large ciphertext modulus $Q$.
For example, ciphertexts with 512-bit coefficients have a multiplicative budget of about 16.
After each multiplication, the ciphertext is \emph{rescaled} to use a smaller modulus
(e.g., dropping 32 bits).
% nikola: modulo switching and rescaling are not the same; in CKKS, you use rescaling (though you sometimes may need modulo-switching, but that's a detail)
This trims the noise and makes computation more efficient over time, as
narrower coefficients are cheaper to operate on.
Ciphertexts run out of multiplicative depth when their coefficients become too
narrow to support further operations (e.g., 32 bits).
In CKKS, the specific number of bits to drop per operation is not fixed, but depends on the precision
that the application requires.

A high multiplicative depth computation can be supported by simply using ciphertexts with
sufficiently high multiplicative budgets, but this adds major overheads.
First, it requires using very wide coefficients, which take more storage per plaintext element and make computations more complex.
Moreover, wide coefficients induce a second hurdle: they force the use of larger vectors.
This is because, for security, $N/\log Q$ must be above a certain threshold.
For instance, a multiplicative budget of 16 requires $Q$ of about 512 bits and $N$=16K (i.e., 2\,MB per ciphertext),
and a multiplicative budget of 32 requires $Q$ of about 1,024 bits and $N$=32K (i.e., 8\,MB per ciphertext).
Though larger vectors can pack more plaintext elements, %and the overheads of larger $N$ are amortized relatively well per plaintext element,
this quickly results in vectors so large that they cannot fit on-chip.
Overall, ciphertext size grows quadratically with multiplicative budget, and compute cost cubically
(inducing linear and quadratic overheads per plaintext element, respectively).


\figBootstrapping

\paragraph{Bootstrapping:} FHE schemes limit the overheads of deep computation
through a procedure called bootstrapping that refreshes the multiplicative budget of a ciphertext.
Bootstrapping enables computations of arbitrary depth by separating them into regions of limited depth.
But bootstrapping~is~an~expensive and deep computation,
so it should happen infrequently.

\autoref{fig:bootstrapping} illustrates a typical evolution of a ciphertext's
multiplicative budget during execution of a program:
computation proceeds until the ciphertext runs out of budget, then bootstrapping is applied to refresh the ciphertext.
For example, in our LSTM benchmark, computation starts with a multiplicative budget of 57 and bootstrapping
consumes the highest 35 levels (in red in \autoref{fig:bootstrapping}), leaving 22 levels for application computation (in blue in \autoref{fig:bootstrapping}).
% Though bootstrapping happens at high depth and is thus more expensive, it can often be amortized well
% if data elements are reused (e.g., when applying bootstrapping between layers of a neural network).
% nikola: I think this is vague and may not even be true. I don't really understand what it means.


\paragraph{Ciphertext sizes needed for deep FHE:}
We now show that \name supports the ciphertext sizes required for deep FHE, and why prior work falls short.
\autoref{fig:bootstrappingFrequency} reports the cost per homomorphic operation
(in scalar multiplies per homomorphic multiply, $y$-axis) of two deep programs,
as a function of the maximum ciphertext size used ($x$-axis).
This determines bootstrapping frequency:
using larger ciphertexts requires less frequent bootstrapping.
\autoref{fig:bootstrappingFrequency} also breaks down cost by~that used for application computation (blue)
and bootstrapping (red).

The left plot is for a serial chain of multiplies, the worst case for bootstrapping cost, as the amount of computation between bootstrappings is minimal. Consequently, bootstrapping cost dominates.
By contrast, the right plot is for a very wide graph with 100 multiplies per depth, which converge to a single output after each level.
This allows boostrapping to be amortized across many operations, a best-case scenario.

Crucially, the optimal choice of maximum ciphertext size (shown with a black dot)
is in a narrow range for both extremes, between 20\,MB (right) and 26\,MB (left).
This is because \emph{both} application computation and bootstrapping become more expensive
with ciphertext size, so regardless of which dominates,
once bootstrapping is infrequent enough, moving to larger ciphertexts only hurts performance.

Thus, 20--26\,MB max ciphertexts are the sweet spot for most deep programs,
which fall between these extremes.
In practice, bootstrapping placement is NP-hard~\cite{benhamouda2017optimization}, because real FHE programs are not as regular.
But all our benchmarks show a similar tradeoff curve to these synthetic programs.
% nikola: i think stating the NP-hardness result is important. Without it, the reviewers might say:
% these guys only optimized for their benchmark, and they don't know whether the Q they support
% will actually work. At least this way they know it is not knowable in general.

\figBootstrappingFrequency

\emph{Prior FHE accelerators cannot efficiently support ciphertexts this large}.
For example, F1~\cite{feldmann:micro21:f1} \emph{becomes inefficient past 2\,MB}.
Prior accelerators~\cite{riazi:asplos20:heax} are limited to even smaller values.
This is insufficient to run even bootstrapping itself.
(Although F1 supports \emph{unpacked} bootstrapping of ciphertexts that encode
only a single element, this is $>$1,000$\times$ slower per element and thus impractical for
full applications, as \autoref{sec:results} shows.)

As we will see, scaling to large ciphertexts is not merely a matter of scaling up hardware;
it requires new algorithms and a new hardware organization to support these algorithms
and to cope with the huge footprint of ciphertexts.

\subsection{Implementation Optimizations}
\label{sec:rns}
\label{sec:ntt}

We use two common FHE implementations optimizations:

\paragraph{Residue Number System (RNS)} representation~\cite{garner:1959:residue}
allows representing each of the wide coefficients of a ciphertext polynomial as $L$ \emph{residue polynomials}
with narrow coefficients. This is achieved by choosing the wide modulus $Q$ to be a product of $L$ smaller factors, $Q = q_1q_2...q_L$, called \emph{small moduli}.
% nikola: need to dene this term cuz it is used later in bitwidth section
Then, $x \bmod Q$ is uniquely represented as $(x \bmod q_1, x \bmod q_2, ..., x \bmod q_L)$.

RNS representation 
% dsm: REVISION-TRIMMED
%brings multiple advantages: \emph{(1)} narrower coefficients are easier to operate on,
%and \emph{(2)} with the algorithms we use, \emph{all FHE operations have lower bit-complexity} than when using wide coefficients.
reduces overall operation cost, and allows supporting many coefficient widths with a single narrow width in hardware.
\name exploits this by using 28-bit elements
(algorithm-related limits prevent using arbitrarily narrow coefficients, as \autoref{sec:bitwidth} explains).
For example, a ciphertext polynomial with 1,500-bit $Q$ is stored using $L$=54 28-bit residue polynomials.

% dsm: REVISION-TRIMMED
\begin{comment}
Second, RNS makes it easy to drop unneeded bits from each coefficient as multiplicative budget decreases
by dropping residue polynomials.
This allows us to implement efficient wide arithmetic on a broad range of widths
while supporting a single, narrow width in hardware.
\end{comment}

%To summarize (\autoref{fig:tiling}), a single ciphertext is represented by two 
%ciphertext polynomials. Each ciphertext polynomial is stored as $L$ RNS polynomials. 
%Each RNS polynomial is stored as a vector of polynomial coefficients modulo a 
%small prime $q_i$ (\autoref{fig:tiling}).

\paragraph{Number-Theoretic Transform (NTT)} is a modular-arithmetic variant of the Fast Fourier Transform.
% dsm: Mentioning complexity to give a sense of why it improves performance (+more expensive than typical FUs)
The NTT is an $O(N\log N)$ operation
that makes polynomial multiplications efficient:
in the NTT domain, the convolution required for polynomial multiplication becomes element-wise multiplication.
% alex: Should we instead say that *we* store polynomials in NTT form most of the time?
FHE implementations often store polynomials in the NTT domain.
\name includes NTT functional units.
% dsm: Unnecessary
%and \name adopts F1's specialized NTT functional unit.

% dsm: REVISION-TRIMMED
%\begin{figure}
%  \begin{lstlisting}[caption={CKKS homomorphic multiplication implementation.\vspace{0.09in}}, mathescape=true, style=custompython, label=listing:homomorphicMult]
%def mult((a$\textsubscript{0}$[0:L], a$\textsubscript{1}$[0:L]), (b$\textsubscript{0}$[0:L], b$\textsubscript{1}$[0:L])):
%  p$\textsubscript{00}$[0:L] = a$\textsubscript{0}$[0:L] * b$\textsubscript{0}$[0:L]
%  p$\textsubscript{01+10}$[0:L] = a$\textsubscript{0}$[0:L]*b$\textsubscript{1}$[0:L] + a$\textsubscript{1}$[0:L]*b$\textsubscript{0}$[0:L]
%  (ks$\textsubscript{0}$, ks$\textsubscript{1}$) = keySwitch(a$\textsubscript{1}$[0:L] * b$\textsubscript{1}$[0:L])
%  res$\textsubscript{0}$[0:L] = p$\textsubscript{00}$[0:L] + ks$\textsubscript{0}$[0:L]
%  res$\textsubscript{1}$[0:L] = p$\textsubscript{01+10}$[0:L] + ks$\textsubscript{1}$[0:L]
%  return (rescale(res$\textsubscript{0}$[0:L]), rescale(res$\textsubscript{1}$[0:L]))
%
%def rescale(res$\textsubscript{i}$[0:L]):
%  xINTT$\textsubscript{i}$ = INTT(res$\textsubscript{i}$[L-1], L-1)
%  subMe$\textsubscript{i}$[0:L-1] = [NTT(xINTT$\textsubscript{i}$, j) for j in [0:L-1]]
%  return res$\textsubscript{i}$[0:L-1] - subMe$\textsubscript{i}$[0:L-1]
%     \end{lstlisting}
%\end{figure}

\begin{comment}
\paragraph{Putting it all together:} \autoref{listing:homomorphicMult} shows the implementation 
of homomorphic multiplication in CKKS using the above techniques. Each of the input ciphertexts (\verb|a| and \verb|b|)
consist of two ciphertext polynomials, and each ciphertext polynomial has $L$ residue polynomials (e.g., \texttt{a$\textsubscript{1}$[0:L]}).
Each residue polynomial is an $N$-coefficient vector (the code is written using vector operations and $N$ is implicit).
Inputs and outputs are in the NTT domain.
The homomorphic multiplication consists of additions and multiplications of these inputs, followed by a \emph{keyswitch} (which we will detail later, and dominates execution for large $L$).
Finally, \emph{rescaling} trims the noise of the resulting ciphertext (and drops one of 
the $L$ residue polynomials).
\end{comment}

\subsection{Prior FHE Accelerators and Their Limitations}\label{sec:drawbacks}

Prior work has proposed multiple FHE accelerators.
From these, F1~\cite{feldmann:micro21:f1} is the closest to \name.
F1 is a statically scheduled processor with clusters of vector functional units
specialized to FHE. In particular, F1 introduces new high-throughput vector 
functional units for NTTs and automorphisms, all-to-all operations that cannot 
be efficiently performed with conventional SIMD datapaths.

Though F1 is programmable and can accelerate full computations,
it targets shallow computations.
Specifically, F1 is tailored to a keyswitching algorithm
that does not scale to high multiplicative budgets $L$ (\autoref{sec:keyswitching}).
As a result, F1 is inefficient when using a more scalable keyswitching algorithm: It has an inappropriate mix of functional units, and
even with the right mix, it would be dominated by simple operations that would require over 100 register file ports for the FUs to be fully utilized.
Moreover, F1's organization incurs excessive communication and is hard to scale to larger systems.
% nikola: 100 ports comes from the fact that the CRB would have everything go through RF ports.
% Even if we are generous, and say that the constants are not streamed from the RF, that's still
% 2*L_max ports (one per multiply + one per add). Which is 2*60=120
%on this algorithm.
% dsm: Also on others though.

As a result, \name introduces a fundamentally different design, needed for deep computations:
it adopts a new, simpler hardware organization and data tiling approach
that reduces communication and scales to the large ciphertexts required,
and it is tailored to use an efficient keyswitching algorithm,
which requires new functional units and optimizations.
%\name adopts some of F1's FUs and retains its programmability.

%\name leverages several of F1's contributions, including adopting its functional units and retaining its programmability.
%Unlike F1, \name is tailored to use the efficient keyswitching needed for deep computations.
%\name uses a simpler structure that reduces communication and footprint, introduces
%new FUs to accelerate key kernels, and allows chaining FUs to operate
%with few register file ports.

Besides F1, prior work proposed accelerators for FPGAs~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,migliore:tecs17:he-karatsuba,riazi:asplos20:heax,turan:tc20:heaws,mert:tvlsi20:bfv-accel}.
These systems are not programmable: they accelerate a small number of primitives and use fixed parameters ($N$ and $L$),
so they cannot execute full applications; they suffer from excessive data movement;
and they achieve limited speedups.
%and they often overspecialize.
%For example, HEAX~\cite{riazi:asplos20:heax}, the state-of-the-art FPGA accelerator,
%builds a fixed keyswitching pipeline using the inefficient algorithm.

% dsm: This text is not parseable by the average reviewer. You first need to describe WHAT PRIOR accelerators do...
\begin{comment}
\paragraph{Optimized to wrong algorithms.} HEAX~\cite{riazi:asplos20:heax} builds
a fixed pipeline for keyswitching, but the algorithm used in the pipeline does not
scale to high multiplicative budgets $L$ (\autoref{sec:keyswitching}), which are necessary
    for deep FHE (\autoref{sec:deepChallenges}).
As the pipeline is fixed, HEAX cannot
be adapted to new algorithms without significant rearchitecting.
F1~\cite{feldmann:micro21:f1} is optimized to the same, suboptimal algorithm as HEAX,
but is at least programmable: it can execute any algorithm. Nonetheless, other
drawbacks disqualify it:

\paragraph{Parallelizing across residue polynomials.} F1 parallelizes
across residue polynomials. We show in \autoref{sec:tiling} that this
approach does not scale to ciphertexts with large multiplicative budgets ($L$) and that
tiling by coefficients is necessary. F1 does not have hardware to support for
this better tiling method.

\paragraph{Narrow vectors.} F1 is a 128-lane architecture. While this
is wide for regular vectors processors, we show that FHE's low
control overheads permit a 2K-lane architecture (\autoref{sec:tiling}).
The main advantage of \name's wide architecture in the context of FHE 
is that the chip's entire logic can operate in tandem on a single
homomorphic operation. This substantially reduces on-chip storage
requirements, as the chip only needs to keep the operands and 
intermediates of a single homomorphic operation on-chip. 
Additionally, \name can
fully utilize its logic even if the FHE program it is running has
scarce or no parallelism.

\paragraph{Poor FU balance.} 
F1 does not have the 
proper balance of functional units for optimal FHE algorithms necessary
for good performance at high multiplicative budgets $L$ (\autoref{sec:keyswitching}).

\paragraph{Register file pressure.} 
Even if F1 had the proper functional unit balance, it would still 
require dozens of register file ports to match \name's performance.
This is because the optimal FU balance prioritizes simple, low-latency FUs 
(i.e., multipliers and adders) in comparison to NTTs;
this increases pressure on register
file ports to a point where F1's architecture is no longer practical.
We show new functional units (\autoref{sec:keyswitching}) \emph{and} 
vector chaining (\autoref{sec:keyswitchingPipeline}) are necessary to 
reduce register file pressure to manageable levels.
\end{comment}

