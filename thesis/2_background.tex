\chapter{Background}\label{sec:background}

Fully Homomorphic Encryption allows performing arbitrary
arithmetic on encrypted plaintext values, via appropriate operations
on their ciphertexts. Decrypting the resulting ciphertext yields the
same result as if the operations were performed on the plaintext
values ``in the clear.''

Over the last decade, prior work has proposed multiple \emph{FHE schemes}, each
with somewhat different capabilities and performance tradeoffs.
CKKS~\cite{cheon:ictaci17:homomorphic}, BGV~\cite{brakerski:toct14:leveled},
B/FV~\cite{brakerski:crypto12:fully,fan:iacr12:somewhat}, and
GSW~\cite{gentry:crypto13:homomorphic} are popular FHE schemes.\footnote{These
scheme names are acronyms of their authors' last names. For instance, BGV is
Brakerski-Gentry-Vaikuntanathan.}~Though these schemes differ in how they
encrypt plaintexts, they all use the same data type for ciphertexts:
polynomials where each coefficient is an integer modulo $Q$. This commonality
makes it possible to build a single accelerator that supports multiple FHE
schemes; F1 and CraterLake both support CKKS, BGV, and GSW.

We describe FHE in a layered fashion: \autoref{sec:fhe_mapping} introduces
FHE's intreface, i.e., its programming model and operations.
\autoref{sec:fhe_operation} describes how FHE operations are
\emph{implemented}. \autoref{sec:fhe_optimizations} presents implementation
\emph{optimizations}. \autoref{sec:drawbacks} discusses how prior accelerators
fall short. \autoref{sec:fhe_analysis} performs an \emph{architectural
analysis} of standard keyswitching, a representative FHE kernel to reveal
acceleration opportunities. \autoref{sec:deepChallenges} introduces the
specific challenges of deep computation and explains why supporting operations
on very large ciphertexts (up to 25\,MB) is critical for performance on deep
benchmarks. \autoref{sec:boostedKeyswitching} describes boosted keyswitching, a
keyswitching algorithm needed for the large ciphertexts in deep computation.
Finally, \autoref{sec:boostedSecurity} presents the performance-security
trade-offs of boosted keyswitching. For concreteness, we \emph{introduce FHE
using the CKKS scheme}, and briefly discuss other FHE schemes in
\autoref{sec:fhe_others}.

\section{FHE Interface}
\label{sec:fhe_mapping}

FHE programs are \emph{dataflow graphs}: directed acyclic graphs where nodes
are operations and edges represent data values. Data values are inputs,
outputs, or intermediate values consumed by one or more operations. All
operations and dependences are known in advance, and data-dependent branching
is impossible.

In FHE, unencrypted (plaintext) data values are always \emph{vectors}; in
CKKS~\cite{brakerski:toct14:leveled}, each vector consists of $N/2$ fixed-point
complex numbers. CKKS provides three operations on these vectors: element-wise
\emph{addition}, element-wise \emph{multiplication}, and vector
\emph{rotations}.

We stress that this is CKKS's \emph{interface}, not its implementation: it
describes \emph{unencrypted} data, and the homomorphic operations that CKKS
implements on that data in its encrypted form.  In \autoref{sec:fhe_operation}
we describe how CKKS represents encrypted data and how each operation is
implemented.

At a high level, FHE provides a vector programming model with restricted
operations where individual vector elements cannot be directly accessed.  This
causes some overheads in certain algorithms. For example, summing up the
elements of a vector is non-trivial, and requires a sequence of vector
rotations and additions.

Despite these limitations, prior work has devised reasonably efficient
implementations of key algorithms, including linear
algebra~\cite{halevi:crypto14:algorithms}, neural network
inference~\cite{brutzkus:icml19:low, gilad:icml16:cryptonets}, logistic
regression~\cite{han:iacr18:efficient}, and genome
processing~\cite{blatt:nas20:secure}. These implementations are often coded by
hand, but recent work has proposed FHE compilers to automate this translation
for particular domains, like deep
learning~\cite{dathathri:pldi19:chet,dathathri:pldi20:eva}.

Finally, note that not all data must be encrypted: CKKS provides versions of
addition and multiplication where one of the operands is unencrypted.
Multiplying by unencrypted data is cheaper, so algorithms can trade privacy for
performance. For example, a deep learning inference can use encrypted weights
and inputs to keep the model private, or use unencrypted weights, which does
not protect the model but keeps inputs and inferences
private~\cite{brutzkus:icml19:low}.

\section{FHE Implementation}
\label{sec:fhe_operation}

We now describe how encrypted data (i.e., a ciphertext) is represented and
processed in FHE schemes. The implementation of each computation on ciphertext
data is called a \emph{homomorphic operation}. For example, the
\emph{homomorphic multiplication} of two ciphertexts yields another ciphertext
that, when decrypted, is the element-wise multiplication of the encrypted
plaintexts. For concreteness, we introduce FHE using the CKKS scheme, and
briefly discuss other FHE schemes in \autoref{sec:fhe_others}.

\paragraph{Encryption:} A ciphertext holds an encrypted vector of plaintext values.
To create a ciphertext, the vector of plaintext values is first encoded, or \emph{packed},
in a polynomial; this polynomial is then \emph{encrypted}.
CKKS packs a plaintext vector of $ n = N/2$ complex fixed-point numbers
into a degree-$(N-1)$ polynomial:

\begin{equation*}
    (c_0, c_1, ..., c_n) \xmapsto{pack} \mathfrak{m} = k_0 + k_1x + ... + k_{N-1}x^{N-1} %\in R_q
\end{equation*}

$\mathfrak{m}$ is then encrypted into a ciphertext. Each ciphertext consists of
$\mathfrak{ct}_0, \mathfrak{ct}_1$---two \emph{ciphertext polynomials}
with coefficients modulo a \emph{ciphertext modulus} $Q$.
Specifically, we encrypt $\mathfrak{m}$
under a \emph{secret key} $\mathfrak{s}$
by sampling a uniformly random $\mathfrak{a}$
and a small \emph{error} $\mathfrak{e}$ ($\mathfrak{s}$, $\mathfrak{a}$, and $\mathfrak{e}$ are also polynomials):
\begin{equation*}
    \mathfrak{m} \xmapsto{encrypt} \mathfrak{ct} = (\mathfrak{ct}_0, \mathfrak{ct}_1) = (\mathfrak{a}, \mathfrak{a}\cdot\mathfrak{s}+\mathfrak{e}+\mathfrak{m})
\end{equation*}

The above process produces a \emph{fully-packed} ciphertext, i.e., one
that encodes as many plaintext values as possible. It is possible
(though almost always less efficient) to pack~fewer~values,
producing a partially packed or unpacked (single-element) ciphertext.

The security of any encryption scheme relies on the ciphertexts not revealing
anything about the value of the plaintext (or the secret key). Without adding
the noise term $\mathfrak{e}$, the original message $\mathfrak{m}$ would be
recoverable from $\mathfrak{ct}$ via simple Gaussian elimination. Including the
noise term entirely hides the plaintext (under cryptographic
assumptions)~\cite{lyubashevsky:tact10:ideal}.

As we will see, homomorphic operations on ciphertexts increase their noise, so
we can only perform a limited number of operations before the resulting noise
becomes too large and makes decryption fail.  In \autoref{sec:noisemgmt} we
describe techniques to keep this noise growth in check.

\subsection{Homomorphic operations}

CKKS supports the following homomorphic operations: element-wise additions,
element-wise multiplications, and cyclic rotations.

\paragraph{\\Homomorphic addition} of ciphertexts
$ct_0 = (\mathfrak{a}_{0}, \mathfrak{b}_{0})$ and
$ct_1 = (\mathfrak{a}_{1}, \mathfrak{b}_{1})$ is done simply by adding
their corresponding polynomials:
$ct_{\text{add}} = ct_0 + ct_1 = (\mathfrak{a}_0 + \mathfrak{a}_1,
\mathfrak{b}_0 + \mathfrak{b}_1)$.

\paragraph{Homomorphic multiplication} requires two steps.
First, the four input polynomials are multiplied and assembled:

\begin{equation*}
  ct_{\times} = (\mathfrak{l}_2, \mathfrak{l}_1, \mathfrak{l}_0) = (\mathfrak{a}_0\mathfrak{a}_1,
  \mathfrak{a}_0\mathfrak{b}_1 + \mathfrak{a}_1 \mathfrak{b}_0,
  \mathfrak{b}_0\mathfrak{b}_1) .
\end{equation*}

This $ct_{\times}$ can be seen as a special intermediate ciphertext encrypted
under a different secret key. The second step performs a \emph{keyswitch\-ing
op\-era\-tion} to produce a ciphertext encrypted under the original secret
key~$\mathfrak{s}$. More specifically, $\mathfrak{l}_2$ undergoes this
keyswitching process to produce two polynomials $(\mathfrak{u}_1,
\mathfrak{u}_0) = \textrm{Keyswitch}(\mathfrak{l}_2)$.  The final output
ciphertext is $ct_{\text{mul}} = (\mathfrak{l}_1 + \mathfrak{u}_1,
\mathfrak{l}_0 + \mathfrak{u}_0)$.

As we will see later (\autoref{sec:fhe_analysis}), keyswitching is an
expensive operation that dominates the cost of a multiplication.

\paragraph{Homomorphic rotations} cyclically rotate the~$N$ plaintext values
that are encrypted in a ciphertext. Homomorphic rotations are
implemented using \emph{automorphisms}, which are special permutations of the
coefficients of the ciphertext polynomials.  There are~$N$ automorphisms,
denoted
$\sigma_k(\mathfrak{a})$ and $\sigma_{-k}(\mathfrak{a})$ for all
positive odd $k<N$. Specifically,

\begin{equation*}
  \sigma_k(\mathfrak{a}): a_i \rightarrow (-1)^{s} a_{i \cdot k \textrm{ mod } N} \text{ for } i=0,...,N-1,
\end{equation*}

where $s=0$ if $i \cdot k \textrm{ mod } 2N < N$, and $s=1$ otherwise. For example,
$\sigma_{5}(\mathfrak{a})$ permutes $\mathfrak{a}$'s coefficients so that $a_0$
stays at position 0, $a_1$ goes from position 1 to position 5, and so on (these
wrap around, e.g., with $N=1024$, $a_{205}$ goes to position~1, since
$205\cdot5 \textrm{ mod } 1024 = 1$).

To perform a homomorphic rotation, we first compute an automorphism on the
ciphertext polynomials: $ct_{\sigma} = (\sigma_k(\mathfrak{a}),
\sigma_k(\mathfrak{b}))$. Just as in homomorphic multiplication, $ct_{\sigma}$
is encrypted under a different secret key, requiring an expensive keyswitch to
produce the final output $ct_{\text{perm}} = (\mathfrak{u}_1,
\sigma_{k}(\mathfrak{b}) + \mathfrak{u}_0)$, where $(\mathfrak{u}_1,
\mathfrak{u}_0) = \text{Keyswitch}(\sigma_k (\mathfrak{a}))$.

We stress that the permutation applied to the ciphertext coefficients
\emph{does not} induce the same permutation on the underlying plaintext vector.
That is, the automorphism permutation of the ciphertext coefficients induces a
cyclic vector rotation of the plaintext vector encrypted by that ciphertext.


\subsection{Managing Noise}
\label{sec:noisemgmt}

As mentioned above, FHE ciphertexts include some \emph{noise} or \emph{error}
to ensure cryptographic privacy~\cite{lyubashevsky:tact10:ideal}. Noise
compounds during homomorphic operations, which adds overheads. Noise increases
primarily during ciphertext multiplications; each ciphertext can tolerate only
a fixed amount of noise before decryption becomes impossible. Therefore, we say
that the \emph{multiplicative depth} a ciphertext can tolerate is the
ciphertext's \emph{multiplicative budget}.

CKKS uses three noise management techniques in tandem: \emph{rescaling},
\emph{modulo switching} and \emph{bootstrapping}.


\paragraph{Rescaling} changes the ciphertext from modulus~$Q$ to a modulus~$Q'
< Q$, which reduces the noise proportionately. Rescaling is usually applied
after each homomorphic multiplication, to reduce its noise blowup. This implies
that ciphertexts with 512-bit coefficients have a multiplicative budget of
about 16, assuming the rescale trims 32 bits. Rescaling not only trims noise
but also makes computation more efficient over time, as narrower coefficients
are cheaper to operate on. Ciphertexts run out of multiplicative depth when
their coefficients become too narrow to support further operations (e.g., 32
bits). In CKKS, the specific number of bits to drop per operation is not fixed,
but depends on the precision that the application requires.

\paragraph{Modulo switching} has similar computation costs and effects to
rescaling: it reduces ciphertext noise by narrowing its modulus. However,
modulo switching does this without changing the underlying plaintext values,
whereas rescaling has a side-effect of dividing the underlying plaintexts by
$Q/Q'$. This division is desirable for CKKS, because CKKS implements
fixed-point arithmetic which requires a division to adjust the fixed-point
after multiplications. While CKKS uses both modulo switching and rescaling,
schemes that support only integer operations, like
BGV~\cite{brakerski:toct14:leveled}, do not use rescaling.

Rescaling and modulo switching imply that having ciphertexts with high
multiplicative budgets requires wide coefficients and a large ciphertext
modulus $Q$. Moreover, wide coefficients induce a second hurdle: they force the
use of larger vectors. This is because, for security, $N/\log Q$ must be above
a certain threshold. For instance, for 80-bit security, a multiplicative budget
of 16 requires $Q$ of about 512 bits and $N$=16K (i.e., 2\,MB per ciphertext),
and a multiplicative budget of 32 requires $Q$ of about 1,024 bits and $N$=32K
(i.e., 8\,MB per ciphertext). Though larger vectors can pack more plaintext
elements, this quickly results in vectors so large that they cannot fit
on-chip. Overall, ciphertext size grows quadratically with multiplicative
budget, and compute cost cubically (inducing linear and quadratic overheads per
plaintext element, respectively).


\figBootstrapping

\paragraph{Bootstrapping} is a procedure that refreshes the multiplicative
budget of a ciphertext. Bootstrapping enables computations of arbitrary depth
by separating them into regions of limited depth. \autoref{fig:bootstrapping}
illustrates a typical evolution of a ciphertext's multiplicative budget during
execution of a program: computation proceeds until the ciphertext runs out of
budget, then bootstrapping is applied to refresh the ciphertext. For example,
in our LSTM benchmark, computation starts with a multiplicative budget of 57
and bootstrapping consumes the highest 35 levels (in red in
\autoref{fig:bootstrapping}), leaving 22 levels for application computation (in
blue in \autoref{fig:bootstrapping}).

\section{Algorithmic Insights and Optimizations}\label{sec:algoInsights}
\label{sec:fhe_optimizations}

We leverage two optimizations developed in prior work:

\paragraph{Fast polynomial multiplication via NTTs:} Multiplying two
polynomials requires convolving their coefficients, an expensive (naively
$O(N^2)$) operation. Just like convolutions can be made faster with the Fast
Fourier Transform, polynomial multiplication can be made faster with the
Number-Theoretic Transform (NTT)~\cite{moenck1976practical}, The NTT takes an
$N$\hyp{}coefficient polynomial as input and returns an $N$\hyp{}element vector
representing the input in the \textit{NTT domain}. Polynomial multiplication
can be performed as element-wise multiplication in the NTT domain.
Specifically,

\begin{equation*}
    NTT(\mathfrak{a}\mathfrak{b}) = NTT(\mathfrak{a}) \odot NTT(\mathfrak{b}),
\end{equation*}

where $\odot$ denotes component-wise multiplication. (For this relation to hold
with $N$\hyp{}point NTTs, a \emph{negacyclic}
NTT~\cite{lyubashevsky:tact10:ideal} must be used (\autoref{sec:fourStepNTT}).)

Because an NTT requires only $O(N \log N)$ modular operations, multiplication
can be performed in $O(N \log N)$ operations by using two forward NTTs,
element-wise multiplication, and an inverse NTT. And in fact, optimized FHE
implementations often store polynomials in the NTT domain rather than in their
coefficient form \emph{across operations}, further reducing the number of NTTs.
This is possible because the NTT is a linear transformation, so additions and
automorphisms can also be performed in the NTT domain:

\begin{align*}
    NTT(\sigma_k(\mathfrak{a})) &= \sigma_k(NTT(\mathfrak{a})) \\
    NTT(\mathfrak{a} + \mathfrak{b}) &= NTT(\mathfrak{a}) + NTT(\mathfrak{b})
\end{align*}

\paragraph{Avoiding wide arithmetic via Residue Number System (RNS) representation:}
FHE requires wide ciphertext coefficients (e.g., 1,500 bits), but wide
arithmetic is expensive: the cost of a modular multiplier (which takes most of
the compute) grows quadratically with bitwidth in our range of interest.
Moreover, we need to efficiently support a broad range of widths (e.g., 64 to
1,500 bits in word-sized increments), both because programs need different
widths, and because rescaling progressively reduces coefficient widths.

RNS representation \cite{garner:1959:residue} enables representing a single
polynomial with wide coefficients as multiple polynomials with narrower
coefficients, called \emph{residue polynomials}. To achieve this, the
modulus~$Q$  is chosen to be the product of $L$ smaller distinct primes, $Q =
q_1q_2\cdots\ q_L$. Then, a polynomial in $R_Q$ can be represented as $L$
polynomials in $R_{q_1}, \ldots, R_{q_L}$, where the coefficients in the $i$-th
polynomial are simply the wide coefficients modulo $q_i$. For example, with $W
= 32$-bit words, a ciphertext polynomial with $512$-bit modulus~$Q$ is
represented as $L = \log Q/W = 16$ polynomials with $32$-bit coefficients.

All FHE operations can be carried out under RNS representation, and have either
better or equivalent bit-complexity than operating on one wide-coefficient
polynomial.

\section{Prior FHE Accelerators and Their Limitations}\label{sec:drawbacks}


Prior work has proposed several FHE accelerators for
FPGAs~\cite{cousins:hpec14:fpga-he,cousins:tetc17:fpga-he,doroz:tc15:accelerating-fhe,roy:hpca19:fpga-he,migliore:tecs17:he-karatsuba,riazi:asplos20:heax,turan:tc20:heaws,mert:tvlsi20:bfv-accel}.
These systems have three important limitations. First, they work by
accelerating some primitives but defer others to a general-purpose host
processor, and rely on the host processor to sequence operations. This causes
excessive data movement that limits speedups. Second, these accelerators build
functional units for \emph{fixed parameters} $N$ and $L$ (or $\log Q$ for those
not using RNS). Third, many of these systems build overspecialized primitives
that limit algorithmic diversity.

Most of these systems achieve limited speedups, about 10$\times$ over software
baselines. HEAX~\cite{riazi:asplos20:heax} achieves larger speedups
(200$\times$ vs.\ a single core). But since it is an FPGA implementation,
HEAX's speedups aren't large enough for it to face memory bandwidth
bottlenecks---a key issue our accelerators tackle. Further, HEAX achieves these
speedups by overspecializing: it builds a fixed hardware pipeline for
\emph{standard keyswitching} (\autoref{sec:fhe_analysis}), a keyswitching
algorithm that does not scale well to large ciphertexts.

\section{Standard Keyswitching}
\label{sec:fhe_analysis}

We now analyze a key FHE kernel in depth to understand how we can (and cannot)
accelerate it. Specifically, we consider the the keyswitching
operation, which is expensive and takes the majority of work in all our
benchmarks.

We focus on standard keyswitching in this section, a keyswitching algorithm
that is the focus of F1 and prior accelerators. In \autoref{sec:deepChallenges}
we will motivate the need for large ciphertexts to support deep computation. We
will then explain in \autoref{sec:boostedKeyswitching} why standard
keyswitching works poorly for large ciphertexts and present \emph{boosted
keyswitching}: a keyswitching algorithm needed for the large ciphertexts used
in deep FHE.

Nonetheless, standard keyswitching is somewhat easier to present and most of
the insights from this section still hold for boosted keyswitching.

\autoref{listing:keyswitch} shows an implementation of standard keyswitching.
Standard keyswitching takes three inputs: a polynomial \texttt{x}, and two
\emph{keyswitch hint matrices} \texttt{ksh0} and \texttt{ksh1}. \texttt{x} is
stored in RNS form as $L$ residue polynomials (\texttt{RVec}). Each residue
polynomial \texttt{x[i]} is a vector of $N$ 32-bit integers modulo $q_i$.
Inputs and outputs are in the NTT domain; only the \texttt{y[i]} polynomials
(line 3) are in coefficient form.


\begin{figure}
\begin{center}
  \begin{lstlisting}[caption={Standard keyswitch implementation. \texttt{RVec} is an $N$-element vector of 32-bit values, storing a single RNS polynomial in either the coefficient or the NTT domain.
    }, mathescape=true, style=custompython, label=listing:keyswitch]
  def keySwitch(x: RVec[L],
        ksh0: RVec[L][L], ksh1: RVec[L][L]):
    y = [INTT(x[i],$q_i$) for i in range(L)]
    u0: RVec[L] = [0, ...]
    u1: RVec[L] = [0, ...]
    for i in range(L):
      for j in range(L):
        xqj = (i == j) ? x[i] : NTT(y[i], $q_j$)
        u0[j] += xqj * ksh0[i,j] mod $q_j$
        u1[j] += xqj * ksh1[i,j] mod $q_j$
    return (u0, u1)
  \end{lstlisting}
\end{center}
\end{figure}

\paragraph{Computation vs.\ data movement:}
A single standard keyswitch requires $L^2$ NTTs, $2L^2$ multiplications, and
$2L^2$ additions of $N$-element \mbox{vectors}. In RNS form, the rest of a
homomorphic multiplication (excluding keyswitching) is $4L$ multiplications and
$3L$ additions (\autoref{sec:fhe_operation}), so keyswitching is dominant.

However, the main cost at high values of $L$ and $N$ is data movement. For
example, at $L = 16$, $N = 16K$, each RNS polynomial (\texttt{RVec}) is 64\,KB;
each ciphertext polynomial is 1\,MB; each ciphertext is 2\,MB; and the
keyswitch hints dominate, taking up 32\,MB. As an example, with F1's compute
throughput, fetching the inputs of each keyswitching from off-chip memory would
demand about 10\,TB/s of memory bandwidth. This issue is even more pronounced
for CraterLake. Thus, it is crucial to reuse these values as much as possible.

Fortunately, keyswitch hints can be reused: all homomorphic multiplications use
the same keyswitch hints, and each automorphism has its own keyswitch hint. But
values are so large that few of them fit on-chip.

Finally, note that there is no effective way to decompose or tile this
operation to reduce storage needs while achieving good reuse: tiling the
keyswitch hints on either dimension produces many long-lived
intermediate values; and tiling across \texttt{RVec} elements is even worse
because in NTTs every input element affects every output element.

\paragraph{Performance requirements:} We conclude that, to accommodate these
large operands, an FHE accelerator requires a memory system that \emph{(1)}
decouples data movement from computation, as demand misses during frequent
keyswitches would tank performance; and \emph{(2)} implements a large amount of
on-chip storage (over 32\,MB in our example) to allow reuse across entire
homomorphic operations (e.g., reusing the same keyswitch hints across many
homomorphic multiplications).

Moreover, the FHE accelerator must be designed to use the memory system well.
First, scheduling data movement and computation is crucial: data must be
fetched far ahead of its use to provide decoupling, and operations must be
ordered carefully to maximize reuse. Second, since values are large, excessive
parallelism can increase footprint and hinder reuse. Thus, the system should
use relatively few high-throughput functional units rather than many
low-throughput ones.

\paragraph{Functionality requirements:}
Programmable FHE accelerators must support a wide range of parameters, both $N$
(polynomial/vector sizes) and $L$ (number of residue polynomials, i.e., number
of word-sized prime factors of $Q$). While $N$ is generally fixed for a single
program, $L$ changes dynamically during the execution of each program.

\section{Challenges of Deep FHE Computation}
\label{sec:deepChallenges}

As mentioned in \autoref{sec:noisemgmt}, FHE schemes
limit the overheads of deep computation through a procedure called
bootstrapping that refreshes the multiplicative budget of a ciphertext.
Bootstrapping enables computations of arbitrary depth by separating them into
regions of limited depth. But bootstrapping is an expensive and deep
computation, so it should happen infrequently.

We now analyze the ciphertext sizes (i.e., multiplicative budgets) needed for
effective deep computation. We will show that F1 and prior accelerators do not
effectively support large enough ciphertexts.

\paragraph{Ciphertext sizes needed for deep FHE:}
\autoref{fig:bootstrappingFrequency} reports the cost per homomorphic operation
(in scalar multiplies per homomorphic multiply, $y$-axis) of two deep programs,
as a function of the maximum ciphertext size used ($x$-axis). This determines
bootstrapping frequency: using larger ciphertexts requires less frequent
bootstrapping. \autoref{fig:bootstrappingFrequency} also breaks down cost by
that used for application computation (blue) and bootstrapping (red).

The left plot is for a serial chain of multiplies, the worst case for
bootstrapping cost, as the amount of computation between bootstrappings is
minimal. Consequently, bootstrapping cost dominates. By contrast, the right
plot is for a very wide graph with 100 multiplies per level, which converge to
a single output after each level. This allows boostrapping to be amortized
across many operations, a best-case scenario. We set the ciphertext polynomial degree
to $N=64K$ in order for the ciphertexts to have sufficient security at the
multiplicative budgets required for bootstrapping (\autoref{sec:noisemgmt}).

Crucially, the optimal choice of maximum ciphertext size (shown with a black
dot) is in a narrow range for both extremes, between the multiplicative budgets
of $L=46$ or 20\,MB when $N=64K$ (right), and $L=60$ or 26\,MB when $N=64K$
(left). This is because \emph{both} application computation and bootstrapping
become more expensive with ciphertext size, so regardless of which dominates,
once bootstrapping is infrequent enough, moving to larger ciphertexts only
hurts performance.

Thus, multiplicative budgets of $L=46$ to $L=60$, i.e., 20--26\,MB ciphertexts
are the sweet spot for most deep programs, which fall between these extremes.
In practice, bootstrapping placement is
NP-hard~\cite{benhamouda2017optimization}, because real FHE programs are not as
regular. But all our benchmarks show a similar tradeoff curve to these
synthetic programs.

\figBootstrappingFrequency

As we will see, F1 does not support ciphertexts this large, because \emph{(1)}
it was designed to support ciphertexts up to $N=16K$, a polynomial degree that
does not provide enough security for ciphertexts at the multiplicative budgets
required for deep FHE; and \emph{(2)} it is optimized for standard
keyswitching, not boosted keyswitching. For example, F1 \emph{becomes
inefficient past 2\,MB}. Prior accelerators~\cite{riazi:asplos20:heax} are
limited to even smaller values. This is insufficient to run even bootstrapping
itself. (Although F1 supports \emph{unpacked} bootstrapping of ciphertexts that
encode only a single element, this is $>$1,000$\times$ slower per element and
thus impractical for full applications, as \autoref{sec:results} shows.)

As we will see in \autoref{sec:architecture}, scaling to large ciphertexts
necessary for deep computation is not merely a matter of scaling up hardware;
it requires new algorithms and a new hardware organization to support these
algorithms and to cope with the huge footprint of ciphertexts.

\section{Boosted Keyswitching}
\label{sec:boostedKeyswitching}

Similar to standard keyswitching, boosted keyswitching consists of a large
number of operations on residue polynomials and requires a large auxiliary
operand, the keyswitch hint; the keyswitch hint adds pressure on memory
bandwidth and on-chip storage.

\figKScompare

As mentioned in \autoref{sec:drawbacks} and \autoref{sec:fhe_analysis}, F1 and
prior accelerators are optimized for the \emph{standard} keyswitching
algorithm, which is inefficient for the large ciphertexts we've shown are
needed in \autoref{sec:deepChallenges}. Boosted keyswitching was originally
proposed by Gentry et al.~\cite[Section 3.1]{gentry:crypto2012:homomorphic}. In
fact, there are multiple variants of this algorithm~\cite[Section
5.3.4]{halevi2020helib}, that we will collectively refer to as boosted
keyswitching. FHE libraries targeting deep computations use boosted
keyswitching~\cite{gentry:crypto2012:homomorphic,halevi2020helib,heaan,mouchet2020lattigo}.

The key innovation in boosted keyswitching is to expand the input polynomial to
use wider coefficients. This simplifies the keyswitch hints and its application
compared to standard keyswitching. Boosted keyswitching variants differ in how
much they expand the input, which introduces a tradeoff between performance and
security. We first analyze the most efficient variant (which expands the input
the most), then discuss the performance and security tradeoffs of different
variants.

\autoref{fig:ksCompare} compares the memory footprint and compute cost
(measured in scalar multiplications) of standard and boosted keyswitching as a
function of $L$ (the number of residue polynomials, proportional to the
bitwidth of $Q$). Both algorithms have similar costs for small values of $L$,
but costs grow much more quickly with $L$ for standard keyswitching.

In particular, keyswitch hints are the size of two ciphertexts in the boosted
algorithm. This footprint reduction is the most important. For instance, at
$N$=64K and $L$=60, a keyswitch hint takes 52.5\,MB instead of 1.7\,GB for the
standard algorithm. This enables holding keyswitch hints on-chip and allows for high
reuse. \autoref{fig:ksCompare} also shows that boosted keyswitching reduces
computational costs across the range of multiplicative budget.

\autoref{listing:boostedKeyswitching} shows the implementation of boosted
keyswitching. As mentioned in \autoref{sec:fhe_analysis}, keyswitching takes a
ciphertext polynomial and the keyswitching hint as inputs, and produces two
ciphertext polynomials as output. This variant of boosted keyswitching expands
the input polynomial to use 2$\times$ wider coefficients; this expansion
reduces the keyswitch hint sizes and their application. In RNS representation,
this is accomplished through \verb!changeRNSBase()!, which is used to both
\emph{expand} the $L$-residue input into a $2L$-residue intermediate and later
to \emph{shrink} the output back to $L$ residues.

    \begin{figure}\label{lst:boostedKeyswitching}
      \begin{center}
          \begin{lstlisting}[caption={Boosted keyswitching implementation.}, mathescape=true, style=custompython, label=listing:boostedKeyswitching]
def boostedKeySwitch(p[0:L]):
  pTmp[0:L] = INTT(p[0:L])
  pTmp[L:2L] = changeRNSBase(pTmp[0:L], [L:2L])
  p[L:2L] = NTT(pTmp[L:2L])
  for i = 0, 1:
    prod$\textsubscript{i}$[0:2L] = p[0:2L] * KSH$\textsubscript{i}$[0:2L]
    tmp$\textsubscript{i}$[0:2L] = INTT(prod$\textsubscript{i}$[L:2L], [0:L])
    mDTmp$\textsubscript{i}$[0:L] = changeRNSBase(tmp$\textsubscript{i}$[L:2L], [0:L])
    modDown$\textsubscript{i}$[0:L] = NTT(mDTmp$\textsubscript{i}$[0:L])
    ks$\textsubscript{i}$[0:L] = prod$\textsubscript{i}$[0:L] + modDown$\textsubscript{i}$[0:L]
  return (ks$\textsubscript{0}$[0:L], ks$\textsubscript{1}$[0:L])

def changeRNSBase(x[0:L], destModIdxList):
  for srcModIdx in [0:L]:
    for destModIdx in destModIdxList:
      C = constant[srcModIdx][destModIdx]
      result[destModIdx] += x[srcModIdx] * C
  return result
          \end{lstlisting}
        \end{center}
      \end{figure}

\tblOpBalance

\autoref{tbl:opBalance} compares the operations used by boosted and standard
keyswitching. Whereas standard keyswitching has $L^2$ NTTs, boosted
keyswitching uses only $O(L)$ NTTs: a 10$\times$ reduction for $L$=60. To
achieve this, boosted keyswitching incurs about 50\% more multiplies and adds
than standard keyswitching. However, trading off fewer NTTs for more multiplies
and adds is highly beneficial, because NTTs are much more complex, requiring
$O(\log N)$ multiplies and adds.

F1 and previous accelerators cannot perform boosted keyswitching efficiently
because they are designed to execute all multiplies and adds separately,
resulting in an overwhelming amount of register port pressure. However, the
bulk of these operations takes place in \verb!changeRNSBase()!
(\autoref{tbl:opBalance}), and are structured as sequence of multiply and
accumulate operations
(\autoref{listing:boostedKeyswitching})~\cite{bajard:2016:full}. CraterLake
exploits this by introducing a novel change-RNS-base functional unit,
\emph{CRB}, that buffers the intermediate sums and thereby reduces the register
file pressure \emph{by a factor of $L$} (i.e., up to 60$\times$).

Additionally, \autoref{listing:boostedKeyswitching} shows that most
intermediate variables are consumed immediately after being produced and can
then be discarded. CraterLake exploits this by building \emph{configurable
pipelines} of functional units, further reducing register file reads and writes
(\autoref{sec:keyswitchingPipeline}).

\subsection{Performance-Security Tradeoffs in Boosted Keyswitching}
\label{sec:boostedSecurity}

As \autoref{sec:noisemgmt} explained, the security level of FHE depends on
$N/\log Q$: the ratio between the number of polynomial coefficients and the width
of each coefficient. By expanding the input polynomial by a factor of two, the
above boosted keyswitching algorithm increases the maximum $\log Q$ by 2$\times$.
This would require either doubling $N$ or using half of the levels to achieve
the same security level as standard keyswitching.

Since boosted keyswitching is much more efficient than standard algorithm, this
is a worthwhile tradeoff. Moreover, other boosted keyswitching variants offer
finer control over this tradeoff. Specifically, boosted keyswitching variants
are parameterized by the number of so-called \emph{digits} $d$. The variant
described above is 1-digit keyswitching.
In $d$-digit keyswitching,
\begin{compactenum}
\item The input polynomial is expanded by a factor $d/(1+d)$;
\item Keyswitch hint footprint is proportional to $1+d$; and
\item Compute operations outside of \texttt{changeRNSBase} also increase, e.g.,
    multipications grow by a factor $1+d$; however, this is a minor effect,
    because \texttt{changeRNSBase} dominates the number of operations
    (\autoref{tbl:opBalance}), and the total number of operations within
    \texttt{changeRNSBase} does not grow with the number of digits.
\end{compactenum}

Concretely, using $d$=2, 3, or 4 digits increases the maximum $\log Q$ by
1.5$\times$, 1.33$\times$, and 1.25$\times$, respectively. Thus, higher-digit
keyswitching variants reduce the $N$ required for a given level of security, or
increase the number of levels allowed between bootstrappings vs.\ 1-digit
keyswitching. The main drawback of these variants is that keyswitch hints grow
quickly with the number of digits (by a factor $d+1$), so whereas in 1-digit
keyswitching each keyswitch hint is the size of 2 ciphertexts, with 2--4-digit
keyswitching each keyswitch hint takes 3--5 ciphertexts. This makes these
variant more memory-bound, especially if the larger KSHs reduce the amount of
on-chip reuse.

Achieving a given level of security efficiently requires carefully trading off
the keyswitching variant used and frequency of bootstrapping. FHE programs also
use multiple keyswitching variants over time: higher-digit keyswitching may be
necessary when $\log Q$ is large, but 1-digit keyswitching can be used when
ciphertexts become narrower, since a 2$\times$ expansion does not affect the
maximum $\log Q$ of the computation. Given these tradeoffs, FHE accelerators
should support different keyswitching variants efficiently. For example, to
achieve 80-bit security with $N$=64K in CraterLake's evaluation, we use 2-digit
keyswitching for multiplicative budgets $L>52$ and 1-digit keyswitching
elsewhere; we also show how to achieve higher security levels, e.g., 128 bits
(\autoref{sec:moreSecurity}).

\section{FHE Schemes Other than CKKS}
\label{sec:fhe_others}

We have so far focused on CKKS, but other FHE schemes provide different
tradeoffs. For instance, whereas CKKS supports fixed-point complex number
plaintexts, BGV~\cite{brakerski:toct14:leveled} supports integers modulo a prime.

B/FV~\cite{brakerski:crypto12:fully,fan:iacr12:somewhat} supports modular
arithmetic similar to BGV. However, it encodes plaintexts in a way that makes
modulus switching before homomorphic multiplication unnecessary, thus easing
programming (but forgoing the efficiency gains of modulo switching). And
GSW~\cite{gentry:crypto13:homomorphic} features reduced, asymmetric noise
growth under homomorphic multiplication, but encrypts a small amount of
information per ciphertext (not a full $N/2$-element vector).

F1 and CraterLake both support CKKS, BGV, and GSW. Accelerating B/FV would
require some other primitives, so, though adding support for them would not be
too difficult, our current implementation does not target it.
