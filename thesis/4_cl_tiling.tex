\section{Architecture Overview}\label{sec:overview}

\autoref{fig:overview} shows an overview of the CraterLake architecture. We first
explain its key elements, and then explain why this is the right architecture
for unbounded FHE by considering design alternatives.

\subsection{Logical Organization}

\paragraph{Vector FUs and data types:}
CraterLake is a vector processor with specialized functional units (FUs)
tailored to FHE operations. CraterLake includes fast vector FUs for modular
additions, modular multiplications, NTTs, and automorphisms, which are adapted
from F1. In addition, CraterLake contributes two novel FUs: a Change-RNS-Base
unit (CRB) that accelerates the bulk of boosted keyswitching, and a Keyswitch
hint generator (KSHGen) that generates half of each keyswitch hint on the fly,
reducing memory traffic and on-chip storage.

\figOverview

CraterLake implements a \emph{single} set of vector FUs that process vectors of
a \emph{configurable length} $N$, which can be any power of 2 from 2,048 to
65,536. Each vector represents one residue polynomial, so vector elements have
a fixed, narrow width (28 bits in our implementation). CraterLake has a large
number of \emph{vector lanes} $E$, 2,048 in our implementation. All FUs are
\emph{fully pipelined}, consuming and producing $E$=2,048 elements/cycle. Each
vector is fed to an FU in $N/E$ consecutive cycles.

\paragraph{Memory system:}
CraterLake's on-chip storage is organized as a single-level \emph{256\,MB}
register file shared by all FUs. While this amount of storage might appear
excessive, it fits \emph{just 10 ciphertexts} at the largest parameters
CraterLake targets ($\Nmax = 64K$, $\Lmax = 60$), and smaller register files
severely limit performance, as we show in \autoref{sec:sensitivity}. The
register file uses an \emph{element-partitioned}
design~\cite{asanovic:ucb98:vector} to efficiently emulate 12 read and write
ports. Still, this is only half of the 24 input ports of our FUs. We bridge
this gap by allowing FUs to be \emph{chained} to form multi-FU pipelines that
execute more complex operations.

CraterLake uses high-bandwidth main memory (HBM2E in our implementation).
Memory controllers interface directly with register file banks. The system uses
decoupled data orchestration~\cite{pellauer:asplos19:buffets} to hide memory
latency: memory transfers are performed independently of compute operations,
staging ciphertexts in the register file ahead of their use.

\paragraph{Static control:}
CraterLake hardware is \emph{statically scheduled} to leverage the regularity
of FHE operations. All operations have a fixed latency, and the compiler is
responsible for scheduling all operations and memory transfers to respect all
data dependencies. This avoids the need for hardware to implement any dynamic
control mechanisms, like backpressure or stalling logic, and enables CraterLake
to support very wide vector FUs with minimal control plane overheads.

\subsection{Physical Organization}
\label{sec:tiling}
Implementing an $E$=2,048-lane vector processor naively would result in
prohibitive on-chip traffic. CraterLake addresses this by splitting its lanes
into $G$=8 \emph{lane groups}. Each lane group is $E_G$=256 elements wide and
occupies a physically distinct region of the chip, as \autoref{fig:overview}
shows. As lane groups contain both FU lanes and register file banks, the
majority of data movement can be performed locally within each group.

Splitting lanes into groups is challenging because NTTs and

\noindent automorphisms have all-to-all dependencies between vector elements,
requiring communication among lane groups. Luckily, F1 showed that these
dependencies can be mapped to \emph{transposes} (one per NTT and two per
automorphism). But F1's implementation of transposes does not scale to this
many lanes. To address this challenge, we contribute a new transpose
implementation that performs all data movement between lane groups through a
simple \emph{fixed permutation network}. Supporting CraterLake's compute throughput
requires this network to have a total bandwidth of $4E$ elements per cycle
(29\,TB/s).

\subsection{Comparison to Prior Work}
\label{sec:comparison}

As CraterLake implements a single set of $E$=2,048-lane FUs, all of its lane
groups operate in tandem on different parts of the \emph{same} residue
polynomial. The only operations that require communication among lane groups
are NTTs and automorphisms, which need $E$ and $2E$ elements per cycle of
bandwidth, respectively (\autoref{sec:network}). This is at most 29\,TB/s for
the 2 NTTs and 1 automorphism unit in CraterLake; each homomorphic multiplication
and rotation transfer $8NL$ and $10NL$ words among lane groups, respectively.

In contrast, prior work implements multiple independent \emph{compute clusters}
(analogous to our lane groups), and assigns all elements of each residue
polynomial to a compute cluster~\cite{riazi:asplos20:heax}.
This makes each NTT and automorphism local to a cluster, but each keyswitch
requires all-to-all communication of residue polynomials at a rate of $GE$
elements per cycle, where $G$ is the number of clusters; in total, each
homomorphic operation transfers $3GNL$ words among clusters. Thus, this
approach scales poorly. Specifically, for $G=8$ (as we use in CraterLake), it
requires double the peak bandwidth of our approach (57\,TB/s), and incurs over
2.4$\times$ more traffic per homomorphic operation. More importantly, this
approach requires a complex network between clusters, 16$\times$ larger than
our fixed permutation network (\autoref{sec:methodology}).

Additionally, having all lane groups operate in lockstep reduces on-chip
storage requirements, as we can dedicate the whole chip to a single homomorphic
operation at a time. By contrast, using compute clusters well often requires
overlapping multiple homomorphic operations, which adds to footprint.

Finally, our approach makes the compiler's job easier: it only needs to decide
on a single polynomial operation to run at a time, instead of needing to
orchestrate for parallelism by scheduling multiple operations across clusters.
This keeps the compiler simple and utilization high.

