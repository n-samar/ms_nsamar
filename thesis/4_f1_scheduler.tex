\section{Scheduling Data and Computation}\label{sec:f1_scheduler}

We now describe F1's software stack, focusing on the new static scheduling
algorithms needed to use hardware well.

\figFOneCompilerOverview

\autoref{fig:f1compilerOverview} shows an overview of the F1 compiler. The
compiler takes as input an FHE program written in a high-level domain specific
language (\autoref{sec:programming}). The compiler is structured in three
stages. First, the \emph{homomorphic operation compiler} orders high-level
operations to maximize reuse and translates the program into a
\emph{computation dataflow graph}, where operations are computation
instructions but there are no loads or stores. Second, the \emph{off-chip data
movement scheduler} schedules transfers between main memory and the scratchpad
to achieve decoupling and maximize reuse. This phase uses a simplified view of
hardware, considering it as a scratchpad directly attached to functional units.
The result is a dataflow graph that includes loads and stores from off-chip
memory. Third, the \emph{cycle-level scheduler} refines this dataflow graph. It
uses a cycle-accurate hardware model to divide instructions across compute
clusters and schedule on-chip data transfers. This phase determine the exact
cycles of all operations, and produces the instruction streams for all
components.

This multi-pass scheduling primarily minimizes off-chip data movement, the
critical bottleneck. Only in the last phase do we consider on-chip placement
and data movement.

\paragraph{Comparison with prior work:}
We initially tried static sched\-uling algorithms from prior
work~\cite{blelloch:acm1999:provably,marchal:jpdc2019:limiting,goodman:ics1988:code,ozer:micro1998:unified,barany:odes2011:register},
which primarily target VLIW architectures. However, we found these approaches
ill-suited to F1 for multiple reasons. First, VLIW designs have less-flexible
decoupling mechanisms and minimizing data movement is secondary to maximizing
compute operations per cycle. Second, prior algorithms often focus on loops,
where the key concern is to find a compact repeating schedule, e.g., through
software pipelining~\cite{lam1989software}. By contrast, F1 has no flow control
and we can schedule each operation independently. Third, though prior work has
proposed register-pressure-aware instruction scheduling algorithms, they
targeted small register files and basic blocks, whereas we must manage a large
scratchpad over a much longer horizon. Thus, the algorithms we tried either
worked poorly~\cite{ozer:micro1998:unified, goodman:ics1988:code,
marchal:jpdc2019:limiting} or could not scale to the sizes
required~\cite{barany:odes2011:register, xu:sigplan2007:tetris,
touati:ijpp2005:register, berson:pact1993:ursa}.

For example, when considering an algorithm such as Code Scheduling to Minimize
Register Usage (CSR)~\cite{goodman:ics1988:code}, we find that the schedules it
produces suffer from a large blowup of live intermediate values. This large
footprint causes scratchpad thrashing and results in poor performance.
Furthermore, CSR is also quite computationally expensive, requiring long
scheduling times for our larger benchmarks. We evaluate our approach against
CSR in \autoref{sec:f1_sensitivity}.

We also attempted to frame scheduling as a register allocation problem.
Effectively, the key challenge in all of our schedules is \emph{data movement},
not computation. Finding a register allocation which minimizes spilling could
provide a good basis for an effective schedule. However, our scratchpad stores
at least 1024 residue vectors (1024 at maximum $N = 16K$, more for smaller
values of $N$), and many of our benchmarks involve hundreds of thousands of
instructions, meaning that register allocation algorithms simply could not
scale to our required sizes~\cite{barany:odes2011:register,
xu:sigplan2007:tetris, touati:ijpp2005:register, berson:pact1993:ursa}.

\subsection{Translating the Program to a Dataflow Graph}
\label{sec:programming}

We implement a high-level domain-specific language (DSL) for writing F1
programs. To illustrate this DSL and provide a running example,
\autoref{listing:mv} shows the code for matrix-vector multiplication. This
follows HELib's algorithm~\cite{halevi:crypto14:algorithms}, which
\autoref{fig:f1MultDataflow} shows. This toy $4 \times 16K$ matrix-vector
multiply uses input ciphertexts with $N=16K$. Because accessing individual
vector elements is not possible, the code uses homomorphic rotations to produce
each output element.

\figFOneMultDataflow

\begin{figure}
\begin{center}
  \begin{lstlisting}[caption={$(4 \times 16K)$ matrix-vector multiply in F1's DSL.}, mathescape=true, style=custompython, label=listing:mv]
p = Program(N = 16384)
M_rows = [ p.Input(L = 16) for i in range(4) ]
output = [ None for i in range(4) ]
V = p.Input(L = 16)

def innerSum(X):
  for i in range(log2(p.N)):
    X = Add(X, Rotate(X, 1 << i))
  return X

for i in range(4):
  prod = Mul(M_rows[i], V)
  output[i] = innerSum(prod)
  \end{lstlisting}
\end{center}
\vspace{0.15cm}
\end{figure}

As \autoref{listing:mv} shows, programs in this DSL are at the level of the
simple FHE interface presented in \autoref{sec:fhe_mapping}. There is only one
aspect of the FHE implementation in the DSL: programs encode the desired noise
budget ($L=16$ in our example), as the compiler does not automate noise
management.

\subsection{Compiling Homomorphic Operations}

The first compiler phase works at the level of the homomorphic operations
provided by the DSL. It clusters operations to improve reuse, and translates
them down to instructions.

\paragraph{Ordering} homomorphic operations seeks to maximize the reuse of
keyswitch hints, which is crucial to reduce data movement
(\autoref{sec:fhe_analysis}). For instance, the program in \autoref{listing:mv}
uses 15 different sets of keyswitch hints: one for the multiplies (line
12), and a different one for \emph{each} of the rotations (line 8). If this
program was run sequentially as written, it would cycle through all 15
keyswitch hints (which total 480\,MB, exceeding on-chip storage) four times,
achieving no reuse. Clearly, it is better to reorder the computation to perform
all four multiplies, and then all four \texttt{Rotate(X, 1)}, and so on. This
reuses each keyswitch hint four times.

To achieve this, this pass first clusters \emph{independent} homomorphic
operations that reuse the same hint, then orders all clusters through simple
list-scheduling. This generates schedules with good keyswitch hint reuse.

\paragraph{Translation:} Each homomorphic operation is then compiled into
instructions, using the implementation of each operation in the target FHE
scheme (BGV, CKKS, or GSW). Each homomorphic operation may translate to
thousands of instructions. These instructions are also ordered to minimize the
amount of intermediates. The end result is an instruction-level dataflow graph
where every instruction is tagged with a priority that reflects its global
order.

The compiler exploits algorithmic choice. For example, in F1, the right choice
between the boosted and standard keyswitching algorithm depends on $L$, the
amount of keyswitch hint reuse, and load on FUs. The compiler leverages
knowledge of operation order to estimate these and choose the right variant.

\subsection{Scheduling Data Transfers}
\label{sec:datatransfers}

The second phase of F1's compiler consumes an instruction-level dataflow graph
and produces an approximate schedule that includes data transfers decoupled
from computation, minimizes off-chip data transfers, and achieves good
parallelism. This requires solving an interdependent problem: when to bring a
value into the scratchpad and which one to replace depends on the computation
schedule; and to prevent stalls, the computation schedule depends on which
values are in the scratchpad. To solve this problem, this scheduler uses a
simplified model of the machine: it does not consider on-chip data movement,
and simply treats all functional units as being directly connected to the
scratchpad.

The scheduler is greedy, scheduling one instruction at a time. It considers
instructions ready if their inputs are available in the scratchpad, and follows
instruction priority among ready ones. To schedule loads, we assign each load a
priority

\begin{equation*}
p(\text{load}) = \max \{ p(u) | u \in users(\text{load})\},
\end{equation*}

then greedily issue loads as bandwidth becomes available. When issuing an
instruction, we must ensure that there is space to store its result. We can
often replace a dead value. When no such value exists, we evict the value with
the furthest expected time to reuse. We estimate time to reuse as the maximum
priority among unissued users of the value. This approximates Belady's optimal
replacement policy~\cite{belady1966study}. Evictions of dirty data add stores
to the dataflow graph. When evicting a value, we add spill (either dirty or
clean) and fill instructions to our dataflow graph.

\subsection{Cycle-Level Scheduling}

Finally, the cycle-level scheduler takes in the data movement schedule produced
by the previous phase, and schedules all operations for all components
considering all resource constraints and data dependences. This phase
distributes computation across clusters and manages their register files and
all on-chip transfers. Importantly, this scheduler is fully constrained by its
input schedule's off-chip data movement. It does not add loads or stores in
this stage, but it does move loads to their earliest possible issue cycle to
avoid stalls on missing operands. All resource hazards are resolved by
stalling. In practice, we find that this separation of scheduling into data
movement and instruction scheduling produces good schedules in reasonable
compilation times.

This stage works by iterating through all instructions in the order produced by
the previous compiler phase (\autoref{sec:datatransfers}) and determining the
minimum cycle at which all required on-chip resources are available. We
consider the availability of off-chip bandwidth, scratchpad space, register
file space, functional units, and ports.

During this final compiler pass, we finally account for store bandwidth,
scheduling stores (which result from spills) as needed. In practice, we find
that this does not hurt our performance much, as stores are infrequent across
most of our benchmarks due to our global schedule and replacement policy
design. After the final schedule is generated, we validate it by simulating it
forward to ensure that no clobbers or resource usage violations occur.

It is important to note that because our schedules are fully static, our
scheduler also doubles as a performance measurement tool. As illustrated in
\autoref{fig:f1compilerOverview}, the compiler takes in an architecture
description file detailing a particular configuration of F1. This flexibility
allows us to conduct design space explorations very quickly
(\autoref{sec:scalability}).
