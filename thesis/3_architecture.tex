
\chapter{F1 Architecture}\label{sec:arch}

\emph{\name was designed in collaboration with Nikola Samardzic, Aleksandar Krastev, and Daniel Sanchez. All work on the design of functional units and register files was conducted by Nikola Samardzic and Alex Krastev. Descriptions of the functional unit and register file design are included in this thesis to fully describe \name's architecture, even though they are not the work of the author.}\\

\autoref{fig:arch} shows an overview of \name, which we derive from the insights in \autoref{sec:fhe_analysis}.

\figArch

\section{Vector processing with specialized functional units}

\name features wide-vector execution with functional units (FUs) tailored to 
%all % dsm: We just said that there are some primitive ops we don;t implement, so saying all here is a bit jarring.
primitive FHE operations.
Specifically, \name implements vector FUs for modular addition, modular multiplication, NTTs (forward and inverse in the same unit),
and automorphisms.
Because we leverage RNS representation, these FUs use a fixed, small arithmetic word size (32 bits in our implementation),
avoiding wide arithmetic.

%Following vector processing terminology,
FUs process vectors of configurable \emph{length} $N$ using a fixed number of \emph{vector lanes} $E$.
Our implementation uses $E=$128 lanes and supports power-of-two lengths $N$ from 1,024 to 16,384.
This covers the common range of FHE polynomial sizes, so an RNS polynomial maps to a single vector.
Larger polynomials (e.g., of 32K elements) can use multiple vectors.

All FUs are \emph{fully pipelined}, so they achieve the same throughput of $E=$128 elements/cycle.
FUs consume their inputs in contiguous chunks of $E$ elements in consecutive cycles.
This is easy for element-wise operations, but hard for NTTs and automorphisms.
\autoref{sec:FUs} details our novel FU implementations, including the first vector implementation of automorphisms.
Our evaluation shows that these FUs achieve much higher performance than those of prior work,
because, as we saw in \autoref{sec:fhe_analysis},
\emph{having fewer high-throughput FUs limits parallelism and thus memory footprint}.

\section{Compute clusters}
Functional units are grouped in \emph{compute clusters}, as \autoref{fig:arch} shows.
Each cluster features several FUs (1 NTT, 1 automorphism, 2 multipliers, and 2 adders in our implementation)
and a banked register file that can (cheaply) supply enough operands each cycle to keep all FUs busy.
The chip has multiple clusters (16 in our implementation).

% TODO(dsm): This may be a good place to explain the banking approach.




\section{Memory system} \label{sec:memsystem}
\name features an explicitly-managed memory hierarchy. As \autoref{fig:arch} shows,
\name features a large, heavily-banked scratchpad (64\,MB across {16} banks in our implementation).
The scratchpad interfaces with both high-bandwidth off-chip memory (HBM2 in our implementation)
and with compute clusters through an on-chip network.

\name uses decoupled data orchestration~\cite{pellauer:asplos19:buffets} to hide main memory latency.
Scratchpad banks work autonomously, fetching data from main memory far ahead of its use.
Since memory has relatively low bandwidth, off-chip data is always staged in scratchpads,
and compute clusters do not access main memory directly.

The on-chip network connecting scratchpad banks and compute clusters provides very high bandwidth,
which is necessary because register files are small and achieve limited reuse.
We implement a single-stage bit-sliced crossbar network~\cite{passas:tocaid12:crossbar} that provides full bisection bandwidth.
Banks and the network have wide ports (512 bytes), so that a single scratchpad bank can send a vector to a compute unit
at the rate it is consumed (and receive it at the rate it is produced).
This avoids long staging of vectors at the register files.

\section{Static scheduling}
Because FHE programs are completely regular, \name adopts a \emph{static, exposed microarchitecture}:
all components have fixed latencies, which are exposed to the compiler.
The compiler is responsible for scheduling operations and data transfers in the appropriate cycles to prevent
structural or data hazards.
%---hardware includes no mechanisms for dynamic arbitration among conflicting requests,
%stalling, or bypassing.
This is in the style of VLIW~\cite{fisher:isca83:very}.

This approach simplifies logic throughout the chip. For example, FUs need no stalling logic;
register files and scratchpad banks need no dynamic arbitration to handle conflicts;
and the on-chip network uses simple switches that change their configuration independently over time,
without the buffers and arbiters of packet-switched networks.

Because memory accesses do have a variable latency, we assume the worst-case latency,
and buffer data that arrives earlier %in the memory controller
(note that, because we access large chunks of data,
e.g., 64\,KB, this worst-case latency is not far from the average).

\section{Distributed control}
Though static scheduling is the hallmark of VLIW, \name's implementation is quite different:
rather than having a single stream of instructions with many operations each, in
\name each component has an \emph{independent instruction stream}.  % nikola: what does component mean here?
This is possible because \name does not have any control flow: though FHE programs may have loops,
we unroll them to avoid all branches, and compile programs into linear sequences of instructions.

This approach may appear costly. But vectors are very long, so each instruction encodes a lot of work and this overhead is minimal. Moreover, this enables a compact instruction format, which encodes a single operation followed by the number of cycles
to wait until running the next instruction. 
This encoding avoids the low utilization of VLIW instructions, which leave many operation slots empty.
Each FU, register file, network switch, scratchpad bank, and memory controller has its own instruction stream,
which a control~unit~fetches in small blocks and distributes to components. 
We use 3 bits to encode the instruction opcode, 17 bits to encode operands, and a further 20 bits to encode the number of cycles until the next instruction is issued. This results in 5 byte instruction format, an insignificant overhead compared to our minimum operand size of 4KB. Across all of our benchmarks, instruction fetches consume less than 0.1\% of main memory traffic.

% dsm: I'm still unconvinced about having this here;
% FIXME(dsm): That 1993 paper is far from the first one to propose an element-partitioned design; take a look at Krste's thesis, there's a much much earlier (70s or early 80s) IBM machine doing this. Regardless, that is arcana; I'd just cite Krste's thesis and say it's an \emph{element-partitioned} design.
\section{Register file (RF) design}  \label{sec:regfiles}
Each cluster in \name requires 10 read ports and 6 write ports to keep all FUs busy.
To enable this cheaply, use an 8-banked \emph{element-partitioned} register file design~\cite{asanovic:ucb98:vector}
that leverages long vectors:
each vector is striped across banks, and each FU cycles through all banks over time, using a single bank each cycle.
By staggering the start of each vector operation, FUs access different banks each cycle.
This avoids multiporting, requires a simple RF-FU interconnect, and performs within 5\%
of an ideal infinite-ported RF.


%enable all units to operate in parallel. This means register files must be banked, but connecting 10 RF bank ports with the FUs poses power and scheduling issues. We address this by using the RF design proposed in~\cite{awaga:micro93:mu}; here, the vectors of a ciphertext are stored in RF banks one vector per bank in a round-robin fashion; each FU is connected to a single RF port each cycle and the assignment between FUs and RF banks is rotated cyclically on each cycle. This means that each FU will have to wait up to 10 cycles to start reading its input. \axelf{worth mentioning that operations take tens to hundreds of cycles for a sense of scale?} 
%Therefore, this design 1) uses a simple interconnect between RFs and FUs and 2) guarantees no bank conflicts at a cost of the added 10 cycle latency; we notice this trade-off is ideal for FHE because the static dataflow graph allows the scheduler to effectively manage the added latency: performance on benchmarks is within 5\% of an ideal infinite-port RF and 2.5$\times$ better than a crossbar-connected 4-read/2-write port RF.

\section{Functional Units}
\label{sec:FUs}

In this section, we describe \name's novel functional units.
These include the first vectorized automorphism unit (\autoref{sec:automorphism}),
the first fully-pipelined flexible NTT unit (\autoref{sec:fourStepNTT}),
and a new simplified modular multiplier adapted to FHE (\autoref{sec:modMult}).

\subsection{Automorphism unit}\label{sec:automorphism}


Each residue polynomial in \name is stored as $G$ independent $E$-element hardware vectors ($N=G\cdot E$).
An automorphism $\sigma_k$ maps the element at index $i$ to index $ki \textrm{ mod } N$;
there are $N$ automorphisms total, two for each odd $k < N$ (\autoref{sec:fhe_operation}).
The key challenge in designing an automorphism unit is that these permutations are hard to vectorize:
we would like this unit to consume and produce $E=$128 elements/cycle, but the vectors
are much longer, with $N$ up to 16\,K, and elements are permuted across different hardware vectors.
Moreover, we must support variable $N$ \emph{and} all automorphisms.

Standard solutions fail: a 16\,K$\times$16\,K crossbar is much too large;
a scalar approach, like reading elements in sequence from an SRAM, is too slow (taking $N$ cycles);
and using banks of SRAM to increase throughput runs into frequent bank conflicts:
each automorphism ``spreads''~elements with a different stride, so regardless of geometry,
some automorphisms will map many consecutive elements to the~same~bank.

\figAutomorphism

We contribute a new insight that makes vectorizing automorphisms simple:
if we interpret a residue polynomial as a $G \times E$ matrix,
an automorphism can always be decomposed into two independent \emph{column} and \emph{row permutations}.
If we transpose this matrix, both column and row permutations can 
be applied \emph{in blocks of $E$ elements}. \autoref{fig:automorphism} shows an example 
of how automorphism $\sigma_3$ is applied to a residue polynomial
with $N=16$ and $E=4$ elements/cycle.
Note how the permute column and row operations are local to each $4$-element hardware vector.
Other $\sigma_k$ induce different permutations, but with the same row/column structure.



Our automorphism unit, shown in \autoref{fig:aut_fu},
uses this insight to be both vectorized (consuming $E=128$ elements/cycle) and fully pipelined.
Given a residue polynomial of $N=G\cdot E$ elements, the automorphism unit first applies the column permutation
to each $E$-element input.
Then, it feeds this to a \emph{transpose unit} that reads in the whole residue polynomial interpreting it as a $G\times E$ matrix,
and produces its transpose $E\times G$.
The transpose unit outputs $E$ elements per cycle (outputting multiple rows per cycle when $G < E$).
Row permutations are applied to each $E$-element chunk, and the reverse transpose is applied.

\figautfu


Further, we decompose both the row and column permutations into a pipeline of sub-permutations that are \textit{fixed in hardware},
with each sub-permutation either applied or bypassed based on simple control logic; this avoids using crossbars for the $E$-element permute row and column operations.

\figQuadrantSwap

\paragraph{Transpose unit:}
Our \textit{quadrant-swap transpose} unit transposes an $E \times E$ (e.g., $128\times 128$) matrix by recursively decomposing it into quadrants and exploiting
the identity

\vspace{-12pt}
\begin{equation*}
  \left[ \begin{array}{c|c}
      \texttt{A} & \texttt{B}\\
      \hline
      \texttt{C} & \texttt{D}
  \end{array}\right]^{\textrm{T}} =   \left[ \begin{array}{c|c}
      \texttt{A}^{\textrm{T}} & \texttt{C}^{\textrm{T}} \\
      \hline
      \texttt{B}^{\textrm{T}} & \texttt{D}^{\textrm{T}}
  \end{array}\right].
\end{equation*}

The basic building block is a $K \times K$ \textit{quadrant-swap} unit, which swaps quadrants \texttt{B} and \texttt{C}, as shown in \autoref{fig:quadrantSwap}(left). Operationally, the quadrant swap procedure consists of three steps, each taking $K/2$ cycles:
\begin{compactenum}
  \item Cycle \texttt{i} in the first step reads \texttt{A[i]} and \texttt{C[i]} and stores them in \texttt{top[i]} and \texttt{bottom[i]}, respectively.
\item Cycle \texttt{i} in the second step reads \texttt{B[i]} and \texttt{D[i]}. The unit activates the first swap MUX and the bypass line, thus storing \texttt{D[i]} in \texttt{top[i]} and outputting \texttt{A[i]} (by reading from \texttt{top[i]}) and \texttt{B[i]} via the bypass line.
\item Cycle \texttt{i} in the third step outputs \texttt{D[i]} and \texttt{C[i]} by reading from \texttt{top[i]} and \texttt{bottom[i]}, respectively. The second swap MUX is activated so that \texttt{C[i]} is on top.
\end{compactenum}
Note that step $3$ for one input can be done in parallel with step $1$ for the next, so the unit is \emph{fully pipelined}.

The transpose is implemented by a full $E \times E$ quadrant-swap followed by $\log_2E$ layers of smaller transpose units
to recursively transpose \texttt{A}, \texttt{B}, \texttt{C}, and \texttt{D}. \autoref{fig:quadrantSwap} (right) shows an implementation for $E=8$. Finally, by selectively bypassing some of the initial quadrant swaps,
this transpose unit also works for all values of $N$ ($N=G\times E$ with power-of-2 $G < E$).

Prior work has implemented transpose units for signal-processing applications,
either using registers~\cite{wang2018pipelined,zhang2020novel} or with custom SRAM designs~\cite{shang2014single}.
Our design has three advantages over prior work: it uses standard SRAM memory,
so it's dense without requiring complex custom SRAMs;
it is fully pipelined; and it works for a wide range of dimensions.

\subsection{Four-step NTT unit}\label{sec:fourStepNTT}

\figFourStepNTT

%The Cooley-Tukey algorithm \cite{cooley:moc65:algorithm} can be parallelized in hardware into a so-called \textit{NTT butterfly} pipeline.
%However, this pipeline requires the datapath to be as wide as the number of elements we want
%to apply the NTT to. This means we would need a $16K$-element datapath to support $N=16K$, which is too wide.
%Ideally, instead of unrolling the datapath prior to an NTT, we would fix it to $128$-element hardware vectors,
%with each ciphertext stored in $8$-$128$ hardware vectors (depending on the value of $N$).

There are many ways to implement NTTs in hardware:
an NTT is like an FFT~\cite{cooley:moc65:algorithm}
but with a butterfly that uses modular multipliers.
We implement $N$-element NTTs (from 1K to 16K) as a composition
of smaller $E$=128-element NTTs,
since implementing a full 16K-element NTT datapath is prohibitive.
The challenge is that standard approaches result in memory access patterns
that are hard to vectorize.

To that end, we use the \textit{four-step variant} of the FFT algorithm~\cite{bailey:supercomputing89:FFTs},
which adds an extra multiplication to produce a vector-friendly decomposition.
\autoref{fig:fourStepNTT} illustrates a simple four-step NTT pipeline for $E=4$;
we use the same structure with $E=128$.
The unit is fully pipelined and consumes $E$ elements per cycle.
To compute an $N=E\times E$ NTT, the unit first computes an $E$-point NTT on each $E$-element group,
multiplies each group with twiddles,
transposes the $E$ groups, and computes another $E$-element NTT on each transpose.
The same NTT unit implements the inverse NTT
by storing multiplicative factors (\textit{twiddles}) required for both forward and inverse NTTs in a small \textit{twiddle SRAM}.

Crucially, we are able to support all values of $N$ using a single four-step NTT pipeline by conditionally bypassing layers in the second NTT pipeline.
We use the same transpose unit implementation as with automorphisms.

The NTT unit is large: each of the 128-element NTTs requires $E(\log (E)-1)/2$=384 multipliers,
and the full unit uses 896 multipliers.
But its high throughput improves performance over many low-throughput NTTs (\autoref{sec:evaluation}). %by gmean 2.5$\times$ on relevant benchmarks (\autoref{tbl:sensitivity}); this is because using many small NTT units in parallel causes the memory footprint to quickly exceed scratchpad capacity.
This is the first implementation of a fully pipelined four-step NTT unit, 
improving NTT performance by 1,600$\times$ over the state of the art (\autoref{tbl:microbenchmark}).


\subsection{Optimized modular multiplier}\label{sec:modMult}
\tblModMult

Modular multiplication computes $a\cdot b \textrm{ mod } q$.
This is the most expensive and frequent operation.
Therefore, improvements to the modular multiplier have an almost
linear impact on the computational capabilities of an FHE accelerator.

Prior work~\cite{mert:euromicro19:design}
recognized that a Montgomery multiplier~\cite{montgomery:mom85:modular} within NTTs can be improved by leveraging
the fact that the possible values of modulus $q$ are restricted by the number of elements the NTT is applied to.
We notice that if we only select moduli $q_i$, such that $q_i = -1 \textrm{ mod } 2^{16}$,
we can remove a multiplier stage from~\cite{mert:euromicro19:design};
this reduces area by 19\% and power by 30\% (\autoref{tbl:modMult}).
The additional restriction on $q$ is acceptable because FHE
requires at most 10s of moduli~\cite{gentry:crypto2012:homomorphic},
and our approach allows for 6,186~prime~moduli.


